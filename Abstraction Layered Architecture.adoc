= Abstraction Layered Architecture
:doctype: article
:encoding: utf-8
:lang: en
:toc: left
:sectnums:
:imagesdir: images
:source-highlighter: highlightjs
:highlightjs-theme: Docco




// blame J R Spray

John R Spray

I would like to acknowledge the help of Roopak Sinha at AUT (Auckland University of Technology) in the writing of the paper for ECSA 2018 including his academic perspective. 

// for web site

link:Abstraction_Layered_Architecture_Paper_ECSA_2018.pdf[ALA Paper presented at ECSA 2018]

// for pdf

// https://AbstractionLayeredArchitecture.com/Abstraction_Layered_Architecture_Paper_ECSA_2018.pdf[ALA Paper presented at ECSA 2018]

This article is organised as passes. The initial pass is brief and each subsequent pass is more grounded. Each pass ends with an example project. These projects get larger in terms of functionality, but because of nature of ALA, it is feasible to use real, large examples rather than the pedagogical, trivial sized examples usually found in articles.      

---
== Pass one - What problem does ALA solve?

Before getting inside the mechanisms of how ALA works, let's explain  ALA in terms of the problem it solves.


=== An optimal reference architecture for certain quality attributes

ALA is a way to structure code. It tells you how to organise your code so it doesn't degenerate, little by little, into a big ball of mad during it's life cycle. Architectural patterns or styles, such as Layers, Event-Driven, MVC, UML Models, Dependency Injection, Functional Programming and Object Oriented Design are insufficient to prevent a big ball of mud, as are software principles such as Single Responsibility, Liskov Substitution, Interface Segregation, and Dependency Inversion.  What is needed is a higher order organisational strategy. That strategy makes it clear where, how and why to use all these other things. That higher order strategy is ALA. 

ALA is a reference structural architecture. Its structure is independent of any specific functional or non-functional requirements, so it is a general reference architecture. It is optimal for these non-functional requirements:

** Readability
** Complexity
** Maintainability
** Testability

If other non-functional requirements are also important, ALA still provides a good starting point. Even if the ALA structure must be compromised in places, it is still better to start from that point. As it happens, the maintainability resulting from ALA frequently makes other quality attributes easy to do as well, without significantly compromising ALA's structure.

=== Readability

[.float-group]
-- 
image::close_up_code.jpg[,400, title="Code quickly becomes a big ball of mud", float="right"]

ALA code is always readable, not because of style, conventions, comments or documentation of the code itself, but simply because code appears as separate little programs.  
--


=== Complexity

There is a meme in the software industry that the complexity of software must be some function of its size. This need not be so. With proper use of abstraction it is possible to have complexity that is constant regardless of program size. In ALA, complexity is constant:

anchor:ComplexityGraph1[]

[chart,line,file="complexity_curve.png", opt="title=Complexity,x-label=KLOC,legend=right"]
--
//Big ball of mud
1,	10
2,	20
5,	50
10,	100
20,	200
50,	500

//Loosely coupled
1,	10
2,	14
5,	22
10,	32
20,	45
50,	71
100,100
200,141
500,224
1000,316

//ALA
1,	10
2,	11
5,	12
10,	13
20,	13
50,	15
100,16
200,17
500,19
1000,20

//Code writer's brain limit
1,	100
2,	100
5,	100
10,	100
20,	100
50,	100
100,100
200,100
500,100
1000,100

//Code reader's brain limit
1,	50
2,	50
5,	50
10,	50
20,	50
50,	50
100,50
200,50
500,50
1000,50
--

This is a qualitative graph comparing the complexity of an ALA application with that of a big ball of mud and an average loosely coupled application. This is further explained later <<ComplexityGraph2,here>>.


=== Maintainability

If ALA is done well, the maintainability characteristic over time should qualitatively follow the green curve in the graph below. If a project will never be maintained past about six months, it probably won't be worth doing ALA.

[chart,line,file="effort_curve.png", opt="title=Effort per user-story,x-label=months"]
--
//Big ball of mud
1,	5
2,	5
3,	6
4,	6
5,	7
6,	8
7,	9
8,	10
9,	12
10,	13
11,	15
12,	17
13,	19
14,	21
15,	24
16,	28
17,	32
18,	37
19,	43

//Cocomo
1,	16
2,	17
3,	17
4,	18
5,	18
6,	19
7,	19
8,	19
9,	19
10,	20
11,	20
12,	20
13,	20
14,	20
15,	20
16,	20
17,	21
18,	21
19,	21
20,	21
21,	21
22,	21
23,	21
24,	21

//ALA
1,	30
2,	21
3,	17
4,	15
5,	13
6,	11
7,	10
8,	9
9,	8
10,	8
11,	7
12,	7
13,	6
14,	6
15,	5
16,	5
17,	4
18,	4
19,	3
20,	3
21,	3
22,	2
23,	2
24,	2
--

The graph compares ALAs maintainability with a typical loosely coupled software project that would fall on the average COCOMO model, and with the big ball of mud software where no effort to create a good architecture is made.


=== Domain oriented

As has been found in other techniques such such as Domain Specific Languages, Domain Driven Design, Model Driven Software Development and Language Oriented Programming, ALA is highly domain oriented. It has a method, or process that starts with describing requirements, inventing the needed domain abstractions as you go.   


=== The software engineer's trap

Typical bright young engineers come out of university knowing C++ (or another low-level imperative language that mimics the silicon), and are confident that because the language is Turing-complete, if they string together enough statements, they can accomplish anything. At first they can. Agile methods only require them to deliver an increment of functionality. There hardly seems a need for a software architect to be involved, and besides, we are told that any design errors can be corrected through incremental refactoring.

image::Cynefin.jpg[,500, title="Code can quickly get complex", float="left"]

As the program gets larger, things are getting a little more complicated, but their brains still handle the complications, so they continue. One day the code transitions into the complex. Unknowingly, they have stepped into a trap. The incremental effort to maintain starts to eat away and eventually outweigh the incremental increase in value. This negative return causes the codebase itself to lose its value. Eventually it is no longer an asset to the business. 

When a new bright young engineer who knows C++ arrives, he looks at the legacy codebase and is convinced (mostly by Dunning-Kruger bias) that he can do better. And the cycle starts over. This is the CRAP cycle (Create, Repair, Abandon, rePlace).

=== My story

From early on in my career, stuck in the CRAP cycle, I wanted a pre-worked generally applicable 'template architecture' that would tell me what the organisation of the code should look like, so that I could concentrate on implementing the functional requirement. I searched for such a thing many times. In my searches, I would find things like 'loose coupling', and I remember asking myself, yes but how does one accomplish that?

Forty years worth of mistakes later, I finally have what I wanted. The turning point was when I noticed a two (accidental) successes. These successes were only noticed years later, 15 years in one case and 5 years in the other. They had both undergone considerable maintenance during that time. But their simplicity had never degraded and their maintenance had always been straightforward. It was like being at a rubbish dump and noticing two pieces of metal that had never rusted.

One of them had the same functionality as another piece of software that I had written years earlier. That software was the worst I had ever written. It was truly a big ball of mud, and maintenance had become completely impossible, causing the whole product to be abandoned. So it wasn't what the software did that made the difference between good and bad. It was how it was done.

Analysing the common properties of those two code bases, which no other pieces of our software had, caused the beginnings of ALA.

As I wrote it down, more and more pieces seemed to fit into place. How it worked made more and more sense. Subsequently, I ran some experiments to see if the quality attributes could be predictably reproduced. These experiments are discussed in detail later in this article.


=== Overwhelming software architecture styles, patterns & principles

Currently the problem of structuring software code to meet these quality attributes involves mastering an overwhelming number of software engineering topics:  

* Complexity, Understandability, Readability, Maintainability, Modifiability, Testability, Extensibility, Dependability, Performance, Availability, Scalability, Portability, Security, usability, Fault-tolerance
* Views, Styles, Patterns, Tactics, Models, UML, ADL's, ADD, SAAM, ATAM, 4+1, Decomposition
* CBD/CBSE, C&C, Pipes & Filters, n-tier, Client/Server, Plug-in, Microservices, Monolithic, Contracts, Message Bus
* Module, Component, Layer, Class, Object, Abstraction, Granularity 
* Information hiding, Separation of Concerns, Loose Coupling & High Cohesion 
* Semantic coupling, Syntax coupling, Temporal coupling, existence coupling, Dependencies, Interactions, Collaboration
* Interfaces, Polymorphism, Encapsulation
* Execution models, Event-Driven, Multithreaded, Mainloop, Data-driven, Concurrency, Reactor pattern 
* Principles: SRP, OCP, LSP, ISP, DIP; MVC, MVP, etc 
* Design Patterns: Layers, Whole-Part, Observer, Strategy, Factory method, Wrapper, Composite, Decorator, Dependency Injection, Callbacks, Chain of Responsibility, etc
* Expressiveness, Fluency, DDD, Coding guidelines, Comments, Documentation 
* Programming Paradigms, Imperative, Declarative, OO, Activity-flow, Work-flow, Data-flow, State machine, GUI layout, Navigation-flow, Data Schema, Functional, FRP, RX, Monads, AOP, Polyglot-Programming Paradigms
* Messaging: Push, Pull, Synchronous, Asynchronous, Shared memory, Signals & Slots
* Memory management, Heap, Persistence, Databases, ORMs
* Up-front design, Agile, Use cases, User stories, TDD, BDD, MDSD

Mastering all these topics takes time and experience. Even if you can master them, juggling them all and being able to use the right ones at the right time, while also mastering technologies and tools, and all the time sprinting is a big ask. In today's Agile world, working code is what the team is judged on, especially by project managers or product owners who have no interest in architecture. They often don't want to know about any attempts to inform them, such as the rather negative sounding term "technical debt". Usually all this pressure for short term gains means that software architecture is neglected. 

=== Pre-solved recipe of architectural styles, patterns and principles  

ALA works by pre-solving for many of these software engineering topics as far as possible without knowing the functional requirements. The result is a reference architecture that gives you meta-structure and some important constraints, especially on what relationships you can and can't have in that structure. 

Consequently, ALA contains no truly novel ideas. Instead it is a recipe based on the aforementioned overwhelming list of software engineering topics. Some ingredients are added in much larger quantity than in conventional software design (such as abstraction). Some are relatively neutral. And there are a few big ones that it surprisingly excludes. Like any good recipe, the ingredients work together to form a whole that is much greater than the sum of parts. That whole makes improvements to quality significantly greater than the incremental improvements that any single topic would be used alone. It continues to surprise me just how effective it is. The big surprise for me was the few very well-established memes in software engineering that the ALA structure says is wrong. We will get to these one at a time in the subsequent passes.

=== The first few strokes

As a software engineer contemplating a new project, I have often asked myself "Where do I start"? This also happens with legacy code, when contemplating the direction that refactorings should take. "If this software were structured optimally well, what would it look like?"

Christopher Alexander, the creator of the idea of design patterns in building architecture, said, "As any designer will tell you, it is the first steps in a design process which count for the most. The first few strokes which create the form, carry within them the destiny of the rest".

In an Agile world, where every stage of software development, including architecture, is emerges from iterative sprints, this wisdom has been lost. ALA restores that wisdom to software development, and gives the software architect the exact process to follow for that piece of up-front design in sprint zero. No more than one sprint is required to do the architecture, regardless of the size of the project.

I believe the result of those first few strokes of the design would never be arrived at by the agile process. Furthermore, once this architecture is done, the Agile process works significantly better thereafter. And my experience so far over several projects is that the architecture does not need to change as the development proceeds. 







=== Example project - Functional composition

In this example, we use 'functional composition' because it is the easiest programming paradigm to use ALA. However, keep in mind that functional composition is not a suitable programming paradigm for most programs. It only suits when the problem requires dedicated CPU to process a job as fast as it can in computer time. Nothing else ever needs doing while that is being done, and we never have to wait for anything else while it is being done. Nevertheless, this is often the case for very small parts of programs, so it's still worth applying ALA to functional composition at this micro scale.

Functional composition means composing with function calls. Applying ALA to functional composition means four things:

*  Make every function an abstraction

For our purpose here, an abstraction means that our brain can easily learn (preferably by reading the function name or a comment) and retain what a function essentially does. It means that when other programmers are reading your code where a function is called, they stay reading that code and don't have to go and find the function to read code there as well. It could be one simple thing like filter, or get an ADC reading. The name of the function would not be generic ProcessData, or CalculateResult. It would not be the name of the event that caused it to be executed like PulseComplete. The first two just separate code without saying what the abstraction is. The last one is just a place where you put everything that must be done now. 

*  Keep function call tree depth shallow

Often only two levels will be used. The top level corresponds to the detail level of a specific application. The second level corresponds to the abstraction level of the domain.

* Abstraction level increases as you go down the levels

The abstraction level should move away from the specifics of the application at the top level, toward more ubiquitous, more reusable, and more stable functions. So the 2nd level should have the abstraction level of the domain. The 3rd level has the abstraction level of the types of programming problems being solved, such as using third party libraries or what's often referred to as middleware, or writing their equivalents ourselves. For completeness, a 4th level would be the library that comes with your language. 

* Keep depth levels at discrete levels of abstraction

A function that doesn't clearly belong at one of those abstractions may be straddling two levels. Refactor so that application specifc details go to the level above, and something more abstract and reusable goes below it.

Let's look at some bad code that breaks each of these constraints and then the corresponding code that fixes them. 

==== Bad code

.main.c
[source]
 void main()
 {
    while (1)
    {
        GetTemperatures(temperatures);  <1>
        for (i = 1; i<100; i++) {  <2>
            ProcessTemperature(temperatures[i]); <3>
        }
    }
 }

.Process.c
[source,C]
 // do everything needed to process a adc reading
 void ProcessTemperature(temperature)
 {
    temperature = (temperature + 4) * 8.3;  <4>
    SmoothAndDisplay(temperature);  <5>
 }

.smooth.c
[source,C]
 // smooth the reading before displaying
 void Smooth(temperature) <6>
 {
    static filtered = filtered/2 + temperature/2; <7>
    Display(filtered); <8>
 }


<1> function name abstraction level incorrect - name is specific to application instead of domain
<2> implementation details that should be in a domain abstraction
<3> function name doesn't describe an abstraction
<4> detail should have an abstraction for offsetting and scaling
<5> function composition in wrong level (should be called from application level which knows this needs doing)
<6> function has two abstractions (responsibilities)
<7> detail should be an abstraction (and keep consistent terms)
<8> function composition too deep (should be called from application level)



==== Better code

.application.c
[source,C]
 void main() <1>
 {
    while (1)
    { 
        #define NREADINGS 100 <4>
        GetAdcReadings(temperaturesAsAdc, NREADINGS); <2>
        OffsetAndScale(temperaturesAsAdc, temperaturesInCelcius, NREADINGS, 4, 8.3); <5>
        smoothTemperature = Filter(temperaturesInCelcius, NREADINGS, 2); <6>
        Display(smoothedTemperature);
    }
 }

.offsetandscale.c
[source,C]
 // offset and scale all elements of an array
 void OffsetAndScale(dataIn, dataOut, n, offset, scale) <3>
 {
    for (i = 0; i<n; i++) {
        dataOut[i] = (dataIn[i] + offset) * scale;
    }
 }

.filter.c 
[source,C] 
 // IIR Filter n readings, using previous result as start value
 float Filter(input, n, filterStrength)  <3>
 {
    static filtered = 0; <7>
    for (i = 0; i<n; i++) {
        filtered = filtered * (filterStrength-1) / filterStrength + input[i]/filterStrength;
    }
    return filtered;
 }

<1> Application is readable on it's own without having to go and read code inside any of the abstractions it uses.
<2> The application fully expresses the requirements, and does nothing else, delegating all the actual work to the abstractions. Only the application knows about the thermometer. Application knows nothing of the how the abstractions work, only what they do.
<3> None of the abstractions used know anything about each other or anything about the application itself - they don't know for example about the thermometer.
<4> Application knows the requirement detail of how many adc readings for each temperature display update.
<5> Application knows conversion offset and factors from adc to Celcius but not how to do offsetting and scaling.
<6> Application knows filtering configuration but not how to actually do filtering.
<7> Filter is a good abstraction despite having a side effect

Remember that while instances of abstractions can be composed perfectly well with functional composition, it is not generally a good programming paradigm or execution model, especially as a program scales up in size. We have used it here to demonstrate ALA in familiar easy code, but we won't get far doing everything with functions. The problem is that functional composition forces the execution flow to follow exactly the composition flow (the instances of abstractions we have composed together). This only suits a narrow range of problems. Usually we will need to use ALA in other programming paradigms and execution models that separate execution flow from the composition flow. All the other examples to follow will do this.


== Pass two - What is ALA?

In this section we give some different perspectives to try to explain what ALA is. Because we all have different experiences, and different hooks on which we hang new ideas, we need different perspectives to 'get' the insight. Find one or two that best explain the insight for you. Don't worry about the ones that don't make sense. 

 

=== Abstraction Layers

This first perspective is the most difficult to understand because it is the most abstract perspective we will use. I introduce it first because it is the essence of ALA and what gives it its name and its structure. We will spend most of this article understanding it until it is second nature. 

The diagram shows the abstraction layers:

image::Layers.png[Layers diagram, title="The four ALA layers", width=75%]

The first problem in understanding abstraction layers is understanding what abstraction means. Unfortunately the software industry has misused the word because it always sees hardware at the bottom, and since hardware is 'concrete', it must be the least abstract and as we build things on top of the hardware we must get more abstract. This is completely wrong if we use the original meaning of the word 'abstract'. A later section goes into depth on what 'abstract' means. For now, just suspend everything you think you know about abstraction, and everything you know about layers. In ALA, the hardware is never at the bottom.

Becasue this perspective doesn't really connect with anything you already know, we will just list three key takeaway points that will gradually become clearer as you reads other parts of this article.

. In ALA the only dependencies you are allowed are instances of abstractions (shown as green arrows on the diagram) and referred to as 'knowledge dependencies'.

. The first three layers are Application, Domain Abstractions, and Programming Paradigms (also referred to as Execution Models).

. As you go down, the abstractions in the layers are more abstract, and therefore more ubiquitous, more reusable, and more stable.


=== Executable Functional Requirements

If I had just two minutes to explain what ALA is, this is the perspective I would use: 

This first perspective puts the focus on your input information - the requirements. ALA is an architecture for directly expressing requirements. And having expressed those requirements, it is executable. Instead of having requirements capture and software as two documents with two sources of truth, ALA combines them into a single document and a single source of truth. 

BDD (Behavioural Driven Design) does something similar, but only achieves it for requirements and their tests. ALA goes one step further and makes the expressed requirements also the executable source code.


===== Executable Architecture

Although the ALA method is to express the requirements, this process produces the architecture or the design. (I do not make a distinction between architecture and design.) ALA isn't just executable functional requirements, it is executable architecture.

=== Agility

ALA is inherently optimally agile. By agile, we mean it's easy to change the functional requirements. ALA achieves this in its highest level separation of concerns. This primary separation is code that just expresses requirements from code concerned with any kind of implementation. The implementation code never has knowledge of any specific requirement, so it doesn't change as requirements change. Only the code that expresses requirements needs to change, and that code has no details other requirements and so is optimally minimal. ALA is the means and method of making this separation possible.

=== Abstractions and relations of their instances

If I had ten minutes to explain what ALA is, this is the perspective I would use: 

Consider this phrase, often found near definitions of software architecture.

[WARNING]
====
[red]#*elements*# and _their_ [red]#*relations*#.
====

Notice the word 'their', which I have italicised to emphasis that the relations are inferred to be between the said elements. It implies that the elements know something about each other. It implies they collaborate. This is really bad meme we are perpetuating as it is what leads to complex software. It is completely unnecessary. 

Here are some examples of this meme you can find in software engineering literature:

* From Wikipedia quoting from Clements, Paul; Felix Bachmann; Len Bass; David Garlan; James Ivers; Reed Little; Paulo Merson; Robert Nord; Judith Stafford (2010:
+
 "Each structure comprises software elements, relations among them, and properties of both elements and relations."

* IBM.com
+
 "Architecture is the fundamental organization of a system embodied in its components, their relationships to each other, and to the environment, and the principles guiding its design and evolution. [IEEE 1471]

* synopsys.com
+
 "Architecture also focuses on how the elements and components within a system interact with one another."

* From an article on coupling by Martin Fowler  https://www.martinfowler.com/ieeeSoftware/coupling.pdf
+
 "You can break a program into modules, but these modules will need to communicate in some way—otherwise, you’d just have multiple programs."

* High cohesion and loose coupling

This meme is well entrenched, and suggests that loose coupling is the best we can do. No, the best you can do is zero coupling. Here is how you do it. Simply change the wording of "elements and their relations" to the following.

[TIP]
====
[green]#*abstractions*# and [green]#*compositions*# of their [green]#*instances*#.
====

This seemingly subtle shift causes a complete change in the structure of the architecture, as described in the two contrasting diagrams below: 


==== The problem with "Elements and their relations"

An architecture based on "elements and their relations" generally turns out like this:

image::Slide8.jpg[Elements and their relations, title="Elements and their relations", align="center"]

The figure above shows five elements (as modules) and their relations (as interactions). Study almost any piece of software, and this is what you will find.

The structure generally can be viewed as 'clumping'. Like galaxies, certain areas have higher cohesion and other areas are loosely coupled, but the difference is only one of degree.

Software health in this type of architecture is effectively management of dependencies. This dependency management has two conflicting forces. One is the need to have interactions to make the modules work as a system. The other is to minimize the interactions to keep the modules as cohesive and loosely coupled as possible. As maintenance proceeds, the number of interactions inevitably increases, and the interfaces get fatter. the clumping is gradually compromised.

Various architectural styles are aimed at managing this conflict. Most notably:

* layering pattern
* MVC pattern
* Dependency rules
. Avoid circular dependencies.
. Avoid high fan-in and high fan-out on a single module.
. Avoid dependencies on unstable interfaces.

Note that none of this 'dependency management' really avoids circular coupling. To some extent there will always be 'implicit coupling' in this type of architectural structure, because inevitably the modules must collaborate. A function, for example, will be written to do what it's caller requires even if it shows no explicit dependency in its code. Circular dependencies may be avoided at compile-time, but will still be present at design-time, even if one or other of the modules has a good interface. That is why in the diagram above, dependencies are drawn from the insides of each of the modules. This indicates that the code inside has some inherent collaborative coupling.


==== Abstractions and relations of their instances

When you use abstractions instead of modules, the qualitative difference is that there are no interactions, no collaboration, no dependencies between your abstractions at all:

image::Slide9.jpg[Abstractions do not interact, title="Abstraction do not interact", align="center"]

The word 'modules' has been changed to the word 'abstractions'. All the dependencies are gone. And with them all their problems, and all their management. The implicit coupling that we talked about earlier is also gone. It not longer has a 'clumping' structure.

The obvious question now is how can the system work? Where do all the interactions we had before go? The answer is they go inside one additional abstraction:

image::Slide10.jpg[Relations of their instances, title="Relations of their instances", align="center"]

Interactions or collaboration should never be implemented in your abstractions. That just destroys them as abstractions. They are implemented inside another new abstraction at a different abstraction level, more specific to your application. Inside that new abstraction the interactions are encapsulated. They are no longer dependencies because they are no longer relations between abstractions. They are just a composition of instances.

At this point, some people are looking for the flaw in the argument. Surely those dependencies have just moved somewhere else? How can we have got this so wrong for so long. 

Consider two well established software abstractions. Lets pick and if statement and a an assignment statement. You would have often composed together two instances of these to make a specific user story. The abstractions "if statement" and "assignment statement" don't have any knowledge of each other. The interaction between the instances only happens inside your module, the more specific one that knows about the user story. 

Now imagine that you didn't have the the if statement and the assignment statement abstractions when you started. So you started writing an if statement module. Then, from inside that module, you connected it to your assignment statement module. And the assignment statement module, you just made it do what the if statement module needed, just a specific variable. Imagine the opportunity you missed to invent those two abstractions. The two modules you wrote are useless for anything else. And to understand the user story you must understand the insides of both of them, because they collaborate. 

But if you invent the two abstractions, the user story, being just a simple composition of instances of the two, is extraordinarily simple. And the abstractions themselves know nothing of each other, so each of them is simple too. This is the opportunity we are missing, all the time everywhere, just because of the wording of those memes we listed above.

Imagine the impact this change has. Instead of managing dependencies, which has been the primary focus of software engineering, they are completely gone! We have only one type of dependency left - "an instance of an abstraction". This dependency type turns out to be a good one, a very good one, for it is the one our brains use to understand complicated things. And it turns out that it's just enough dependency to make the system work. Software engineering should not be 'managing dependencies'. It should be 'inventing abstractions'.  

=== Real world metaphors

==== Atoms and molecules

Here are two atom abstractions:
image:oxygen.png[Oxygen atom, 200, title="Oxygen atom"]
image:hydrogen.png[Hydrogen atom, 200, title="Hydrogen atom"]

Instances can be composed to make a molecule:
image:water_molecule.jpg[Water molecule, 300, title="Water molecule"]


If water was implemented in the same way we typically write software, there would be no water molecule, and the oxygen molecule would be modified to instantiate hydrogen atoms and interact with them. Even if dependency injection is used to avoid the instantiating, it is still unlikely that a water abstraction would be invented to do that, and there still be the problem of the oxygen module being modified to interact with hydrogen's interface. Either way, the oxygen module ends up with some implicit knowledge of hydrogen. And hydrogen probably ends up with some implicit knowledge of oxygen in providing what it needs. 

This implicit knowledge is represented by the following diagram. The lines are shown coming from the inner parts of the modules to represent implicit knowledge of each other.



While oxygen and hydrogen are modules, as abstractions they are destroyed because oxygen now comes with two hydrogen and cant be used as a building block type for any other types of modules.

To keep the oxygen as abstract as it is in the real world, an interface must be conceived that is even more abstract than oxygen or hydrogen. In the molecule world this is called a polar bond.

The corresponding software would look like this:


image::Slide15.jpg[, title="", align="center"]

The water molecule has "uses instances of" relationship with the two atoms, and the atoms have "uses instances of" relationships with the even more ubiquitous polar bond. Polar bond is an axample of what we call an 'abstract interface'.

==== Lego

The second real world meatphor is Lego. Shown in the image below is the same three layers we had above for molecules, atoms and bonding types.

image::Slide16.jpg[, title="", align="center"]

The abstractions are the various lego pieces, instances of which can be assembled together to make things. Lego pieces themselves have instances of an abstract interface, which is the stud and tube. There is a second abstract interface, the axle and hole. We also call the abstract interface the 'execution model' and here with the lego metaphor we start to see why it can be thought of in this way - when the model roles, the axle and hole interface executes.

==== Electronic schematic

The third real world metaphor comes from electronics. The abstractions are electronic parts, instances of which can be composed as a schematic diagram:  

image::Slide17.jpg[, title="", align="center"]

In this domain, the abstract interfaces (execution models) are both digital signals analog voltage levels.

==== A clock

The forth and final real world metaphor is a clock. Here we show composition of abstractions to make new abstractions as a process. The process is a circle because instances of the new abstraction can themselves be used to make still more specific abstractions. Each time around the circle adds one layer to the abstraction layering.

image::Slide18.jpg[, title="", align="center"]

To go round the circle once, we start with abstract versions parts such as cog wheels and hands. Instances of these have abstract ways they can interact, such as fitting on axles and meshing teeth. The next step is to instantiate some of these abstractions and configure them. For example, configure the size and number of teeth of the cog wheels. Next comes the composition step, where they are assembled. Finally we have a new abstraction, the clock. Instances of them can be used to know when to do things during your day, but that is a whole different abstraction. 

There are many other instances of this pattern in the real world, and in nature. In fact almost everything is composed in this way, or is understood by such composition.

=== Example project - Ten-pin bowling

The ten-pin bowling problem is a common coding kata. Usually the problem is just to return the total score, but in this example project we will tackle the bigger problem of keeping the score required for a real scorecard, which means we need to keep all the individual frame ball scores:


[plantuml,file="bowling_scorecard2.png"]
----
@startditaa --no-separation --no-shadows
/-----+-----+-----+-----+-----+-----+-----+-----+-----+--------\
|   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |    10  |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
| 1| 4| 4| 5| 6| /| 5| /|  | X| -| 1| 7| /| 6| /|  | X| 2| /| 6|
+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+--+
|   5 |  14 |  29 |  49 |  60 |  61 |  77 |  97 | 117 |   133  |
\-----+-----+-----+-----+-----+-----+-----+-----+-----+--------/
@endditaa
----


The ALA method starts by "expressing the requirements in terms of abstractions that you invent". When we start expressing requirements of ten-pin bowling, we immediately find that "a game consists of multiple frames", and a "frame consists of multiple balls". Let's invent an abstraction to express that "multiplicity". Lets call it a "Frame".  Instances of Frame can be wired together to make a "ConsistsOf" relationship. So lets invent an abstract interface to represent that, and call it 'IConsistsOf'.

Here is the diagram of what we have so far.

////
[plantuml,file="bowling.png"]
----
@startditaa --no-separation --no-shadows utf-8

 nFrames==10     score==10 || nBalls==2
   |              |
   v              v
+-----+        +-----+
|     |        |     |
|Frame|------->|Frame|
|     |        |     |    
+-----+        +-----+
@endditaa
----
////

[plantuml,file="diagram-bowling-1.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
frame [label="Frame|\"frame\"|balls==2 \|\| pins==10"]
ball [label="SinglePlay"]
game -> frame -> ball [label = "ConsistsOf"]
}
}
@enddot
----

The Frame abstraction is configured with a lambda function to tell it when to stop. These lambda functions are written inside the Frame instances. A Frame will tell its parent it is complete when the lambda expression is true and the last child Frame is complete. 

The idea of the Frame abstraction is that at run-time it will form a composite pattern when as runs. As each down-stream child frame completes, a Frame will copy it to start a new one. This will form a tree structure where the "game" will end up with 10 "frames", and the frame will end up with one, two or three SinglePlays. In other words, it will form a tree. 

The end of the chain is terminated with a leaf abstraction that also implements the 'IConsistsof' interface called 'SinglePlay'. It represents the most indivisible play of a game, which in bowling is a throw of a ball. Its job is to record the individual throw score. 

Note, in reference to the ALA layers, this diagram sits in the top layer, the Application layer. The boxes are instances of abstractions that come from the second layer, the Domain Abstractions layer. The arrows are instances of the Programming Paradigm, 'InConsistsOf', which comes from the third layer, the Programming Paradigms layer.  

This diagram will score 10 frames of ten-pin bowling but does not yet handle strikes and spares.

So lets do some 'maintenance' of our application. Because the application consists of simple abstractions, which are inherently stable, maintenance should be easy without spoiling the abstractions.

The way a ten-pin bowling scorecard works, bonuses are scored in a different way for the last frame than the first 9 frames. In the last frame, they are shown as scores on the scorecard, so there are up to 3 balls in that frame. In the first nine frames, the bonus balls come from following frames, and just appear added to the previous frames total. 

To handle the last frame, we just need to modify the completion lambda expression. To handle the first 9 frames, we introduce a new abstraction. Lets call it Bonuses. Although we are inventing it first for the game of ten-pin bowling, it is important to think of it as a general purpose, potentially reusable abstraction.

What the Bonus abstraction does is, after its child frame completes, it continues adding plays to the score until its lambda function returns true.

The completed ten-pin bowling scorer is this:

[plantuml,file="diagram-bowling-2.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
bonus [label="Bonus||score\<10 \|\| plays==3"]
frame [label="Frame|\"frame\"|frameNum\<9 && (balls==2 \|\| pins==10)\n \|\|\ (balls==2 && pins\<10 \|\| balls==3)"]
ball [label="SinglePlay"]
game -> bonus -> frame -> ball
}
}
@enddot
----

At the moment, because there are no automated tools for converting such diagrams to code, we do it manually, which is a pretty mechanical step, and we get the code below:

....
private IConsistsOf game = new Frame("game")
    .setIsFrameCompleteLambda((gameNumber, frames, score) => frames==10)
    .WireTo(new Bonuses("bonus")
        .setIsBonusesCompleteLambda((plays, score) => score<10 || plays==3)
        .WireTo(new Frame("frame")
            .setIsFrameCompleteLambda((frameNumber, balls, pins) => frameNumber<9 && (balls==2 || pins[0]==10) || (balls==2 && pins[0]<10 || balls == 3))
            .WireTo(new SinglePlay("SinglePlay")
    )));
....

So far, this has been a fairly top-down, waterfall-like approach. We have something that represents all the details of the requirements, but we haven't considered implementation at all. Past experience tells us this may lead us into dangerous territory. Will the devil be in the details? Will the design have to change once we start implementing the abstractions? The first few times I did this, I was unsure. I was not even sure it could actually be made to work. Is it just a simple matter of writing classes for those three abstractions and the whole thing comes to life? 

I can only answer that question from experience: In all the examples and projects I have do so far, the system has worked, and implementation of the abstractions has not caused their design, or the application layer to change. I think it is because of the use of abstractions. Abstractions are naturally decoupled. The application design leaves out no details except inside abstractions. These details inside abstractions are decoupled at design-time.

The hard architectural work is done. Implementing the three abstractions turns out to be straightforward.

Here is the IConsistOf interface:

....
    public interface IConsistsOf
    {
        void Ball(int score);
        bool IsComplete();
        int GetScore();
        int GetnPlays();
        IConsistsOf GetCopy(int frameNumber);
        List<IConsistsOf> GetSubFrames();
    }
....

The first four methods are fairly obvious. The Ball method receives the score on a play. The Complete, GetScore and GetnPlays methods return the state of the sub-part of the game. The GetCopy method asks the object to return a copy of itself (prototype pattern). When a child frame completes, we will call this on it to get another one. The GetSubFrames method is there to allow getting the scores from individual parts of the game if this is required.

Now let's do the interesting parts of the Frame abstraction. It has one 'state' variable which is the list of subframes. This is the composite pattern we referred to earlier, and what ends up forming the tree.

....
private List<IConsistsOf> subFrames;
private readonly Func<int, int, int, bool> isFrameComplete;
private readonly int frameNumber = 0;
....

The second variable is the lambda expression that is passed to us by the application. It would be readonly (immutable) except that I wanted to user a setter to pass it in, not the constructor, to indicate optional configuration. 

The third variable is the frameNumber. It allows frame objects to know which one they are to their parent -1st child, 2nd child etc. This value is passed to the lambda expression in case it wants to use it. For example, lambda expression for a bowling frame needs to know if it is frame number 9.  

The methods of the IConsistsOf interface are now straightforward to write. Lets go over a few of them to get the idea. Here is the Ball method implementation for Frame:

....
public void Ball(int player, int score)
{
    // 1. Check if we are complete, and do nothing
    // 2. See if the last subframe is complete, if so, start a new subframe
    // 3. Pass the ball score to all subframes

    if (IsComplete()) return;

    if (subFrames.Count==0 || subFrames.Last().IsComplete())
    {
        subFrames.Add(downstream.GetCopy(subFrames.Count)); 
    }

    foreach (IConsistsOf s in subFrames)
    {
        s.Ball(player, score);
    }
}
....

It just has to pass on the ball score to all the child objects. Any that have completed will ignore it. Then it looks to see if the last child has completed, and if so starts a new child.

The IsComplete method checks if the last child object is complete and the lambda expression also says our local frame is complete:

....
private bool IsComplete()
{
    if (subFrames.Count == 0) return false; // no plays yet
    return (subFrames.Last().IsComplete()) && 
        (isLambdaComplete == null || isLambdaComplete(frameNumber, GetnPlays(), GetScore()));
}
....

....

....

GetScore simply gets the sum of the scores of all the child objects:


....
private int GetScore()
{
    return subFrames.Select(sf => sf.GetScore()).Sum();
}
....

The GetCopy method must make a copy of ourself. This is where the prototype pattern is used. This involves making a copy of our child as well. We will be given a new frameNumber by our parent.

....
IConsistsOf GetCopy(int frameNumber)
{
    var gf = new Frame(frameNumber);
    gf.objectName = this.objectName;
    gf.subFrames = new List<IConsistsOf>();
    gf.downstream = downstream.GetCopy(0);
    gf.isLambdaComplete = this.isLambdaComplete;
    return gf as IConsistsOf;
}
....

The few remaining methods of the IConsistOf interface are trivial. The implementation of IConsistsOf for the other two abstractions, SinglePlay and Bonuses, is similarly straightforward. Note that whereas Frame uses the composite pattern, Bonuses uses the decorator pattern. It implements and requires the IConsistsOf interface. The SinglePlay abstraction, being a leaf abstraction, only implements the IConsistsOf interface. 

One method we haven't discussed is the wireTo method that we used extensively in the application code to wire together instances of our abstractions. The wireTo method for Frame is shown below:  

....
public Frame WireTo(IConsistsOf c)
{
    subFrames = new List<IConsistsOf>();
    subFrames.Add(c);
    return this;
}
....

This method does not need to be implemented in every domain abstraction. I use an extension method for wireTo. The WireTo extension method uses reflection to find the local variable to assign the object being wired.

The WireTo method will turn out to be useful in many ALA designs. Remember in ALA we "express requirements by composing instances of abstractions". If the 'instances' of 'abstractions' are implemented as 'objects' of 'classes', then we will use the wireTo method. If the 'instances' of 'abstractions' are 'invocations' of 'functions', as we did in example project in pass 1, we wont use WireTo obviously. In the coffeemaker example to come, 'instances' of 'abstractions' are 'references' to 'modules' because a given application would only have one of each abstraction.

The wireTo method returns 'this', which is what allows the fluent coding style used in the application code. The configuration setter methods also return a this reference so that they too can be used in the fluent style. 

The full source code for the bowling application can be downloaded from here: https://github.com/johnspray74/GameScoring[GameScoring code]


=== Example project - Tennis

Now let's modify the bowling application to score tennis. If the bowling game hadn't been implemented using ALA, you probably wouldn't contemplate doing this. But ALA has provided us with domain abstractions and a useful programming paradigm for the domain of game scoring. That Frame abstraction looks like it could be used for the match, which consists of sets, for the set which consists of games, and for the game which consists of SinglePlays.

We will need to make a small generalization to the Frame abstraction first. This will allow it to keep score for two players. We just change the type of the score from int to int[]. The Ball method will be generalized to take a player parameter to indicate which player won a play.

The only other thing we will need to do is invent a new abstraction to convert a score such as 6,4 into a score like 1,0, because, for example, the winner of a game takes one point into the set score. This new abstraction is called WinnerTakesPoint (WTP in the diagram). 

Here is the tennis scoring game:

[plantuml,file="tennis1.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
match -> wtp1 -> set -> wtp2 -> game -> play
// }
}
@enddot
----

The diagram expresses all the details of the requirements of tennis except the tiebreak.

Here is the diagram's corresponding code:

....
private IConsistsOf match = new Frame()
    .setIsFrameCompleteLambda((matchNumber, nSets, score) => score.Max()==3)
    .WireTo(new WinnerTakesPoint()
        .WireTo(new Frame()                     
            .setIsFrameCompleteLambda((setNumber, nGames, score) => score.Max()>=6 && Math.Abs(score[0]-score[1])>=2)
            .WireTo(new WinnerTakesPoint()
                .WireTo(new Frame()          
                    .setIsFrameCompleteLambda((gameNumber, nBalls, score) => score.Max()>=4 && Math.Abs(score[0]-score[1])>=2) 
                    .WireTo(new SinglePlay()))))));
....

Again, the new WinnerTakesPoint abstraction is easy to write. It is a decorator that implements and requires the IConsistsOf interface. Most methods pass through except the GetScore, which returns 0,0 until the down-stream object completes, then it returns either 1,0 or 0,1 depending on which player has the higher score.

Now let's switch our attention back to another example of maintenance. Lets add the tiebreak feature. Another instance of Frame will score the tiebreak quite nicely. However we will need an abstraction that can switch us from playing the set to the tie break. Let's call it Switch, and give it a lambda function to configure it with when to switch to the tiebreak. Switch returns the sum of scores of its two downstream objects. Here then is the full expression of the requirements of the tennis scorer:


[plantuml,file="tennis2.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
switch [label="Switch||(setNumber\<4 &&\n score[0]==6 && score[1]==6"]
wtp3 [label="WTP"]
tiebreak [label="Frame|\"tiebreak\"|score.Max()==7"]
play2 [label="SinglePlay"]
match -> wtp1 -> switch -> set -> wtp2 -> game -> play
switch:s -> wtp3:w
wtp3 -> tiebreak -> play2
{rank=same set wtp3}
// }
}
@enddot
----

And here is the code version. This application passes an exhastive set of tests for the scoring of tennis.

....
private IConsistsOf match = new Frame("match")
    .setIsFrameCompleteLambda((matchNumber, nSets, score) => score.Max()==3)
    .WireTo(new WinnerTakesPoint("winnerOfSet")
        .WireTo(new Switch("switch")
            .setSwitchLambda((setNumber, nGames, score) => (setNumber<4 && score[0]==6 && score[1]==6))   
            .WireTo(new Frame("set")                     
                .setIsFrameCompleteLambda((setNumber, nGames, score) => score.Max()>=6 && Math.Abs(score[0]-score[1])>=2)
                .WireTo(new WinnerTakesPoint("winnerOfGame")            
                    .WireTo(new Frame("game")          
                        .setIsFrameCompleteLambda((gameNumber, nBalls, score) => score.Max()>=4 && Math.Abs(score[0]-score[1])>=2) 
                        .WireTo(new SinglePlay("singlePlayGame"))
                    )
                )
            )
            .WireTo(new WinnerTakesPoint("winnerOfTieBreak")
                .WireTo(new Frame("tiebreak")          
                    .setIsFrameCompleteLambda((setNumber, nBalls, score) => score.Max()==7)
                    .WireTo(new SinglePlay("singlePlayTiebreak"))
            )
        )
    )
);
....

Notice that I have added string names to the instances of Frame and other objects. This is not required to make the program function, but generally is a good habit to get into in ALA. It is because in ALA we typically use multiple instances of abstractions in different parts of the program. The names give us a way of identifying the different objects during any debugging. Using them I can Console.Writeline debugging information depending on the object's name.

So there you have it. Around 8 lines of code express the scoring requirements of ten-pin bowling and around 15 lines of code express the scoring requirements of tennis. That sounds about right for the inherent complexity of the two games. The two scorers actually execute and pass a large battery of tests. 

The domain abstractions are zero-coupled with one another, and are each straightforward to write by just implementing the methods of the IConsistOf interface according to what the abstraction does. The abstractions are simple and stable. So no part of the program is more complex than its own local part.

The domain abstractions are reusable in the domain of game scoring.

And, most importantly, the experience was that as the details inside the abstractions were implemented, the application design didn't have to change. Here is a link to the code on Github: https://github.com/johnspray74/GameScoring[GameScoring code]

Why two applications? The reason for doing two applications in this example is to emphasis where all the details of the requirements end up. The only difference between the bowling and tennis applications is the two diagrams, which are translated into two code files: bowling.cs and tennis.cs. These two files completely express the detailed requirements of their respective games. No other source files have any knowledge of these specific games. Furthermore, Bowling.cs and Tennis.cs do not do anything other than express requirements (in an executable way). All implementation to actually make it execute is hidden in domain abstractions and programming paradigm abstractions. 

== Pass three - Why ALA Works


=== Run-time and Design-time dependencies

We can distinguish two types of dependencies. On one hand we have run-time dependencies. These are dependencies in the code that are there because one module will need another module to be present at run-time, usually to communicate data or events. On the other hand we have design-time dependencies. These are dependencies in the code needed to understand a module that is using another. The dependency is on knowledge you need to read the code. I will often refer to this type as a "knowledge dependency". It is also sometimes called "semantic coupling".

A simple example of a run-time dependency is a display module calling an ADC module to get the data to display. A simple example of a knowledge dependency is some code that calculates a standard deviation using a squareroot function. You must have knowledge of what the squareroot function does before you can understand the standard deviation function. But the display module's code does not need knowledge of an ADC to be understood.

We find both types of dependencies in our code. Whether a knowledge dependency or a run-time dependency, they all just look like a function call or a 'new' keyword in our code (or a reference to an out-scope variable). We generally don't distiquish between them. They are all just called dependencies. We lump them together when we talk about dependency management. 

It turns out that knowledge dependencies are good if they are on good abstractions. All run-time dependencies are bad.

[TIP]
====
[green]#*In ALA we eliminate all run-time dependencies*#.
====

That's right - in ALA we eliminate all run-time dependencies!

In the diagram below, there are two run-time dependencies.

[plantuml,file="dependency-diagram.png"]
----
@startditaa --no-separation --no-shadows --scale 1.1

   <----"Depends On"

+----+     +----+     +----+
|    |     |    |     |    |
| A  |<----| B  |<----| C  |
|    |     |    |     |    |
+----+     +----+     +----+

  upstream      downstream
  
    <----         ---->
@endditaa
----

Because these are only run-time dependencies, they should not affect our ability to understand the code inside A, B or C at design-time. But they do because they are dependencies. In other words, to understand C, we  need to know about B. C will contain code that is collaborating with B. And similarly B will contain code that is collaborating with A.

Although there are no compile-time dependencies in the reverse direction, it is likely that A contains code that is collaborating with B (doing what it needs) and B will contain code that is collaborating with C. In other words there is a lot of implicit coupling, which we also consider to be run-time dependencies.

A typical program is chock full of these run-time dependencies. 

We can list the following consequences:

* B's function name will relate to what B does, and so A must know B's name. 

* Since there is now a fixed arrangement between C and B, they are much more likely to be collaborating, meaning that C is making implicit assumptions about what B does.

* Although there is no explicit compile-time dependency from B on C, the fixed arrangement is also likely to make B collaborate with C, making it have a design-time dependency on C.

* The fixed arrangement between A, B and C is not obvious. It is buried inside of C and B (Remember there is no diagram like we have above, or if there is, it is a second source of truth and is likely out of date and lacking in details).

* The fixed arrangement between A, B and C makes it difficult to reuse B without A or C. B and C cannot be abstractions.

* The fixed arrangement between A, B and C makes it difficult to insert another operator between A and B or between B and C. 

* If the observer pattern is used, so that the compile-time dependency from C to B is reversed, it only mirrors the same problems. And because it adds indirection, the observer pattern actually makes the program even harder to understand.

* If dependency injection is used with automatic wiring, the problems remain. C and B still have a fixed arrangement, will still be collaborating, and will have a specific interface between them. And again the additional indirection will only make the program even harder to understand.

During code creation, run-time dependencies are easily introduced, and never seem too terrible at the time. But when there are tens or hundreds of them, that's when they turn into a big ball of mud.

As mentioned above, in ALA we elliminate all run-time dependencies, including all the collaboration. While it uses things like dependency injection, observer (publish/subscribe) pattern, callbacks, or the like, they don't solve the problem by themselves. Something more is required. 

To completely remove a run-time dependency, the two modules must know nothing about each other. Zero. Zilch. They each do a job oblivious to the other. Abstractions do that. Something else, lets call it the 'application' knows that in a particular application, A, B and C will talk to each other at run-time, or put more accurately, instances of A, B and C will talk to each other. This application will use instances of A, B and C and either move the data between them itself, or wire them together. That wiring will be explicit, and in one place. It will not just be buried inside of A, B and C.  

The application will now have design-time dependencies on A, B and C. This is natural and good, for A, B and C will be good abstractions, and it is at design-time that we want to understand how we are interconnecting instances of A, B and C in a coherent to create a particular application. 

The final part of the solution, if we are using wiring, is that the interfaces between modules are not specific to them as pairs. These are called abstract interfaces. The interface design is such that there could potentially be many abstractions that either implement it or accept it. Then the application has choices about how it combines abstractions up the instances. This is called compositionality. 

When using the observer pattern, a 3rd party, the application, does the subscribing for the receiver. The receiver must not do the subscribing.

When using dependency injection, the wiring must not be automatic. It must not just grab objects from a container that matches interfaces. This would make the wiring implicit and the application even more difficult to understand. It will also encourage specific interfaces for pairs of classes, which is bad. I prefer not to use XML for wiring either. XML is not very readable, and it only handles tree structures well. If you must use text, it should be normal code. But I much prefer a diagram, and there are other reasons why diagrams work way better than code at the application layer. I will go into these in a later section.

When you structure code in this way, all the run-time dependencies have gone. You may be wondering, where did they go? How can the program work without them? Therefore, haven't I just moved them somewhere else? No. As dependencies, they are gone. A, B and C now know nothing whatsoever about each other.

The knowledge that at run-time instances of A, B and C will talk to each other is still there, just not as dependencies. That knowledge is now  completely contained inside one module. The only dependencies you end up with are good design-time dependencies:

. Dependencies from the application to the domain abstractions whose instances it is using. An application 'knows' at design-time what abstractions it needs.

. Dependencies from the abstractions to the abstract interfaces they implement or accept. Abstractions know at design-time the interface they need for their I/O. 

=== "Express requirements by composing abstractions"

If I had twenty minutes to explain ALA, I would put it this way. 

A common software architecture design approach is stated something like this:

[WARNING]
====
The [red]#*decomposition*# of the [red]#*system*# into [red]#*elements*# and their [red]#*interactions*#.
====

In ALA, the design approach is instead stated this way:

[TIP]
====
The [green]#*expression*# of the [green]#*requirements*# by [green]#*composition*# of [green]#*abstractions*#.
====

All four big words are changed and some are exact opposites. Indeed, the architecture comes out almost "inside out".

Rather than try to just explain the "composition or abstractions" approach, I am going to try to relate these two structures to each other, one difference at a time. 

==== Decomposition of the system into elements and their interactions

First, let's consider a representation of a decomposition structure. The outer box is the system. It shows decomposition into four elements, and then those in turn are decomposed into four elements each. 

image::Slide11.jpg[Decomposition structure, title="Decomposition Structure", align="center"]

The elements are labelled with numbers to emphasise that they are not very good abstractions. Of course, in practice these elements are abstract to a point (they have a name), but abstraction is relative. 

The next diagram shows the same structure but with parts relevant to a user story marked in red. This is the "their interactions" part of the "The decomposition of your system into elements and their interactions".

image::Slide13.jpg[Decomposition structure, title="Tracing a User story", align="center"]

==== Expression of the requirements by composition of abstractions

The next diagram is a representation of the "Composition approach" structure, which we will use to compare the two structures. 

image::Slide14.jpg[Decomposition structure, title="Composition Structure", align="center"]

Only one 'composition' relationship is shown from the top layer to the second layer, the one from [underline]#c# to C. In practice we wouldn't normally draw them at all. We wouldn't even draw a diagram like this - the abstractions would be just referred to by name. Normally only the parts inside the User Story abstractions would be drawn and shown. But here we are trying to make a combined diagram of the meta-architecture and the specific architecture. The meta-architecture is the three layers, what goes in them, and the knowledge dependencies on abstractions in lower layers. The specific architecture is the user stories containing a composition of instances.

==== Comparison of the two approaches


.Comparison of two approaches
[width="100%",options="header,footer"]
|====================
| Decomposition | Composition
| image:Slide13.jpg[Decomposition structure, title="Tracing a User story", align="center"] | image:Slide14.jpg[Decomposition structure, title="Composition Structure", align="center"]
|Hierarchical (fractal) structure |  Layered structure
|Elements become less abstract as you zoom in. They are specific parts of specific parts. | Parts become more abstract. They get more abstract as you go down the layers. 
| Because inner elements are less abstract, the strategy is to hide their public interfaces by encapsulation. | We want the parts  to be more and more public. They are things to know about and reuse.
| The dependencies go in the direction from the outermost element to the innermost, in the direction of the less abstract. | The dependencies go in the direction of the more abstract. We put these more abstract pieces into lower layers so that dependencies go down.
| Encourages the same element to be used for both abstraction and instance - often called a module or component. | Has two distinct types of elements - abstractions and instances.  
| Elements are loosely coupled (very bad) | Abstractions are zero coupled (within each layer). The only relationships allowed between abstractions are knowledge dependencies on an abstraction (down the layers).
| Hides details through encapsulation. Encapsulation only hides details at compile-time - not readily visible to the brain at design-time. | Hides details through abstraction. Abstractions hide details at design-time, because they are the modules of the brain.
| Discourages reuse. 16 elements all different from each other. | Encourages reuse. Only 5 abstractions. 16 instances of those five abstractions. 
| SMITA - Structure missing in the action. If you are interested in a particular user story, you will typically have to trace it through multiple elements, multiple interfaces, and their interactions across the structure. An example of this is shown by the diagram with the red lines. | Eliminates this problem. The structure is explicit and in one place.
| Coupling increases during maintenance. This is because details are not hidden inside abstractions, only encapsulations. Any of them can be needed at any time by an outer part of the structure. So as maintenance proceeds, more of them will need to be brought into the interfaces, increasing the coupling as time goes on. | Coupling between abstractions is zero, and we respect that constraint during maintenance. Coupling does not increase. The one type of coupling allowed between abstractions, the knowledge dependencies that go down the limited number of layers, are good dependencies as they are what makes creativity possible. During maintenance, an operation called generalizing an abstraction is sometimes done. This increases the versatility, reuse and ubiquity of the abstraction, making it more abstract. 
| Because of the loose coupling, the complexity of the decomposition structure increases as the system gets larger. | The complexity stays constant as the system gets larger. This is because abstractions are zero coupled. Each abstraction is its own stand-alone program. If we choose an ideal granularity of say 200 lines of code, the complexity in any one part of the program is that of 200 lines of code.  
| The maintenance cost (effort per user story or effort per change) increases over time. This is because complexity is increasing. Changes will tend to have ripple effects, but that isn't the biggest problem. Even if a change ends up being in one local scope, reasoning about the system to determine where that change should be is hard. | The maintenance cost reduces as the system grows. This is because as the domain abstractions mature, the user stories become less and less work to do, as they simply compose, configure and wire together instances of abstractions.  
|====================


==== Transforming a decomposition structure into a composition structure

* Hierarchical levels of encapsulation turn into layers.
* Elements are taken outside of their respective encapsulations. Parts of them become programming paradigm abstractions, some parts become domain abstractions, and some parts become application specific configuration of instances of those abstractions. 

==== Why is the decomposition approach so prevalent?

The decomposition approach is often the defacto or informal method used by developers. It is also encouraged by many architecture styles and patterns, for example layering or MVC. It is the method used in ADD (Attribute Driven Design). Indeed it is sometimes described as the very definition of what software architecture is.

The intended idea is that the decomposed elements are easier problems to solve. Keep decomposing in a fractal manner and eventually the elements are small enough to be solvable.

I hope with the table above that I have successfully argued that it is a flawed argument. 

=== Expression of requirements

The other half of our statement "The expression of the requirements by composition of abstractions" concerns "Expression of requirements."

In conventional software development, we first break a user story (or functional requirement) up into different implementation responsibilities. These are often layers like GUI, business logic and database, or another pattern such as Model, View, Controller (MVC). 

In ALA we break up the code in a different way. It is because the user story, as written in the requirements, is already in itself cohesive, and should *_not_* be broken up. That cohesive knowledge is at a different, more specific abstraction level than what all the other code should be. We want to keep that requirements-specific, cohesive knowledge together in one place. We also want that code to represent the user story with about the same level of expressiveness as when the user story was explained in English by the product owner. After all, the language he used would have contained domain specific terms so it could be explained concisely. The same thing ought to be possible in the code that represents the user story. Any knowledge that does not come directly from the requirement specification is separated out. It comes out into separate abstractions. These abstractions typically contain knowledge of how things are implemented. But not how a specific user story is implemented - how different aspects of typical user stories are implemented. That is the way to separate programs into parts.

Now it usually turns out that these abstractions that know how to implement things useful to user stories are reusable across both user stories and applications in a domain. In other words, they are domain level abstractions. A typical user story might use several of them, some to implement the user story's UI, some to implement the user story's business, and some to implement the user story's data. A user story instantiates the abstractions referred to in the requirements, configures them with the specific knowledge from the requirement, and then wires them together. 

==== SMITA (Structure Missing in the Action)

The maintainability quality attribute is often thought of in terms of ripple effects of change. While that is certainly a problem for maintainability, it is not the biggest problem. The biggest problem is the time you have to spend understanding enough of the system structure to know where to make the changes and be confident about their impact. Even if the change, in the final analysis, is one line of code, the potential for ripple effects must be understood. So you have to understand enough of the relationships and coupling to that one line in the system's structure to be confident you can make the change safely.

The problem in most large code bases is that the system structure is not explicit. It is distributed inside the modules themselves. Or if there is any collaboration between modules, it is implicitly hidden inside multiple modules. Finding this structure, even for a single user story can be time consuming. I have often spent a whole day doing that, just to change that one line of code, and many developers I have spoken to can identify with this experience.

It can get a lot worse as the system gets larger. In a bizarre twist, the more loosely coupled you make the elements, the harder it gets to trace a user story through them (because of indirections). Some people conclude that loose coupling and explicit relationships between the elements involved in a user-story are naturally in conflict.

I call this situation SMITA (Structure Missing in the Action). This hidden structure is sometimes partially brought out as a model, a sort of high-level documentation of the internal structure. But such models are a secondary source of truth.

ALA completely eliminates this conflict. The structure of a user story is explicitly coded in one place, without any indirections. Yet the abstractions are zero-coupled. 

anchor:DSL1[]

==== DSLs

ALA's succinct expression of requirements sounds similar to the way requirements might be represented in a DSL (Domain Specific Language). Under the broader definition of a DSL the domain abstractions layer is a DSL. But ALA is more than just a DSL. ALA, as its name suggests, is fundamentally about layering of abstractions. It layers them in a small number of layers, according to their abstraction level. When you do this, the top two layers emerge as the specific application and the domain level. Therefore ALA happens to converge on the same solution as DSLs for these top two layers.

In coming to this same solution from a different direction it has a different emphasis than a DSL has. It does not pursue the idea of an external DSLs, nor even the syntactic elegance of DSLs. It doesn't move application development away from the developer as DSLs can. You don't get a different language such as XAML that a UI specialist designer can learn. These things may still be desirable qualities and ALA does not preclude them, it is just not what ALA does. ALA says that just getting the abstraction layering right is enough to deal with complexity and ongoing reducing effort for maintainability.

As a DSL, in ALA you usually just wire together plain old objects, or functions. Their classes (the domain abstractions) and their 3rd layer interfaces collectively form the DSL. The grammar is defined by which classes use which interfaces. This sets the rules for what types of objects can be wired together.

By the way, ALA also emerges other already discovered architectural styles such as CBE (Component Based Engineering), and compositionality, and these are discussed later.

=== Example Project - Coffee machine

Before continuing with the full explanation of how ALA works, we will look at a small example that was used as an early ALA experiment. It needs a little cleaning up, and you can't get the full idea of ALA from one example, but it is still a useful small example to start with. 

Robert Martin posed an interesting pedagogical sized embedded system problem about a coffee maker in his book “Agile Software Development: Principles, Patterns and Practices”. The original chapter is called “Heuristics and Coffee”.

Most solutions to his problem that you can find on the Internet are quite complicated. Here is an ALA solution.

ALA is about paying attention to the knowledge you need to read code in higher layers, called a knowledge dependency, so before presenting the application that sits in the highest layer, we first familiarise ourselves with the abstractions we need from the domain layer, and the layer below that, which is called the Programming Paradigms layer.

==== Domain layer

In the Domain Abstractions layer are these three domain abstractions:

image::Coffee%20Maker%20Domain%20Components.vsd.jpg[CoffeeMaker Domain Abstractions, title="Coffee maker domain abstractions"]

Take a moment to look at these three abstractions. The UI has a lamp you can control, and a push button which outputs an event. The WarmerPlate tells you whether or not the pot is on the warmer plate, and whether or not it is empty. It controls its own heater. The Boiler can be turned on or off. It will tell you when it is empty of water. And you can stop water flow instantly with a steam release valve. It will turn its own heater off if it runs out of water, or the valve is opened. That's all there is to the three abstractions that reside in the Domain Abstractions layer.

==== Programming Paradigms layer

We haven't talked about the Programming Paradigms Layer of ALA yet. Suffice it to say it contains what we might refer to by any of the terms: 'execution models', 'programming paradigms', 'paradigm interfaces', or 'abstract interfaces'. They are interfaces used to wire together the domain abstractions in arbitrary ways. Each gives a different meaning to the lines we will use in the applciation diagram.  

Generally we need multiple programming models for an application. For the coffeemaker we will use three

. live-data-flow model (works like an electronic circuit)
. events
. simple state machine

The API for the Programming Paradigms layer is the key on the right of the diagram below. It gives you all the knowledge from the layer to be able to read the diagram. So, for example, a solid line is a data-flow; the rounded box is state with the states enumerated inside it.

The details of how to turn the diagram into code is explained in a project document, also provided in the Programming Paradigms layer.

==== Application layer

Now that we have understood the knowledge dependencies in all lower layers, we can read the diagram that resides in the top layer, the application layer.

==== Reading the code

image::Coffee%20Maker%20Dataflow%20diagram.vsd.jpg[CoffeeMaker Dataflow diagram, title="Coffee maker solution"]

The diagram to the left is the application. The three domain abstractions, UI, Boiler and Warmer plate are instances of abstractions in the Domain Abstractions layer.

Follow me now as we go through the user stories by looking at the lines on the diagram:

* When the UI push button is pressed, we set the state to Brewing, provided the Boiler has water and the pot is on the Warmerplate. 

* When the state is brewing, it turns on the boiler, and coffee making starts.

* If someone takes the pot off, the valve is opened to momentarily release pressure from the boiling water, which stops the water flow. 

* When the boiler becomes empty, the state is set to Brewed. When the state is Brewed, the light in the UI is turned on.

* When the coffee pot is replaced empty, the state goes back to the idle state where we began.

That's all there is to reading this application. The code for the coffee machine can be read and understood in about one minute. Compare that with reading other solutions to the coffee machine problem found on the Internet.

Note that the paragraph above is pretty much a restatement of the requirements in English. It could have been the requirements. The amount of information in the English form and the diagram form is about the same, thus the Domain Abstractions gave us the correct level of expressiveness. Further confirmation of this is if the level of expressiveness allows us to modify it.

For example, say a requirement was added that a coin device was to enable the machine to be used. The coin device is an abstraction that provides an output when a coin is given, and has a reset input. Looking at the diagram, and being able to reason about its operation so easily, you can see that the coin device's output would intercept the Pushbutton using another instance of an AND gate. And to reset the coin device, you could use the boiler empty output event.

==== Making it work

To make it actually execute, we apply the manual procedure documented in “Execution models.doc”. This document is in the Programming Paradigms layer. It will generate these 6 lines of code, which are the essence of the application diagram:

    if (userInterface.Button && warmerPlate.PotOnPlate && !boiler.Empty) { state = Brewing; } userInterface.Button = false;
    boiler.OpenSteamReleaseValve = !warmerPlate.PotOnPlate;
    boiler.On = state==Brewing;
    if (boiler.Empty && !prevBoilerEmpty) { state = Brewed; } prevBoilerEmpty = boiler.Empty;
    if (warmerPlate.PotEmpty && !prevPotEmpty) { state = Idle; } prevPotEmpty = warmerPlate.PotEmpty;
    userInterface.LightOn = state==Brewed;
 

There is a one-to-one correspondence between the lines in the diagram and the code. 

As you can see, the execution model is a simple one. The 6 lines of code are continually executed. This execution model is effective and appropriate for this small application.

The 6 lines of code can be built into a complete program shown below:

....
 #ifndef _COFFEE_MAKER_H_
 #define _COFFEE_MAKER_H_
 // Coffee Maker domain abstraction
 #include "CoffeeMakerAPI.h"  // original hardware abstraction supplied by hardware engineers
 // Knowledge dependencies :
 // "PolledDataFlowProgrammingParadigm.doc" -- explains how to hand compile a data flow diagram of this type to C code
 // Following are 3 Domain abstractions that the application has knowledge dependencies on
 
 
 
 #include "UserInterface.h"
 #include "Boiler.h"
 #include "WarmerPlate.h"
 
 
 
 class CoffeeMaker
 {
 private:
    enum {Idle, Brewing, Brewed} state;
    Boiler boiler;
    UserInterface userInterface;
    WarmerPlate warmerPlate;
    bool prevBoilerEmpty, prevPotEmpty;
    void _Poll();
 public:
    CoffeeMaker()
        : state(Idle), prevBoilerEmpty(boiler.Empty), prevPotEmpty(warmerPlate.PotEmpty)
    {}
    void Poll();
 };
 #endif //_COFFEE_MAKER_H_
....
 
....
 // CoffeeMaker.c
 // This is not source code, it is code hand compiled from the CoffeeMaker application diagram
 #include "CoffeeMaker.h"
 
 void CoffeeMaker::_Poll() <1>
 {
    if (userInterface.Button && warmerPlate.PotOnPlate && !boiler.Empty) { state = Brewing; } userInterface.Button = false;
    boiler.OpenSteamReleaseValve = !warmerPlate.PotOnPlate;
    boiler.On = state==Brewing;
    if (boiler.Empty && !prevBoilerEmpty) { state = Brewed; } prevBoilerEmpty = boiler.Empty;
    if (warmerPlate.PotEmpty && !prevPotEmpty) { state = Idle; } prevPotEmpty = warmerPlate.PotEmpty;
    userInterface.LightOn = state==Brewed;
 }
 
 
 
 void CoffeeMaker::Poll()
 {
    // get inputs processed
    userInterface.Poll();
    boiler.Poll();
    warmerPlate.Poll();
    // run application
    _Poll();
    // get outputs processed
    userInterface.Poll();
    boiler.Poll();
 }
....

<1>  The 6 lines of code appear in the "CoffeeMaker::_Poll()" function.


If you are using a diagram as we are in this solution, you always change the diagram first when the requirements change. It provides the expressiveness needed to see the application’s requirements represented in a clear, concise and coherent way. There the logic can be ‘reasoned’ with. It is not documentation, it is the source code representation of the requirements, and executable, both important aspects of ALA.

The next step is to implement the three abstractions. These are straightforward using the same execution model as was used for the application, so are not shown here.

The resulting application passes Martin's original acceptance tests plus a number of additional tests of behaviour gleaned from his original text.




== Pass four - Under the covers

=== Execution models (Programming paradigms)

ALA requires the use of execution models other than the computer CPU's execution flow. Some programmers will be so used to thinking in terms of sequential execution of instructions, statements, procedures or methods that it can be very difficult to think in terms of a different execution model. You keep wanting to know what the equivalent sequentially executed model is in order to understand what is going on instead of letting go and just thinking in the different programming paradigm. Certainly it is nice to know what is going on under the covers from a performance or resourcing point of view. But from a logical point of view, it is better to let go and start again in a new way of thinking in a totally different dimensional space from sequential execution flow. 

image::FSM-generic.png[State machine execution model, title="State machine execution model", float="left"]

The canonical example is a state machine. At first it can be difficult to write a program as a state machine, even if a state machine is a more suitable execution model. It takes some getting used to. Another example is data-flow such as Reactive Extensions (the same as functional programming with monads). Another is the asynchronous event driven model.

ALA makes use of multiple execution models. This is sometimes referred to as polyglot programming paradigms. The most common is a data-flow model. A data-flow model is a model in which adjacent instances in the program (or connected boxes on a diagram) follow the path the data instead of following the path of execution. The execution flow is like in another dimension relative to the data flow - all over the place. The ALA architect will even invent new execution models, whatever makes the expression of those requirements convenient and declarative.

=== Example execution models (Programming paradigms)

This is not an exhaustive list, it just collects up a few examples of  programming paradigms to get a better idea of what they are. In ALA you can invent your own programming paradigms to allow you to better express the requirements, just as we did with the 'IConsistsOf' paradigm that we used in the game scoring domain. 

==== Imperative

The default provided by your programming language. It reflects the underlying machine. Connected components are executed consecutively as fast as the CPU can do them. The connected components are usually language statements, functions, or methods.  

==== Activity-flow

As for a UML activity diagram. Connected activities start when the previous one finishes. The activity may take a long time to complete without holding the CPU. Activity flows can split, run in parallel or pseudo-parallel and recombine. 

==== Work-flow

Persisted Activity-flow. This includes long running activities within a business process such as an insurance claim.

==== Data-flow

A stream of data flows between connected components in a network. Each component processes data at its inputs and passes it to its outputs.

Each input and output can be operated in push or pull mode. Usually the system prescribes all pull (LINQ), all push (RX), inputs pull and outputs push (active objects with queues) or outputs pull and inputs push (active connectors).

The connection can be circular provided some kind of execution semantic finishes the underlying CPU execution at some point, such as using ticks to move data between components as done in function blocks.

The data-flow paradigm raises the question of type compatibility and type safety arises. In many systems the types used by the components are either parameterised or determined through type inference. Type inference is preferred because it allows the one source of truth for the type, which is in the application, to be passed to just one of the domain abstractions instead of many of them.  

==== Live-data-flow

As used in the coffee-maker example earlier, this paradigm simulates electronic circuits. Semantically the outputs and inputs are live at all times. This type of flow is most readily implemented with shared memory and on-change events.

==== Event-flow

This is similar to activity-flow and data-flow (when the events carry data) but the flows are always asynchronous, and the priority of processing of events from the queues can be explicitly controlled according to the applications knowledge of the urgency and performance of the connected components.

It has many examples of usage such as in Node.js or the reactor pattern.

What is not so commonly done is that ALA enforces the wiring of the components to be done by a higher layer abstraction, not by the modules themselves.

==== State machine (transition-flow)



==== UI layout

==== UI navigation flow

==== Data schema



=== Methodology

==== Iteration zero

When a new project begins, the only new information we have is the requirements. Any design decisions that don’t depend on the requirement could already have been made beforehand. It is those decisions that form the ALA reference architecture. Therefore,, when we get the requirements, that is our immediate and total focus. We may not know all of them, but we will only need a sample to build a picture of the architecture. 

Looking at the available new information as a whole first instead of taking it a bit at a time during the project's sprints will make a huge difference to the eventual architecture quality.

The process in the first iteration takes requirements one by one, and represents them, in all their detail. Domain abstractions will be invented as you go, and they will have parameters or properties that will handle those details from requirements. 

For the first green field application, you spend a maximum of one sprint. After that you do need to find out if your design works. So you may not get through all the known requirements. That does not matter. 

To know whether knowledge from your design goes in the application layer of the domain abstractions layers, you consider what the scope of that knowledge is. Is the knowledge specific to this one requirement in the one application, or is it potentially reusable in the same or other applications? A softkey label is clearly specific. The concept of softkeys is clearly in the domain. 

The output of the first sprint does not implement any of the invented abstractions, but it does include all details of the requirements that are looked at. In so doing, you design the first approximation of a DSL. The DSL may be refined later as more requirements are looked at.

Each abstraction will eventually be implemented as a class, but initially we just record the names of the abstractions, and a short explanation that provides the insight into what this abstraction does. 

By the end of the first sprint the requirements will have become easier and easier to represent, as the set of abstractions will have taken shape. Sometimes you will generalize an abstraction further to enable it to be useful in for more things.

By keeping moving through the requirements at a much faster pace than in normal development (say one feature per hour instead of one per week), we can keep representing them in a coherent way, revising abstraction ideas we have already invented. Ideally, we will end up with a set of domain abstractions that can be wired together in different ways to represent a ‘domain space’ of requirements. That domain space will grow slightly as time goes on and it accommodates a growing family of products, but we don’t want it to grow beyond that. We don’t want to invent ways of implementing things we will never do. If you leave the invention of the set of abstractions to be done gradually during the longer time scale of the project as a whole, you will lose the opportunity to have a coherent set that will compose with each other in an infinite variety of ways.

The output of sprint zero is usually a diagram showing the wiring of instances of abstractions, together with annotated configuration information for those instances.

It doesn’t matter if some of the requirements are wrong. Chances are they are still useful for scoping out the domain space. What we are actually producing in this phase are the necessary abstractions for the Domain layer. If the requirements change later, it will be trivially simple to change them as only the Application layer wiring should change.

Once this process has started to become easy, which should happen within the first sprint, the burning question in our minds will become “Will all this actually work?” We have to trust that there will be a way to write those abstractions to make the whole system work.

==== 2nd Sprint

In the second sprint we start converging on normal Agile. You pick one feature to implement first. Agile would say it should be the most essential feature to the MVP (minimum viable product), but this can be tempered by the need to choose one that requires a fewer number of abstractions to be implemented. Next, you design the interfaces that will allow those abstractions to plug together according to the wiring you already have from the first sprint. What messages will these paradigm interfaces need to pass at runtime between the unknown clients to make them work?

It may take several sprints to produce the first working feature, depending on the number of abstractions it uses. 

At first this sounds as if it might be just the waterfall method reincarnated. Do an overall design, document it or model it, and then write lots of code before everything suddenly starts working. But the design we created in iteration zero is very different from what a normal waterfall would produce, and is resilient to the sorts of problems waterfall creates. Instead of a ‘high level’ design of how the implementation will work which is lacking in detail, the design is a representation of requirements, in full detail. The design is not a model. It is executable.

There is one more important thing that the design phase in Iteration Zero does. While it deliberately doesn’t address any implementation, it does turn the remainder of the implementation into abstractions, and those abstractions are zero coupled. To convert from executable to actually executing, it only remains to implement these now completely separate parts. You can give these abstractions to any developer to write.  Together the developers will also easily be able to figure out the paradigm interface methods needed to make them work, and the execution models to take care of the execution flow through them with adequate performance.

Often when a project is split into two phases, the first phase turns out to be waste. The devil is in the details so to speak. This happens because the implementation details in phase two are coupled back to and affect the design in phase one. As learnings take place during implementation, the design must change. In ALA the output from phase one is primarily abstractions, which are inherently stable and therefore hide details that can't affect the overall design. If the abstractions are good, phase two will typically have little effect on the work done in phase one.

Once the first feature is working, several abstractions will have been implemented. The second feature will take less time because some of the abstractions are already done. In ALA velocity increases as time goes on and keeps increasing until new features only involve instantiating, configuring and wiring domain abstractions in new ways. This velocity acceleration is the complete opposite of what happens in monolithic code.

==== Later sprints

Imagine going into a sprint planning meeting with a Product Owner, a small team of developers, and a mature ALA domain that already has all the common domain abstractions done. As the Product Owner explains the requirements, one of the team members writes them down directly as they would be represented in terms of the domain abstractions. Another team member watches and remembers any lost details without slowing the product owner down. A third member implements the acceptance tests in similar fashion, and a fourth provides him with test data. It would be nice to have a tool that compiles the diagram into the equivalent wiring code. With such a tool, the team could have it executing by the end of the meeting. At the end of the planning meeting the development team say to the product owner "Is this what you had in mind?". The team can get immediate feedback from the Product Owner that the requirements have been interpreted correctly. 

Of course, the planning meeting itself would only produce 'normal' functionality. Usually it is up to the development team, not the Product Owner, to uncover all the abnormal scenarios that can happen, and that is usually where most of the work in a software system goes. Having said that, in a mature domain, the validation of data already has decorator abstractions ready to go.


[chart,line,file="effort_curve.png", opt="title=Effort per user-story,x-label=months"]
--
//Big ball of mud
1,	5
2,	5
3,	6
4,	6
5,	7
6,	8
7,	9
8,	10
9,	12
10,	13
11,	15
12,	17
13,	19
14,	21
15,	24
16,	28
17,	32
18,	37
19,	43

//Cocomo
1,	16
2,	17
3,	17
4,	18
5,	18
6,	19
7,	19
8,	19
9,	19
10,	20
11,	20
12,	20
13,	20
14,	20
15,	20
16,	20
17,	21
18,	21
19,	21
20,	21
21,	21
22,	21
23,	21
24,	21

//ALA
1,	30
2,	21
3,	17
4,	15
5,	13
6,	11
7,	10
8,	9
9,	8
10,	8
11,	7
12,	7
13,	6
14,	6
15,	5
16,	5
17,	4
18,	4
19,	3
20,	3
21,	3
22,	2
23,	2
24,	2
--

The graph shows the effort per user story against months into a green-field project. The left axis is arbitrary - the shape of the curves is what is important. For a big ball of mud, experience tells us that the effort increases dramatically and can asymptote at around 2 years as our brains can no longer handle the complexity, and the project must be abandoned.

The COCOMO model, which is an average of industry experience, has a power relationship with program size, with an exponent of around 1.05 to 1.2. I have used the mid point, 1.1, for this graph. The model appears to imply that getting lower than 1.0 is a barrier, but there is no reason to believe this is the case. Reuse can make the power become less than 1. The range of 1.05 and 1.2 probably results from some reuse mitigating some ever increasing complexity.

ALA takes advantage of the fact that zero-coupled abstractions can keep complexity relatively constant and drastically increase reuse. A spectacular fall in effort per user story is thus possible.  



=== Folder structure

....
[tree,file="folderstructure.png"]
--
root
|--Applications
|  |--Application1
|  |  `--application.cpp
|  |--Application2
|  |  `--application.cpp
|  |--DomainAbstractions
|  |  |--abstraction1.cpp
|  |  |--abstraction1.h
|  |  |--abstraction2.cpp
|  |  `--abstraction2.h
|--DomainHardware
|--DomainConnectivity
|--DomainDatabase
`--ProgrammingParadigms
   |--Paradigm1.h
   `--Paradigm2.h
--
....


[tree,file="folderstructure.png"]
--
root
|--Applications
|  |--Application1
|  |  `--application.cpp
|  |--Application2
|  |  `--application.cpp
|  |--DomainAbstractions
|  |  |--abstraction1.cpp
|  |  |--abstraction1.h
|  |  |--abstraction2.cpp
|  |  `--abstraction2.h
|--DomainHardware
|--DomainConnectivity
|--DomainDatabase
`--ProgrammingParadigms
   |--Paradigm1.h
   `--Paradigm2.h
--


////
[tree,file="folderstructure.png"]
--
#root
##DomainProjects
###Application1
####Application.cpp
###Application2
###DomainAbstractions
####abstraction1.cpp
####abstraction1.h
####abstraction2.cpp
####abstraction2.h
##HardwareDomain
##ConnectivityDomain
##DatabaseDomain
#ProgrammingParadigms
##Paradigm1.h
##Paradigm2.h
--
////
// (TBD - The folder structure feature of AsciiDocFX causes it to hang)

This is a suggested folder structure for ALA. Because ALA does not use decomposition, you don't end up with components that are contained by the applications, so there are no subfolders under the application. Instead, you end up with Domain Abstractions outside the application, so they go in their own folder in a flat structure.

Similarly, the Programming Paradigms code is not contained by an application, or even by the domain, so would not be contained by the domain's projects folder.

=== Convention over configuration

When the application create an instance of an abstraction, most of the configuration of that abstraction should have defaults. In ALA, setters allow optional configuration, reducing the amount of information that would otherwise be required in the application to fully configure each abstraction. Any configuration that we wish to enforce goes into the constructor.

There is a counter argument that says that all configuration should be explicit so that nothing can be forgotten. ALA prefers optional configuration because we want the application to just express the requirements. Also optional configuration allows abstractions to default to their simplest form, making them easier to learn.

=== Everything through interfaces

A class, in contrast to an abstraction, has an interface comprising all the public members. In ALA we only want this interface to be used by the application when it instantiates and configures an instance of an abstraction. All other inputs and outputs that are used at run-time are done through interfaces (abstract interfaces). 

=== Knowledge prerequisites.

When other programmer are doing maintenance on your code, you should make sure they have the knowledge they need. They will need knowledge of ALA. They will need to know about the programming paradigms used. They will need to know about the domain abstractions, and the insight of what each one does. And then they should know that the application diagram is the source code. It is up to you that every develop that follows will know all this.

==== Intellisence

After they have modified the diagram, the maintaining developers will need to manually modify the corresponding code. Here they will see instances of abstractions being used all over the place, either 'new' keywords or function calls. If we have done our job with knowledge prerequisites, they will have been introduced to these abstractions. However, it doesn't hurt to have brief reminders of what they are pop up when the mouse is hovered over them. So put in triple slash comments (or equivalent) describe the abstraction succinctly, with the intention of it being a reminder to someone who has already met the abstraction. Put a full explanation in the remarks and examples sections. 

The class name after a new keyword is actually the constructor name, so you must duplicate the summary section there. Often in ALA, the class name is not referred to at all in the application.

=== Example project - Game scoreboard

For the example project for this pass, we return to the tenpin bowling and tennis applications that we used in Pass 3, and add a scoreboard feature (well a simple ASCII scoreboard in a console app rather than real hardware).

We want the ASCII boards to look like these examples:

....
 -----+-----+-----+-----+-----+-----+-----+-----+-----+--------
|   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |    10  |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
| 1| 4| 4| 5| 6| /| 5| /|  | X| -| 1| 7| /| 6| /|  | X| 2| /| 6|
+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+--+
|   5 |  14 |  29 |  49 |  60 |  61 |  77 |  97 | 117 |   133  |
 -----+-----+-----+-----+-----+-----+-----+-----+-----+--------
....

....
 -----++----+----+----+----+----++--------
|   1 ||  4 |  6 |  5 |    |    ||    30  |
|   2 ||  6 |  4 |  7 |    |    ||  love  |
 -----++----+----+----+----+----++--------
....



As usual in ALA, our methodology begins with expressing those requirements directly, and inventing abstractions to do so. So, we invent a 'Scoreboard' abstraction. It will have a configuration which is an ASCII template of the board. Here are instances of it configured for Bowling and Tennis:

....
 -------+-------+-------+-------+-------+-------+-------+-------+-------+-----------
|   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |     10    |
+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
|F00|F01|F10|F11|F20|F21|F30|F31|F40|F41|F50|F51|F60|F61|F70|F71|F80|F81|F90|F91|F92|
+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+---+
|  T0-  |  T1-  |  T2-  |  T3-  |  T4-  |  T5-  |  T6-  |  T7-  |  T8-  |    T9-    |
 -------+-------+-------+-------+-------+-------+-------+-------+-------+-------------
....

....
 -----++----+----+----+----+----++--------
| M0  ||S00 |S10 |S20 |S30 |S40 || G0---  |
| M1  ||S01 |S11 |S21 |S31 |S41 || G1---  |
 -----++----+----+----+----+----++--------
....

It has single letter place-holders for the scores. (A single letter is used so it doesn't take up much space on the scoreboard design.) Different letters are used for different types of scores. Digits are used to specify where multiple scores of the same type are arranged on the scoreboard, like indexes.

An abstraction is needed to bind the single letters to data. It will be configured with a function that knows how to get the data from the scoring tree structure that we already have. We wire in these little bindings like this. 



[plantuml,file="diagram-bowling-3.png"]
----
@startdot
digraph foo {
rankdir=LR

#note rankdir does not work inside subgraphs
subgraph cluster_C {
fontsize=20
label="Ten-Pin Bowling                                                            "
style=rounded

node [shape=Mrecord]
console [label="ConsoleGameRunner|\"Enter number of pins\""]

scoreboard [fontsize=14,label=<
<table border='0' cellborder='1' cellspacing='0'>
<tr><td colspan="21" sides="B"><font point-size="14">Scorecard</font></td></tr>
<tr><td colspan="2">1</td><td colspan="2">2</td><td colspan="2">3</td><td colspan="2">4</td><td colspan="2">5</td><td colspan="2">6</td><td colspan="2">7</td><td colspan="2">8</td><td colspan="2">9</td><td colspan="3">10</td></tr>
<tr><td sides="LTR">F00</td><td>F01</td><td sides="LTR">F10</td><td>F11</td><td sides="LTR">F20</td><td>F21</td><td sides="LTR">F30</td><td>F31</td><td sides="LTR">F40</td><td>F41</td><td sides="LTR">F50</td><td>F51</td><td sides="LTR">F60</td><td>F61</td><td sides="LTR">F70</td><td>F71</td><td sides="LTR">F80</td><td>F81</td><td sides="LTR">F90</td><td>F91</td><td>F92</td></tr>
<tr><td colspan="2" sides="LBR">T0</td><td colspan="2" sides="LBR">T1</td><td colspan="2" sides="LBR">T2</td><td colspan="2" sides="LBR">T3</td><td colspan="2" sides="LBR">T4</td><td colspan="2" sides="LBR">T5</td><td colspan="2" sides="LBR">T6</td><td colspan="2" sides="LBR">T7</td><td colspan="2" sides="LBR">T8</td><td colspan="3" sides="LBR">T9</td></tr>
</table>
>]

framebind [label="Binding|F"]
totalbind [label="Binding|T"]
game [label="Frame|\"game\"|nFrames==10"]

node [shape=record]
function1 [label="GetSubFrames()\n.Select(sf =\> sf.GetScore()[0])\n.Accumulate()"]
function2 [label="GetSubFrames()\n.Select(f =\> f.GetSubFrames()\n.Select(b =\> b.GetScore()[0])"]
translate [label="Translate\nX,/,- etc"]

console -> game  [label = "IConsistsOf"]
console -> scoreboard [constraint=false, label = "IPullDataFlow"]
scoreboard -> framebind -> translate -> function2 -> game
scoreboard -> totalbind -> function1 -> game

{rank=same console scoreboard}
{rank=same framebind totalbind}
{rank=same function1 function2}

}
}
@enddot
----

The other thing added to the diagram is an instance of a ConsoleGameRunner abstraction. Its job is to prompt for a score from each play, display the score, and repeat until the game completes. It is wired into the instances of the other abstractions we have discussed. The 'game' instance of the Frame abstraction on the right of the diagrams is the same one we already had in pass 2. Together the two halves of the diagram make up the entire application. That application is now an executable console application.

The rounded boxes in the diagram are instances of domain abstractions as usual for ALA diagrams. The rectangles are instances of Application layer abstractions. They contain a small function to get the appropriate score. The code is shown inside the boxes.

So now that we have these domain abstractions for doing console game scoring applications, lets do tennis:


////
[plantuml,file="diagram-bowling-4.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
#subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
bonus [label="Bonus||score\<10 \|\| plays==3"]
frame [label="Frame|\"frame\"|frameNum\<9 && (balls==2 \|\| pins==10)\n \|\|\ (balls==2 && pins\<10 \|\| balls==3)"]
ball [label="SinglePlay"]
game -> bonus -> frame -> ball
}
}
@enddot
----
////


[plantuml,file="diagram-tennis-3.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Tennis"
style=rounded

node [shape=Mrecord]
console [label="ConsoleGameRunner|\"Enter winner of play\""]

scoreboard [label="Scoreboard| -----++----+----+----+----+----++--------\n\| M0  \|\|S00 \|S10 \|S20 \|S30 \|S40 \|\| G0---  \|\n\| M1  \|\|S01 \|S11 \|S21 \|S31 \|S41 \|\| G1---  \|\n -----++----+----+----+----+----++--------\n"]

gamebind [label="Binding|G"]
setbind [label="Binding|S"]
matchbind [label="Binding|M"]
match [label="Frame|\"match\"|score.Max()==3"]

node [shape=record]
function1 [label="GetScore()"]
function2 [label="GetSubFrames()\n.Select(sf =\> sf.GetSubFrames().First())\n.Select(s =\> s.GetScore()).ToList()"]
function3 [label="GetGameOrTieBreakScore\n(see function)"]

console -> scoreboard [constraint=false, label = "IPullDataFlow"]
console -> match [label = "IConsistsOf"]
scoreboard -> setbind -> function2
scoreboard -> matchbind -> function1
scoreboard -> gamebind -> function3
function1 -> match
function2 -> match
function3 -> match

{rank=same console scoreboard}

}
}
@enddot
----

////
[plantuml,file="tennis4.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]

node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
switch [label="Switch||(setNumber\<4 &&\n score[0]==6 && score[1]==6"]
wtp3 [label="WTP"]
tiebreak [label="Frame|\"tiebreak\"|score.Max()==7"]
play2 [label="SinglePlay"]
match -> wtp1 -> switch -> set -> wtp2 -> game -> play
switch:s -> wtp3:w
wtp3 -> tiebreak -> play2
{rank=same set wtp3}

// }
}
@enddot
----
////

I left the code out of the GetGameOrTieBreakScore box as it is a little big for the diagram here. It is similar to the Linq query above but it must first determine if a tiebreak is in progress and get that if so. Also it translates game scores from like 1,0 to "15","love".

Having designed the complete ALA applications in diagram form, implementation in code for the application is trivial as it is just use of the WireTo extension method to wire exactly according to the diagram. The domain abstractions are also easy because they are zero coupled with one-another and with the application layer. The method to write them is simply to determine what is needed in teh interfaces to make the them able to communicate, and then implement those interfaces for each one.

Although the diagram must be turned into text code to actually execute, it is important in ALA to do these architecture design diagrams first. They not only give you the application, they give you the architectural design by giving you the domain abstractions and programming paradigms as well. If you try to design an ALA structure in your head while you write it directly in code, you will get terribly confused and make a mess. UML class diagrams will be even worse. Code at different abstraction levels will end up everywhere, and run-time dependencies will abound. Our programming languages, and the UML Class diagram, are just not designed to support abstraction layered thinking. And it is too easy to add bad function calls or 'new' keywords into code in the wrong places. Not using an application diagram to do the design in ALA is like an electronics engineer not using a schematic to design an electronic circuit. The reason boxes and lines work is that it just doesn't look right when you draw a line from the inside of the box to the outside (which would create a run-time dependency). It's clearly breaking the abstraction. Yet it's fine to add a box (a design-time dependency). In text, these two things look the same. In UML class diagrams, they look the same also.

Here is the code for the diagram.
....
....
....
consolerunner = new ConsoleGameRunner("Enter number pins:", (pins, scorer) => scorer.Ball(0, pins))
.WireTo(game)
.WireTo(new Scorecard(
"-------------------------------------------------------------------------------------\n" +
"|F00|F01|F10|F11|F20|F21|F30|F31|F40|F41|F50|F51|F60|F61|F70|F71|F80|F81|F90|F91|F92|\n" +
"|    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+----\n" +
"|  T0-  |  T1-  |  T2-  |  T3-  |  T4-  |  T5-  |  T6-  |  T7-  |  T8-  |    T9-    |\n" +
"-------------------------------------------------------------------------------------\n")
.WireTo(new ScoreBinding<List<List<string>>>("F", 
    () => TranslateFrameScores(
        game.GetSubFrames().Select(f => f.GetSubFrames().Select(b => b.GetScore()[0]).ToList()).ToList())))
.WireTo(new ScoreBinding<List<int>>("T", 
    () => game.GetSubFrames().Select(sf => sf.GetScore()[0]).Accumulate().ToList()))
);
....

If you compare this code with the disgram, you will see a pretty direct correspeodnece. 
Remember 'game' comes from our previous example.

That's pretty much all the code in the application. Oh there is the 'translate' function, but it is pretty straight forward once the requirements for the way a Tenpin scorecard is actually filled in and enumerated. You can see it, and all the code at the github link below. 

Note that at run-time, not all data-flows have to go directly between wired up instances of domain abstractions. The data can come up into the application layer code, and then back down. This was the case when we did the functional composition example in pass 1. In this application we are doing that with the code in the square boxes that get the score from the game tree. The important thing is that all the code in the application is specific to the application requirements.  

Here is the code for the Tennis diagram:
....
consolerunner = new ConsoleGameRunner("Enter winner 0 or 1", (winner, scorer) => scorer.Ball(winner, 1))
.WireTo(match)
.WireTo(new Scorecard(
        "--------------------------------------------\n" +
        "| M0  |S00|S10|S20|S30|S40|S50|S60|  G0--- |\n" +
        "| M1  |S01|S11|S21|S31|S41|S51|S61|  G1--- |\n" +
        "--------------------------------------------\n")
    .WireTo(new ScoreBinding<int[]>("M", () => match.GetScore()))
    .WireTo(new ScoreBinding<List<int[]>>("S", () => 
        match.GetSubFrames()
            .Select(sf => sf.GetSubFrames().First())
            .Select(s => s.GetScore())
            .ToList())
    .WireTo(new ScoreBinding<string[]>("G", () => GetGameOrTiebreakScore(match)))
);

....

If you compare this code with the diagram, you can see a pretty direct correspondence. match comes from the code from the other half of the diagram done in pass 2.
////

Here is the application abstraction that knows how to translate throw scores into Xs and slashes and dashes.

....

/// <summary>
/// Get frame scoring translated from numbers to Xs, slashes and dashes.
/// </summary>
/// <example>
/// 7,2 -> "7","2"
/// 7,0 -> "7","-"
/// -,3 -> "-","7"
/// 7,3 -> "7","/" 
/// 10,0 -> "",X
/// 0,10 -> "-","/"
/// additional ninth frame translations:
/// 10,0 -> "X","-"
/// 7,3,2 -> "7","/","2"
/// 10,7,3 -> "X","7","/"
/// 0,10,10 -> "-","/","X"
/// 10,10,10 -> "X","X","X"
/// </example>
/// <param name="frames">
/// The parameter, frames, is a list of frames, each with a list of integers between 0 and 10 for the numbers of pins.
/// </param>
/// <returns>
/// return value will be exactly the same structure as the parameter but with strings instead of ints
/// </returns>
/// <remarks>
/// This function is an abstraction  (does not refer to local variables or have side effects)
/// </remarks>
private List<List<string>> TranslateFrameScores(List<List<int>> frames)
{ 
    // This function looks a bit daunting but actually it just methodically makes the above example tranlations of the frame pin scores 
    List<List<string>> rv = new List<List<string>>(); 
    int frameNumber = 0;
    foreach (List<int> frame in frames)
    {
        var frameScoring = new List<string>();
        if (frame.Count > 0)
        {
            // The first 9 frames position the X in the second box on a real scorecard - handle this case separately
            if (frameNumber<9 && frame[0] == 10)
            {
                frameScoring.Add("");
                frameScoring.Add("X");
            }
            else
            {
                int ballNumber = 0;
                foreach (int pins in frame)
                {
                    if (pins == 0)
                    {
                        frameScoring.Add("-");
                    }
                    else
                    if (ballNumber>0 && frame[ballNumber]+frame[ballNumber-1] == 10)
                    {
                        frameScoring.Add(@"/");
                    }
                    else
                    if (pins == 10)
                    {
                        frameScoring.Add("X");
                    }
                    else
                    {
                        frameScoring.Add(pins.ToString());
                    }
                    ballNumber++;
                }

            }
        }
        rv.Add(frameScoring);
        frameNumber++;
    }
    return rv;
}
....
////


////
Now lets have a look at some of the code in the two of the new domain abstractions. Here is the essence of the Scoreboard domain abstraction (remember we are down a layer now, so it has no knowledge of bowling):

....
public string GetScorecard()
{
    var matches = Regex.Matches(ASCIITemplate, "(([A-Z][0-9][0-9])|([A-Z][0-9])|([A-Z]))-*"); // The regular expression matches e.g. A, B1, C12, D-, E00--
    var rv = ASCIITemplate;
    foreach (Match match in matches)
    {
        char id = match.Value[0];
        foreach (IScoreBinding sg in scoreGetters)
        {
            if (id == sg.Label[0])
            {
                if (match.Length>=2 && char.IsDigit(match.Value[1]))
                {
                    if (match.Length >= 3 && char.IsDigit(match.Value[2])) // e.g. A11
                    {
                        rv = rv.Replace(match.Value, sg.GetScore(Convert.ToInt32(match.Value[1]) - Convert.ToInt32('0'), Convert.ToInt32(match.Value[2]) - Convert.ToInt32('0')).PadLeft(match.Length));
                    }
                    else // e.g. A1
                    {
                        rv = rv.Replace(match.Value, sg.GetScore(Convert.ToInt32(match.Value[1]) - Convert.ToInt32('0')).PadLeft(match.Length));
                    }
                }
                else // e.g just A, no index
                {
                    rv = rv.Replace(match.Value, sg.GetScore().PadLeft(match.Length));
                }
            }
        }
    }
    return rv;
}
....

The ScoreBinding domain abstraction has three overloads of GetScore - one for two indexes, one for one index, and one for zero indexes. Here is the code for the one that has one index. The other two are similar. Because we are given one index, we expect the function that we have been wired to will return a one dimensional something. It could be a List or array, of type int or string. T tells us what type it is. Our job is to index into whatever it is, and return it as a string:

....
public string GetScore(int x)
{
    object temp = function();
    if (typeof(T) == typeof(List<int>))
    {
        List<int> list = (List<int>)temp;
        if (x < list.Count) return list[x].ToString();
    }
    if (typeof(T) == typeof(int[]))
    {
        int[] array = (int[])temp;
        if (x < array.Length) return array[x].ToString();
    }
    if (typeof(T) == typeof(List<string>))
    {
        List<string> list = (List<string>)temp;
        if (x < list.Count) return list[x];
    }
    if (typeof(T) == typeof(string[]))
    {
        string[] array = (string[])temp;
        if (x < array.Length) return array[x];
    }
    return "";
}
....


////

Remember that designing an ALA application takes some up-front effort, which pays back in the maintenance phase months later. It is difficult to show a payback within a pedagogical sized example. But hopefully we can get a glimpse of it when we see that having done the Tenpin bowling application, Tennis can be done just by a single diagram.

That completes our discussion of all the new code for the console applications for bowing and tennis. The full project code can be viewed or downloaded here:

https://github.com/johnspray74/GameScoring[GameScoring code]

As


== Pass five - The philosophy behind ALA


=== The human brain

In this perspective of ALA, we look at the problem of complexity in software in the context of how the human brain works.

Software design involves our intelligence or brain power. Understandability, readability, complexity are all things very closely related to the brain. Yet in the field of software engineering we pay little attention to how the brain understands our complicated world in order to understand how we should do our software.

Our brains do it primarily through one mechanism which we have come to call 'abstraction'. We learn abstractions from the commonality of multiple examples, and we then use abstractions without those examples cluttering up the common notion that was learned.

image::Paintings_from_the_Chauvet_cave.jpg[,400, title="", float="right"]

Our ancestors could use word like 'bring your spear' and it had a simple meaning to them only because all the detail and knowledge that goes into building a spear is replaced with the abstraction 'spear'. Without the abstraction, the sentence would have to be more like "bring the object that we made by joining the object we made by applying blows to the hard material we found at the place..., with the long material we cut in the place with the tall..., by tying with the long grass material using the guey stuff we found at the...". Even this sentence was only made possible by other abstractions: joining, material, blows, hard, long (twice), cut, tall, tying, guey, and found. If we expanded all of them until we were only using a few basic concepts like 'object' and 'place' we would have a sentence so long that we could never communicate at all. That's what abstractions do, and how our brains make use of them. The word spear, in turn can be used to create a new abstraction, a hunting plan, while all of that other detail remains hidden from that new context.

The problem with software engineering is we are not making use of this way that the brain works. Simply put, we are not creating good abstractions. This lets the complexity inside one 'module' spill out into other modules. Abstractions, not modules, are the only mechanism that allows us to hide details at design-time. 

image::neuron.svg[, 300, title="", float="left"]

As software engineers we do learn and use many abstractions. For example if we want to protect simultaneous access to the resource, our brain should conjure up 'mutex'. 

If the brain already 'knows about' an abstraction, the abstraction is like any other single line of code, such as a mutex. We can make use of a mutex without having to deal with the details and complexities of how it works. We don't have to think about the fact that we may have to wait for the resource. Nor that another thread may start running if we have to wait. Nor that if a higher priority thread preempts us while we have the resource, we may have it for a long time. Nor that if a still higher priority thread needs it during that long time, we will be given temporary priority to finish our use of it. We can just simply use the abstraction for protecting a resource.

Abstractions like mutex, regex, SQL are already invented by the 'culture' of software engineering, much like memes in the real world have been passed down to us. Where we fall down is when we get into a particular domain where the abstractions have not yet been invented, and we need to invent them. It is not easy to invent new abstractions, but invent them we must, at a rate far higher than is normal for cultural evolution.

Good domain abstractions, introduced and learned by new developers in the domain, then appear to them as normal program elements - things they can use with extraordinary convenience like any other line of code.



=== Abstraction

Of the overwhelming list of engineering topics that we listed in Pass One, this topic that is the most fundamental to ALA, and the one most needed for explaining it. It's also probably the vaguest and most misunderstood topic in software engineering, so we will spend some time understanding it.

Abstraction will be the king. The short reason why we start with abstraction is that our quality attributes, complexity and understandability are very much to do with how our brains work, and for 100,000 years at least, our brains have worked with abstractions to understand our world. Abstractions are the only mechanism our brains use for dealing with otherwise complex things. 

As in a chess game, winning is only about protecting the king. But this Abstraction king is benevolent. If he is destroyed, you do not lose the game immediately. It will take time, but you will lose.

There are other contenders to be king in the engineering topics list. For example, it is said that the best thing about TDD is not the testing but the emergence of better abstractions. TDD is like a lord that serves the king. It usually serves the king, causing you to make better abstractions. But sometimes it just serves its own purpose and makes the abstraction worse. It just produces code that works where it passes and no more.

Another contender is microservices. It is popular because it improves your abstractions by making them harder to destroy with cross coupling. But it too is just a lord. Because it provides physical boundaries that in normal software would be crossed, it serves the king. But by serving the abstraction king directly we can have logical boundaries, and all their benefits, even in 'monolithic' code.

Another contender to be king is 'no side effects' used by the functional mathematical purity guys. There are those who talk as if disobeying this king is absolute treason. But again, this lord is only effective because he usually serves the abstraction king. But, again, there are times when he doesn't, and 'no side effects' is not enough to make a good abstraction.

ALA always follows the one true king.

==== Classes, Modules, functions and encapsulation. 

Classes, Modules, functions and encapsulation are artefacts of the language and do their thing at compile-time. They are not necessarily abstractions. They have been around for about 60 years, not enough time for our brains to see them in the same way as the compiler does. Although abstractions are implemented using these artefacts, ALA needs them to also be abstractions. In ALA "abstraction" is the term we use for the artefacts of our design instead of classes, modules, functions, or components, all of which are extremely fragile as abstractions.  

==== Wikipedia on abstraction

"Thinking in abstractions is considered by anthropologists, archaeologists, and sociologists to be one of the key traits in modern human behaviour, which is believed to have developed between 50 000 and 100 000 years ago. Its development is likely to have been closely connected with the development of human language, which (whether spoken or written) appears to both involve and facilitate abstract thinking."

In the real world, new abstractions come along infrequently, and are conceived of by few. People quickly begin using them to understand new insights or compose new things. They become so natural to us that we forget that they are abstractions. In no other field do we need to create them as fast as in software engineering. It is the most important skill a developer needs to have.

==== Defining abstraction

The term abstraction is arguably one of software engineering's vaguest and most overloaded terms. For the purpose of this article we will need a definition. I find the easiest way to define it is to provide a set of 'statements about', 'qualities of', and 'what it is nots':

* What our brains evolved to use to understand things
* Etymology: 'to draw out commonality'
* The concept or notion represented by the commonality of many instances
* Has inherent stability - as stable as the concept itself
* Increases with ubiquity and reuse
* Decreases as you get closer to your specific application
* The only mechanism that separates and hides knowledge _at design-time_


==== Dijkstra on abstraction

anchor:Dijkstra[]

"It has been suggested that there is some kind of law of nature telling us that the amount of intellectual effort needed grows with the square of program length. But, thank goodness, no one has been able to prove this law. And this is because it need not be true. We all know that the only mental tool by means of which a very finite piece of reasoning can cover a myriad cases is called “abstraction”; as a result the effective exploitation of his powers of abstraction must be regarded as one of the most vital activities of a competent programmer. In this connection it might be worth-while to point out that the purpose of abstracting is not to be vague, but to create a new semantic level in which one can be absolutely precise. Of course I have tried to find a fundamental cause that would prevent our abstraction mechanisms from being sufficiently effective. But no matter how hard I tried, I did not find such a cause. As a result I tend to the assumption —up till now not disproved by experience— that by suitable application of our powers of abstraction, the intellectual effort needed to conceive or to understand a program need not grow more than proportional to program length."

The "conceive" part I agree with, if by that we mean the development. However, the "intellectual effort to understand" part needs further insight. We shouldn't have to read an entire program to understand a part of it. We ought to be able to understand any one part of it in isolation. The effort to read any one part should be approximately constant.

In an early section of this article there was a quality graph of complexity <<ComplexityGraph1, here>>. ALA uses abstraction as the only mechanism available to us to achieve this constant complexity.

==== The three stages of creativity

image::creativity.jpg[Creativity with abstractions, title="The creativity cycle", width=80%, align="center"]

A good abstraction separates the knowledge of different worlds. A clock is a good abstraction. On one side is the world of cog wheels. On the other side is someone trying to be on time in their busy daily schedule. Neither knows anything about the details of the other. SQL is another good abstraction. On one side is the world of fast indexing algorithms. On the other is finding all the orders for a particular customer. Let us consider a domain abstraction - the calculation of loan repayments. On one side is the world of mathematics with the derivation and implementation of a formula. On the other, the code is about a person wanting to know if they can afford to buy a house. If your abstractions don't separate knowledge of different worlds like this, then you are probably just factoring common code. Find the abstraction in that common code. Make it hide something complicated that's really easy to use and really useful, like a clock.

The creativity cycle starts with abstractions, such as cogs and hands, instantiates them, configures them for a particular use, then composes them into a new abstraction. In ALA we usually go around the creativity cycle three times, creating three layers on top of our base programming language.


==== An illustration of abstraction at work

Imagine you are reading the following function, xyz123, at design-time and trying to understand it:

 real xyz123(real)
 {
     ...
     b = fubar(a)
     ...
 }

 real fubar(real)
 {
     // complicated code
 }

You don't know what fubar is (fubar stands for messed up beyond all recognition), so you follow the indirection, an inconvenience at the least because you are really just interested in xyz123. You begin reading the code at fubar. It only has about 20 lines but it is complicated. A comment mentions that it uses a CORDIC algorithm and gives a reference. But before following that indirection as well, you note that fubar has the following properties:

* a module
* has a simple interface
* encapsulated
* uses nothing but what is in the interface
* no side effects
* hides information
* is loosely coupled
* separates two concerns
* is small
* follows the coding guidelines
* has comments

It could even have a good name describing what it returns in terms of its input, something like Returns X such that X is ...., but that still wouldn't solve the issue.

Why, if it has all these good software properties, is it destroying our ability to understand function xyz123?  The missing ingredient is of course abstraction, the thing the brain needs for meaning. So let's go ahead and add the abstraction property. To do that, change fubar to just 4 letters (it doesn't even need to be a good name): SQRT.  While we are at it, let's add the abstraction property to xyz123 as well, by naming it StandardDeviation. 

 real StandardDeviation(real)
 {
     ...
     b = SQRT(a)
     ...
 }

 real SQRT(real)
 {
     // complicated code
 }

Suddenly understandability in the first function is unlocked. The complicated code inside SQRT no longer matters. It is completely isolated by the abstraction. If your brain already knows the SQRT abstraction (I had to choose one that I knew you already knew), there is no need to follow the indirection. The reader of the code continues reading with the next line of code after the SQRT invocation as if it is just like any other line of code in their language. That's what abstraction, at least as used in ALA, is.

All those other properties made no difference while the abstraction property was missing. But any one of them could destroy the abstraction. Our current programming languages provide nothing to help us with abstraction. At least for the meantime, it remains the one cerebral activity software engineers must do for themselves. 

=== No Loose Coupling

Here we meet the first meme from our list of software engineering topics that we must throw out. To many, this will seem a surprising one. Yes, I am saying 'loose coupling' is undesirable.

==== A common argument

An argument is sometimes stated along these lines: "There must be at least some coupling, otherwise the system wouldn't do anything." Hence we have the common meme about "loose coupling and high cohesion". In this section we show how this argument is false and resolve the apparent dilemma. We will eliminate all forms of design-time coupling except one. That one remaining one is anything but loose and very desirable.



==== Classifying coupling

Think of some instances of dependencies you know of in a system and try to classify them into these three types by asking when the system would fail without it. 

For example, let's say that data flows from an ADC (analog to digital converter) to a display as part of a digital thermometer. At run-time, both must exist. At compile-time both must have the same method signature:

[plantuml,file="diagram-01.png"]
----
@startdot
digraph foo {
// size="4!"
graph [rankdir=LR]
ADC -> display [dir=forward, arrowhead=open, color=red]
}
@enddot
----

Or the display may tell the ADC when to do the conversion. At run-time there is temporal coupling. 

[plantuml,file="diagram-02.png"]
----
@startdot
digraph foo {
// size="4!"
graph [rankdir=LR]
ADC -> display [dir=both, arrowhead=none, arrowtail=open, color=red]
}
@enddot
----

In this one there is an association from a Customer class to an Account class to facilitate communication between them. At run-time there is coupling. At compile-time there is coupling too - the type of the Account class must be exactly the same as expected by the Customer class:

////
[plantuml,file="diagram-03.png"]
----
@startdot
digraph foo {
// size="4!"
graph [rankdir=LR]
Customer [shape=box]
Account [shape=box]
Customer -> Account [dir=forward, arrowhead=open, color=red]
}
@enddot
----
////

[plantuml,file="diagram-04.png"]
----
scale 2
class Customer
class Account
Customer->Account
----


In all the above diagrams, relationships shown in red indicate they are disallowed by the ALA constraints. Green is for desirable relationships, of which there is only one. When we disallow all these types of coupling, the modules, components, functions and classes can now be abstractions.


==== Run-time, Compile-time and Design-time

A few times already in the article, I have sneaked in a magic qualifier, 'design-time'. You know how we sometimes talk about run-time and compile-time with reference to binding. In ALA we recognise that understandability, complexity, etc, are all happening at design-time. By design-time I mean any time you are reading code, writing code, or changing code.

At run-time, the CPU processes data. At compile-time, the compiler processes code. At design-time the brain is processing abstractions. 

In conventional code, it is common for all forms of coupling, run-time, compile-time, and design-time, to appear as coupling between modules or classes.

You can work out what type of dependency you have by when it first breaks. A run-time dependency doesn't break until the program runs. The program can still be compiled and it can still be understood. 

A compile-time dependency first breaks at compile-time. At design-time the code can still be understandable. 

A design-time dependency prevents code from even being understood. The code loses its meaning. 

==== Zero coupling

When we say _no_ loose coupling, it means there is zero coupling between the details and knowledge contained in any two abstractions. Abstractions are free floating little independent programs.

Since our conventional programs are typically full of coupling of all sorts, this constraint on the architecture will obviously change how we write programs significantly. But surprisingly, things quickly get easier, not harder with these constraints.


==== Knowledge dependencies

The one form of coupling allowed in relation to abstractions is from the hidden code of one abstraction to another abstraction. It is always one directional. Because it is always directional, we call it a dependency rather than coupling. From now on it will be referred to as a 'knowledge dependency'. Elsewhere it is referred to as semantic coupling. Since it's the only relationship we have between abstractions, it's obviously important and we will be giving it a lot of attention in following sections. It will drive the layering structure, which in turn gives rise to the name 'Abstraction Layered Architecture'. 

A knowledge dependency is the code implementation of one abstraction requiring knowledge of another abstraction to be understood. Without the knowledge, the code using the abstraction would lose meaning and no longer make any sense to a human reader. It could not even be written in the first place. 

==== Knowledge dependencies always go down

In ALA, knowledge dependencies must be from inside an abstraction that is less abstract to one that is more abstract. We will always put abstractions that are more abstract lower down in the layers. In everyday design, knowledge dependencies are not normally drawn. You simply use the abstraction by its name. But in this article, just so we can explain the meta-architecture, we will sometimes draw knowledge dependencies like this (always downward).

[plantuml,file="diagram-05.png"]
----
@startdot
digraph foo {
// size="3!"
C -> A [dir="both", arrowhead="open", arrowtail="diamond", color=green]
}
@enddot
----

This represents that the implementation of abstraction C knows about abstraction A. A is more abstract than C. C and A cannot therefore be peers, as was the case with the components above. Peer abstractions cannot have any coupling with one another.

==== Whole-Part pattern

If you are familiar with the Whole-Part pattern, ALA uses it extensively. But there is a constraint. The Whole-Part pattern is only used with knowledge dependencies (since that is the only relationship you are allowed). It may of course be used in other forms inside an abstraction, provided it is completely contained in a single abstraction.

A real world example of the Whole-Part Pattern with knowledge dependencies is Molecules and Atoms. A water molecule, for example, is the whole. 

[plantuml,file="diagram-06.png"]
----
@startdot
digraph foo {
// size="5!"
edge [color=green]
H2 [label=H]
Water -> O [dir="both", arrowhead="open", arrowtail="diamond"]
Water -> H [dir="both", arrowhead="open", arrowtail="diamond"]
Water -> H2 [dir="both", arrowhead="open", arrowtail="diamond"]
}
@enddot
----

Oxygen and hydrogen are the parts. Note that oxygen and hydrogen are abstractions, and they are more abstract than water because they are more ubiquitous, more reusable and more stable (as a concept) than any specific molecule. We could make a different molecule but still use exactly the same oxygen and hydrogen as parts to compose the new molecule.

NOTE: When we use the word 'ubiquitous', it refers to the number of times the abstraction is used in a Whole-Part pattern to make other abstractions. It doesn't refer to the number of abstractions that are instantiated. So just because there is a lot of water, that doesn't make the abstraction ubiquitous. In comparing the abstraction levels of Oxygen and Hydrogen with water, Oxygen and Hydrogen are more ubiquitous because they are used to make more abstractions than water is.  

The molecules and atoms analogy with ALA is very close, and we will return to it when we come to explain in more detail how run-time and compiler-time dependencies are moved inside a single abstraction.

For now we just need to remember that we are using the whole-part pattern with knowledge dependencies only. At design-time, the whole is explained and reasoned about in terms of the parts, just as the water molecule is in terms of the oxygen and hydrogen.

==== Run-time/design-time congruence

A software program can be temporally confusing. Everything that happens at design-time is in preparation for what will happen at run-time. Our low-level imperative languages tend to keep the two congruent. The statements in the program at design-time follow in the same order as they will execute at run-time. The only difference between the two is a time shift and the speeding up of the clock.

When we want the knowledge of run-time dependencies to be moved inside another abstraction, this congruence between design-time and run-time must be broken. Unfortunately, developers start out by learning a low-level imperative language, so it becomes unnatural to them to architect their programs without this congruence. Indeed, breaking this congruence needs a pattern to be learned, and then carefully protected from the temptations of our imperative languages. I call it the Ẃiring pattern'.

Before going into the pattern, we need to round out the most important aspects of ALA.


==== Wiring pattern - Part one

We now introduce the pattern that both solves the congruence problem just discussed in the previous section, and provides the alternative to all those disallowed coupling types discussed earlier. This pattern is usually an important part of ALA. 

Note: The wiring pattern is not necessarily a part of an ALA architecture. For example, if your whole problem is just an algorithm, and therefore suits a functional programming style, then you can still compose abstractions with function abstractions, provided all function calls are knowledge dependencies, and not say, just passing data or events. 

If you are using monads, especially I/O monads, or RX (reactive extensions), especially with hot observables, you are already using the wiring pattern. The pipes and filter pattern is also an example of the wiring pattern. Labview or Node-Red can use the wiring pattern. There are many other examples of the wiring pattern. Most support a data-flow programming paradigm. Here we generalize the pattern to support any programming paradigm. 

The wiring pattern may be the same as the "Component Pattern" in some literature if used with what is referred to as 'configurable modularity' or 'abstracted interactions'. 

The wiring pattern allows lines on your application diagram to mean any programming paradigm you want that express your requirements. It also allows you to implement multiple programming paradigms together in the same diagram.

If you are using dependency injection with explicit code for the wiring (not auto-wiring), then you are half way there. 

The wiring pattern separates design-time/run-time congruence. It works by having a 'wiring-time' that is separated from run-time. 'Wiring-time' can happen any time before run-time. It can happen immediately before it, as for instance in LINQ statements or RX with a cold observable. It becomes powerful when we make wiring-time congruent with design-time. Usually the wiring code will actually run at initialization time, when the program first starts running. That initialization code becomes the architectural design.   

Let's suppose you have designed your system with two modules, A and B. There will be one of each in your system.

[plantuml,file="diagram-07.png"]
----
@startdot
digraph foo {
// size="4!"
graph [rankdir=LR]
A -> B [color=red]
}
@enddot
----

At run-time we know that A will talk to B. So we design A to have an association with B. The association may first appear on a UML model, or it may go straight into the code something like this:


 static component A
 {
    B_method();
 }

 static component B
 {
    public B_method() { }
 }

A and B may be implemented as non-static, with only one instance of each. The association is still there.

 component A
 {
    private var b = new B();
    b.method();
 }

 component B
 {
    public method() { }
 }

A may create B itself, which is a composition relationship, as above. Or A may have a local variable of type B passed in by some kind of dependency injection, which is still an association relationship.

 component A
 {
    B b;
    public setter(B _b) {b = _b}
    b.method();
 }

Note that although dependency injection was used, it only eliminated part of the dependency, that of which particular subtype of B it is going to talk to, but A still knows the general type B, which is not allowed in ALA. (Part of the problem here is that A and B were probably arrived at by decomposition, and so they have subtle knowledge of each other, for example of how they collaborate.)

If A and B are collaborating, they are not abstractions. Their knowledge of each other at design-time (to enable their relationship at run-time) binds them to each other so that neither can be reused in any other way. And if they can't be reused, they can't be abstract. 

Let's revisit the water molecule analogy we discussed earlier for the Whole-Part pattern, and develop it further to be clearer how these dependencies affect abstractions. Let's say we have decomposed water into two components, Oxygen and Hydrogen. Oxygen will talk to Hydrogen to get an electron, so we write:

 component Oxygen 
 {
    var h1 = new Hydrogen();
    var h2 = new Hydrogen();
    h1.getElectron();
    h2.getElectron();
 }

The diagram for that looks like this:

[plantuml,file="diagram-08.png"]
----
@startdot
digraph foo {
// size="4!"
edge [color=red]
H2 [label=Hydrogen]
Oxygen -> Hydrogen [dir="both", arrowhead="open", arrowtail="diamond"]
Oxygen -> H2 [dir="both", arrowhead="open", arrowtail="diamond"]
}
@enddot
----

In the real world, oxygen is a very useful abstraction for making other molecules. In writing code this way to make water, we have tied it to hydrogen. Oxygen can't be used anywhere else, at least not without bringing with it two hydrogens, rendering it useless. By implementing the Oxygen-Hydrogen relationship needed to make water in oxygen, we have destroyed the oxygen abstraction. We never even made the water abstraction. To understand water, we would have to read the code inside oxygen, where the parts about water have become entangled with the inner workings of oxygen, protons and neutrons and all that stuff. Oxygen is also used to make caffeine. We could never make coffee!

image::caffeine%20molecule.png[Caffeine molecule,300,title="caffeine - oxygen atoms are red"]

Abstractions are fragile and get destroyed easily, so we have to take care to protect them. What we needed to do was to put the knowledge about the relationship between oxygen and hydrogen to make water in a new abstraction called Water.

[plantuml,file="diagram-09.png"]
----
@startdot
digraph foo {
size="2!"
edge [color=green]
Water -> Oxygen
Water -> Hydrogen
}
@enddot
----


In general, to break coupling between peer modules A and B, we move the knowledge of the coupling to a higher level abstraction (less abstract level) where it belongs. Let's call it C. C is a more specific abstraction. The knowledge is encapsulated there - it never appears as a dependency of any kind. And it is cohesive with other knowledge that may be contained inside abstraction C.

[plantuml,file="diagram-10.png"]
----
@startdot
digraph foo {
// size="4!"
graph [rankdir=LR]
A -> B [color=red]
}
@enddot
----

becomes


[plantuml,file="diagram-11.png"]
----
@startdot
digraph foo {
// size="4!"
edge [color=green]
C -> A
C -> B
}
@enddot
----

The diagram above is only to show the ALA knowledge dependency relationships between the three abstractions. It doesn't yet show explicitly that an instance of Abstraction A will be wired to an instance of Abstraction B. In practice we never actually draw knowledge dependencies. We are just doing so here to show how ALA works. We would draw it in this way instead:

[plantuml,file="diagram-12.png"]
----
@startdot
digraph foo {
size="2!"
graph [rankdir=LR]
subgraph cluster_C {
label=C
style=rounded
A -> B [color=green]
}
}
@enddot
----

[plantuml,file="diagram-13.png"]
----
@startdot
digraph foo {
size="2!"
graph [rankdir=LR]
A -> B [style=invis]
#a -> b [color=red]
}
@enddot
----

Now we have the explicit wiring. It looks a lot like the original diagram where we had no C. But where the knowledge is coded is very different. Because it is C and not A that has the knowledge of the relationship between A and B, Abstractions A and B do not change. They continue to know nothing of the connection. They remain abstractions. They remain re-usable.

It may seem at first that adding the extra entity C is a cost, but in fact C is an asset. It shows the structure of the system. It shows it explicitly. It shows it in one small understandable place. And it is executable - it is not a model.

The original abstractions were left below C to show that they still exist as free abstractions to be used elsewhere. They are not contained by C in any way as modules from a decomposition process would be. The A and B inside C are only instances. We wouldn't normally bother to draw the abstractions below. So we just draw this:

[plantuml,file="diagram-14.png"]
----
@startdot
digraph foo {
size="2!"
graph [rankdir=LR]
subgraph cluster_C {
label=C
style=rounded
A -> B [color=green]
}
}
@enddot
----

C must achieve the connection between A and B either at compile-time or run-time. With current languages, the easiest time to do this is at initialization time, when the program first starts running. This is similar to dependency injection, except that we are not going to inject the instance of B into A.  

This is what the code inside C might look like:

 Abstraction C
 {
    var a = new A();
    var b = new B();
    a.wireTo(b);
 }

Typically we will write the code using the fluent pattern, with the wireIn method always returning the object on which it is called, or the wireTo method always returning the object wired to. The constructor already returns the created object by default. 

 Abstraction C
 {
    new A().wireTo(new B());
 }

If A and B are static modules, this produces something like:

 Abstraction C
 {
    A_setcallback(B_method);
 }


==== Wiring pattern - part two 

We are half-way through explaining the wiring pattern. Now we turn our attention to how A and B can communicate without knowing anything about each other. 

This part of the pattern is also called "Abstract Interactions"

Of course, one way is that C acts as the intermediary. This way is less preferred because it adds to C's responsibilities. But it is sometimes necessary if there are some abstractions brought in from outside. Such abstractions will 'own' their own interfaces or may come with a contract which C will have to know about. C will usually have to wire in an adapter, or handle the communications between the two abstractions itself.

A better way, because it leads to an architectural property of composability, is that A and B know about a 4th abstraction that is more abstract than either of them. This is legal because it is a design-time knowledge dependency.  Let's call it I. 

[plantuml,file="diagram-15.png"]
----
@startdot
digraph foo {
edge [color=green]
size="2!"
C -> A
C -> B
A -> I
B -> I
}
@enddot
----

I is an interface of some kind. It may or may not be an actual artefact. What it must be is knowledge that is more abstract than A and B and therefore knows nothing of A and B. It is more ubiquitous and more reusable than A and B are. In other words we can't just design I to meet the particular communication needs of A and B. That would cause A and B to have some form of coupling or collaboration with each other, and again destroy them as abstractions. 

I is so abstract, ubiquitous and reusable, that it corresponds to the concept of a programming paradigm. We will cover programming paradigm abstractions in following sections because they are a critically important part of ALA. We will see that ALA is polyglot with respect to programming paradigms.

image::circuit%20diagram.gif[Schematic, title="In an electronic schematic, the components are abstractions that are composed using two paradigm interfaces - live analog signals and live digital signals"]

Returning to a software example, let's choose a single simple programming paradigm: activity flow. This programming paradigm is the same as the UML Activity diagram. When we wire A to B and they use this paradigm, it means that B starts after A finishes. If A and B accept and provide this interface respectively, then wiring them together by drawing an arrow will have that meaning, and cause that to happen at run-time.

[plantuml,file="diagram-16.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
size="2!"
subgraph cluster_C {
label=C
style=rounded
A -> B [label="activity flow", color=green]
}
}
@enddot
----
It is easy to create an interface for the activity-flow programming paradigm. It has a single method, let's call it 'start'. Many abstractions at the level of A and B can either provide or accept this paradigm interface. Then instances of them can be wired up in any order and they will follow in sequence just like an Activity diagram. 

Note that the Activity Diagram is not necessarily imperative in that any Activity can take an amount of time to complete that is not congruent with the actual CPU execution of code. In other words activities can be asynchronous with the underlying code execution, and for example, delay themselves during their execution, or wait for something else to finish, etc.  

The code in Abstraction A could look something like this. Don't take too much notice of the exact method used to accomplish the wiring. There are many ways to do this using only knowledge dependencies. The important thing is that A continues to know nothing about its peers, continues to be an abstraction, and yet can be wired with its peers to take part in any specific activity flow sequence:

....
 Abstraction A : IActivity
 {
    private IActivity next = null;
    
    public IActivity wireTo(IActivity _next) 
    {
        next = _next;
        return _next;
    }
    
    IActivty.start()
    {
        // start work
    }
    
    // code that runs when work is finished.
    // may be called from the nd of start, or any time later
    private finishedWork()
    {
        if (next!=null) next.start();    
    }
 }
....

Abstraction A both _provides_ and _accepts_ the interface. This allows it to be wired before or after any of its peer abstractions. In ALA we use the word 'accepts' rather than 'requires' because there is often an end to a chain of abstractions wired together. If no next interface is wired in, the activity flow ends. 

Abstraction B would be written in the same way, as it also knows about the Activity flow interface:
....
 Abstraction B : IActivity
 {
    private IActivity next = null;
    
    public IActivity wireTo(IActivity _next) 
    {
        next = _next;
        return _next;
    }
       
    IActivty.start()
    {
        // start work
    }
    
    // code that runs when work is finished.
    // may be called from the end of start, or asychronously later
    private finishedWork()
    {
        if (next!=null) next.start();    
    }
 }
....

NOTE: As an aside, in some C# experiments, we wrote wireTo as an extension method for all objects. It used reflection to look at the interfaces provided by the class, and the private local interface variables in the class. It would then match up the interface types and do the wiring automatically. It could even set up reverse direction interfaces so that complicated programming paradigms that required method calls going in both directions could be wired by the one wireTo operation.   

Now let's revisit the molecule analogy. By now we would know to put the knowledge that Oxygen is bonded to two Hydrogens inside the water abstraction where it belongs.

[plantuml,file="diagram-17.png"]
----
@startdot
graph foo {
size="2!"
graph [rankdir=LR]
subgraph cluster_C {
label=Water
style=rounded
edge [color=green]
H2 [label=Hydrogen]
Oxygen--Hydrogen
Oxygen--H2
}
}
@enddot
----

In terms of knowledge dependencies it means this:

[plantuml,file="diagram-18.png"]
----
@startdot
digraph foo {
size="2!"
edge [color=green]
Water -> Oxygen
Water -> Hydrogen
Oxygen -> PolarBond
Hydrogen -> PolarBond
}
@enddot
----

The programming paradigm here is a polar bond. It is more abstract (more ubiquitous and reusable) than any particular atom.  We could have a second programming paradigm, a covalent bond, as well. Again, the important thing here is not what the code does - that is arbitrary (and not actually correct chemistry) but how the atoms can be made to interact while retaining their abstract properties with only design-time knowledge dependencies:


 Abstraction PolarBond
 {
    GiveElectron();
 }

....
 Abstraction Oxygen
 {
    private PolarBond hole1 = null;
    private PolarBond hole2 = null;
    
    public Oxygen wireIn(PolarBond _pb) 
    {
        if (hole1==null) hole1 = _pb; else
        if (hole2==null) hole2 = _pb;
        return this;
    }
       
    public Initialize()
    {
        if (hole1!=null) { hole1.getElectron(); BecomeNegativelyCharged(); }
        if (hole2!=null) { hole2.getElectron(); BecomeNegativelyCharged(); }
    }
 }
....
....
 Abstraction Hydrogen : PolarBond
 {
    PolarBond.getElectron()
    {
        BecomePositivelyCharged();
    }
 }
....
....
 Abstraction Water
 {
    new Oxygen()
        .wireIn(new Hydrogen())
        .wireIn(new Hydrogen())
        .Initialize();
 }
....

Let's do one more example, this time with a Data-flow programming paradigm. I have found that data-flow is the most useful programming paradigm in practice. It suits a large range of problems. 

Let's construct a thermometer. Assume we already have in our domain several useful abstractions: an ADC (Analog Digital Converter) that knows how to read data from the outside world, a Thermistor abstraction that knows how to linearise a thermistor, a Scale abstraction that knows how to offset and scale data, a filter abstraction that knows how to smooth data, and a display abstraction that knows how to display data.

We also know that these abstractions all support the Data-flow programming paradigm.

So we can go ahead and create a Thermometer application just by doing this:

[plantuml,file="diagram-19.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label=Thermometer
style=rounded
#node [style=rounded]
node [shape=Mrecord]
ADC [label="<f0> ADC|<f1> Port=2|<f2> Pin=3|<f3> Frequency=1kHz"]
Thermister [label="<f0> Thermister|<f1> Type='K'|<f2> InputRange=20-1023"]
Scale [label="<f0> Scale|<f1> Offset=32|<f2> Slope=0.013"]
Display [label="<f0> FloatDisplayField|<f1> Digits=4|<f3> Decimals=1"]
ADC -> Thermister -> Scale -> Display
}
}
@enddot
----

Note that we configure all the abstraction instances for use in the Thermometer by adding configuration information into rows on the instances.

When we manually compile the diagram (assuming we don't have automated code generation), it might look something like this (again using fluent coding style):

 Abstraction Thermometer
 {
    new ADC()
        .setPort(2).setPin(3),setFrequency(1000)
        .wireTo(new Thermister().setType('K').setInputRange(20,1023))
        .wireTo(new Scale().setOffset(32).setSlope(0.013))
        .wireTo(newDisplay().setDigits(4).setDecimals(1));
 }

NOTE: The configuration setter returns the object on which the call is made, but the wireTo method returns the object passed in so that you can wire up a chain of data-flow abstraction instances by stringing them together.

The diagram is the architecture of the specific application, Thermometer, but is also executable.

The diagram has all the cohesive knowledge that is a thermometer, and no other knowledge.

The diagram can be read stand-alone, because all the dependencies in it are knowledge dependencies on abstractions we would already know in the domain.

Let's say when the Thermometer runs, there is a performance issue in that the ADC is producing data at 1kHz, and we don't need the display to be showing Temperatures at that rate. Also the temperature readings are noisy (jumping around). Let's make a modification to the Thermometer by adding a filter to reduce the rate and the noise: 

[plantuml,file="diagram-20.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label=Thermometer
style=rounded
#node [style=rounded]
node [shape=Mrecord]
ADC [label="<f0> ADC|<f1> Port=2|<f2> Pin=3"]
Filter [label="<f0> LowPassFilter|<f1> Cutoff=1000"]
Thermister [label="<f0> Thermister|<f1> Type='K'|<f2> InputRange=20-1023"]
Scale [label="<f0> Scale|<f1> Offset=32|<f2> Slope=0.013"]
Display [label="<f0> FloatDisplayField|<f1> Digits=4|<f3> Decimals=1"]
ADC -> Filter -> Thermister -> Scale -> Display
}
}
@enddot
----

If the domain abstractions are not already implemented, we have got the architecture to the point where we can ask any developer to implement them, provided we first give them knowledge of ALA and of the programming paradigm(s) being used.

But let's look how the data-flow paradigm might work.

NOTE: If you are familiar with RX (Reactive extensions) with a hot observable source (which is an example of the wiring pattern), this is similar in concept although RX tries to have duality with for-loops iterating through the data. The data-flow paradigm we set up here will just be a stream of data. The IDataFlow interface corresponds to IObserver, and the wireTo method corresponds to the Subscribe method.

NOTE: The ideal would be a language where we don't have to decide if the data-flow will be push or pull, synchronous or asynchronous, buffered or unbuffered or other characteristics of communications. The abstractions would not need to know these things - they would just have logical I/O ports, and the type of communications could be binded in at compile-time as part of the performance configuration of the system.

NOTE: Later we will introduce an asynchronous (event driven) execution model. It is preferable to do the data-flow paradigm interface using that because it allows better performance of other parts of the system without resorting to threads.    

For simplicity, we will just implement a synchronous push system. Again, don't worry about the filter itself. The code is just there to see how the LowPassFilter fits in with the Data-flow programming paradigm, and how simple doing that can be. 

 Abstraction IDataFlow<T>
 {
    push(T data);
 }

....
 /// LowPassFilter is a Data-Flow paradigm decorator to be used in an ALA archtecture.
 /// 1. Decimates the incoming data rate down by the setCutoff configuration
 /// 2. Smooths the data with a single pole filter with cutoff frequency equall to the input frequency divided by the cutoff. T must be a numeric type.
 /// Normal checks and exceptions removed to simplify
 Abstraction LowPassFilter<T> : IDataFlow<T>
 {
    private Dataflow next = new DataFlow();
    
    public IDataflow wireTo(IDataflow _next) 
    {
        next = _next;
        return _next;
    }
    
    integer cutoff;
    
    setCutoff(integer _cutoff)
    {
        cutoff = _cutoff;
    }
    
    int count = 0;
    T filterState = NAN;
       
    IDataFlow.push(T newData)
    {
        if (filterState==NAN) filterState = newData * cutoff;
        filterState = filterState - filterState/cutoff + newData;
        count++;
        if (count==cutoff)
        {
            count = 0;
            if (next!=null) next.push(filterState/cutoff);
        }
    }
 }
....

You will notice that both the Domain abstraction and the connection interface abstraction used a parameterised type. This makes sense because only the application, the Thermometer, knows the actual types it needs to use. Also, the interface didn't look a lot different from the Activity interface. 

////
Suppose we wanted to do something more with the programming paradigm, let's say to support fan-out of the data so that multiple domain abstractions can be wired to the same data stream output. 
////
////
TBD: change the following code to use an intermediary (framework) abstraction to support fan-out using publish/subscribe, and asynchronous calls (event driven programming paradigm), and allow the framework to work without using parameterised types by using a capsule pattern, just to show that can be done. 


 Abstraction LowPassFilter<T> : IDataFlow<T>
 {
    private Dataflow next = new DataFlow();
    
    public IDataflow wireTo(IDataflow _next) 
    {
        if (next==null) next = new DataFlow();
        next = _next;
        return _next;
    }
    
    integer cutoff;
    
    setCutoff(integer _cutoff)
    {
        cutoff = _cutoff;
    }
    
    int count = 0;
    T filterState = NAN;
       
    IDataFlow.push(T newData)
    {
        if (filterState==NAN) filterState = newData * cutoff;
        filterState = filterState - filterState/cutoff + newData;
        count++;
        if (count==cutoff)
        {
            count = 0;
            if (next!=null) next.push(filterState/cutoff);
        }
    }
 }
////


===== Abstractions and Instances

[IMPORTANT]
====
All *software architectures* should contain *two concepts* for its *elements*  equivalent to *abstractions* and *instances*.
====
Abstractions are design-time elements. Instances are run-time elements. Object oriented programming has the two concepts in classes and objects. But many discussions on software architecture seem to combine them into one term, such as modules, components or layers. They may implicitly contain the separate concepts, as components may, but not having them explicit will inevitably lead to confusion. 

The problem is their different dependencies. Dependencies between Instances are run-time or compile-time dependencies. Dependencies between Abstractions are _only_ knowledge dependencies. If we don't have separate terms for design-time and run-time elements, we will tend to implement run-time dependencies in the design-time elements, turning them into design-time dependencies. 

Nearly all common layering schemes have this problem. Another common example of the problem is associations between classes. The most important idea that OOP brought us, the idea of different design-time elements and run-time elements, has been ruined by associations. They let you implement run-time dependencies between objects in their classes.

What we should be doing is representing the knowledge of run-time dependencies between objects inside another class. By using the terms *Abstraction* and *Instance*, ALA honours the separation of run-time elements and design-time elements. All knowledge of dependencies between instances must be implemented inside another abstraction, so that at design-time they are not dependencies at all.





=== No decomposition into interacting elements


==== "Decomposition of the system"

As discussed in the sections 2, the common software design approach is stated this way:

[WARNING]
====
The [red]#*decomposition*# of your [red]#*system*# into [red]#*elements*# and their [red]#*interactions*#.
====

In ALA, we do the exact opposite. The design method is stated this way:

[TIP]
====
The [green]#*expression*# of your [green]#*requirements*# by [green]#*composition*# of [green]#*abstractions*#.
====

In section 2, we discussed decomposition/composition difference between these two approaches. Here we review that difference, and look at other differences in the wording of the two approaches. 

==== Decomposition creates less abstract elements

The tell-tale sign that this is happening is when we draw hierarchical diagrams. Boxes contained inside boxes. Even if we don't draw them that way, the 'containment' or encapsulation is still implied. This is what package diagrams do. ALA has no use for package diagrams in the logical view. (However, they may be useful in the deployment or physical viewpoints.)

We want dependencies to be in the direction of the more abstract, more ubiquitous, more stable and more reusable. To achieve that, the abstractions that are depended on must not be encapsulated by the parent element. Instead they need to be more in a lower layer where they are meant to be more public.

==== Composition creates more abstract elements

When you are starting a green fields project, when no abstractions have yet been invented for the domain, this is the one method to have foremost in your mind.

"The [green]#*expression*# of your [green]#*requirements*# by [green]#*composition*# of [green]#*abstractions*#."

Just expressing the requirements, without thinking about their implementation, will create elements that will be abstract, and potentially reusable, if not elsewhere in your requirements, then in other applications in your domain. 

==== Decomposing the system



In every creative process outside of software, for example painting or composing music or designing a clock, the process would be much better described as composition of abstractions.

==== "interacting elements"

The elements resulting from any decomposition process are likely to interact or collaborate to accomplish the purpose of the parent element. The design methodology actually includes the word "interacting".

As discussed in the previous section on "no loose coupling", interactions, which are run-time coupling, are not desirable between abstractions. In ALA they become knowledge inside another abstraction.


==== Disappearing structure

However 'decomposition' is accomplished, because the parent element is simply split into pieces, it is likely to be left empty and disappear. So, having decomposed the system iteratively into small elements, the elements themselves may now be manageable, but the system as a whole is lost. Nothing explicit remains to hold the elements together. 

The knowledge of the coupling and collaboration between elements, once cohesive knowledge in the brain of the designer before the decomposition, is now distributed inside the pieces themselves. For example, the pieces will send messages to each other, knowing who should get the message. Or two elements will have collaborative knowledge about the meaning of a message sent and received. Or they will subscribe to messages from another, knowing whom they need to get messages from. Understanding the system requires reading inside the pieces to get all this system level knowledge.

To mitigate the problem, we typically aim to make the small pieces 'loosely coupled' so that when the pieces are viewed as a system, that system will be understandable. However, even in a loosely coupled system, this doesn't necessarily work. Understanding the system as a whole still requires you to understand how _all_ the little pieces collaborate, to whatever extent they are coupled. And that coupling and collaboration knowledge is mixed in with all the other details inside the elements.

The more loosely coupled it is, the harder it is to ascertain the structure from this inside information. The more coupled it is, the more complex is the whole system. Consequently, there is a meme floating around that decoupling and clear system structure are in conflict. You can have one or the other but not both. That is why indirection is seen as a two-edged sword.

[TIP]
====
In ALA, there is no conflict between decoupling and explicit structure.
====




==== Expression of requirements

[TIP]
====
One of the fundamental parts of ALA is the succinct [green]#*expression*# of [green]#*requirements*# in the same way that a DSL or fluent code does.
====

I noticed that in the 40 years of code bases written at our company, two did not deteriorate under maintenance, and remained as easy to maintain as ever, while others deteriorated badly. Some deteriorated so badly that they could no longer be maintained at all. How code would turn out was accidental at the time. It seemed as if you just got lucky or unlucky as to whether any particular changes that came along would be easy or hard. But two code bases that were easy seemed to be easy for any kinds of change. And the parts that were hard were hard for any change. This would continue to hold for years on end. Of course, most changes were changes to requirements, but often enough, changes would be for performance or other reasons. These also seemed easy in these two code bases.

I then looked at the properties of the easy and hard code. The easy code was not even complicated while the hardest code had degenerated well into the complex. The two easy code bases were doing very different things in very different ways. But they had one thing in common. The code that represented the knowledge from the requirements was separate, it _only_ contained knowledge from requirements, and it was expressed in terms of other things that were reusable and easy to understand, in other words abstractions. 
 
This gives rise to one of the core tenets in ALA. The first separation is not along lines of functional parts of the system, such as UI, Business logic, and Data model. The first separation is the code that has the requirements knowledge. In the first sprint of a new application, the method begins with the question: How can I represent this requirement knowledge, precisely and fluently, in terms of abstractions that I will invent as I go?

The end result has a strong parallel with how DSLs work. However, in ALA we don't go as far as an external DSL, and we don't try to create a sandbox language for writing applications. We just express the requirements knowledge in terms of abstractions and then trust that those abstractions, when written, will take care of how it will all work.

It is a little bit like how a (DOM) Document Object Model represents the knowledge contained in some XML. The wired up domain class objects represent the knowledge of a requirement, without regard to how it will work. The code inside these classes is then written to actually make it work.


==== Expressiveness

Requirements are usually understated initially in terms of abnormal conditions. However, they are usually communicated quite quickly relative to the time to write the code. In ALA, they are separately represented. The precise expression of the requirements using the right programming paradigms should take about the same amount of information as the English explanation of them.

In general, ALA probably requires about the same amount of total code. But once the requirements are represented, the domain abstractions are known and they are independent small programs with dependencies only on the programming paradigm interfaces used. This independence should make them much easier to write. As the system matures, the effort to modify gets less as more domain abstractions come on line as tested, mature and useful building blocks. The final cost of maintenance should be much less than an equivalent ball of mud architecture.





=== No models

[IMPORTANT]
====
Leave out details only inside abstractions
====

It is generally accepted that a software architecture must, by necessity, leave out some details. Somehow we need to find a satisfactory architecture without considering all the details. Often models are used to represent the architecture. Like its metaphor in the real world, a model leaves out details. The problem is they can leave out arbitrary details. We can't be sure that some omitted detail won't turn out to be important to the architectural design.

ALA therefore does not use the model metaphor. Instead, it uses diagrams (if not plain old text). Of course, this distinction comes down to semantics. I define a diagram as different from a model in that it does not leave out details arbitrarily. The only way to leave out details in an ALA diagram is inside the boxes, in other words inside abstractions. Because abstractions already have the required meaning when used in the diagram, the details omitted can't be important to the diagram, and can't affect the architectural design.

==== Executable architecture
[IMPORTANT]
====
Your architecture should be executable
====

The distinction between diagrams and models explained in the previous section gives rise to an interesting property of the ALA architecture. Diagrams are executable. Therefore the architecture itself will be executable. When the implementation of the abstractions is complete, there will be no work left to do to make the architecture execute (apart from practical considerations of bugs, misinterpretations of the requirements, performance issues, improvements to the initially conceived set of domain abstractions, and the like).

There should be two aspects of an architecture, the meta-architecture and the specific architecture. If using ALA, ALA itself is the meta-architecture and the top level application diagram is the specific architecture.  

If your specific architecture is executable, it is also code. There is no separate documentation or model trying to act as a second source of truth.

==== Granularity

The final architecture of your software will consist only of abstractions. These abstractions will need to be independently readable and understandable. To meet this need, all of the abstractions will be small, even the 'system level' ones.   

Conversely, none should be too small. We want them small enough to allow the human brain to understand them, but there is no need for them to be smaller, or we will just end up with an inordinate number of them. This inordinate number will tax the brain in a different way, by causing it to have to learn more abstractions than necessary in a given domain.

The ideal abstraction size is probably in the range of 50 to 500 lines of code, depending on its internal complexity.


==== Modules, Components, Layers 

The common terms, modules, components, or layers often result from a decomposition process and therefore are parts of a specific system. The system may have only one of each type. The parts have a lower abstraction level than the system because they are just specific parts of it. In ALA we want to reverse this so that parts are more abstract than the system. 

But say you do end up with some single use abstractions and implement it in a static way, it is important to still see these entities as two aspects in one: an abstraction and an instance.

////
A and B have two aspects, the design-time aspect and the run-time aspect. This is exactly analogous to classes and objects. Even if you intend to have only one of a module or component, we still need to think about it in these two different aspects. In ALA we wont call these aspects classes and objects. We will instead call them Abstractions and Instances (first letter capitalized). The reason ís that classes and object carry with them a lot of baggage, such as associations and inheritance, which we are not allowed in ALA. We need a clean start. We want to remember that we have zero coupling by calling them Abstractions. So now we have Abstraction A and Abstraction B and Instance a and Instance b. When we have only one instance, A and a are two aspects of the same entity, as is B and b.

At runtime, Instances a and b will be communicating:

This knowledge that Instances a and b will be communicating at run-time must of course be represented somewhere at design-time. But we must not put that knowledge into either Abstraction A or Abstraction B, or we will destroy them as abstractions, like what happened to oxygen. The knowledge must go inside a 3rd Abstraction, C.
////
////
The A and B inside C are the Instance aspect of A and B. Even if A and B are never actually explicitly instantiated (because they are written as static modules), C still deals with their Instance aspect. If A and B are written in such a way that they need to be explicitly instantiated, C will do that.  
////

=== Layers

==== Layers pattern

With only design-time knowledge dependencies to deal with, layers are used for organising these dependencies so that there are no circular dependencies, and that they all go toward more abstract, more stable abstractions. As the name "Abstraction Layered Architecture" suggests, layers are crucially important to ALA.

In the section on the wiring pattern we ended with three layers:

[plantuml,file="diagram-21.png"]
----
@startdot
digraph foo {
edge [color=green]
size="2.5!"
fontsize=6
fontname=Ariel
labeljust=l
subgraph cluster_1
{
label="Features layer"
C
}
subgraph cluster_2
{
label="Domain Abstractions layer"
A
B
}
subgraph cluster_3
{
label="Programming Paradigms layer"
I
}
C -> A
C -> B
A -> I
B -> I
}
@enddot
----


There is a Layers pattern that also controls dependencies, but since most systems have numerous run-time dependencies between elements represented as design-time dependencies, these layers are used for the run-time dependencies. It is usually explained that each layer is built on services provided by the layer below it. 

One example is the UI/Business Logic/Data model. Another example is the OSI communications model, where the layers are Application, Presentation, Session, Transport, Network, Data link, and Physical. In ALA, each of these ends up being turned 90 degrees. Metaphorically they become chains. In ALA each component wouldn't know about the components next to it. That applies symmetrically, to the left and to the right. Data goes in both directions. At run-time, everything must exist for the system to work. It doesn't really make sense to use a asymmetrical layers metaphor.

The design pattern for layers does have one or two examples of layering used by knowledge dependencies. The term ‘layer’ is therefore an overloaded term in software engineering. When used for knowledge dependencies, the English term 'layer' is a better metaphor. If a lower layer of a wall were to be removed, the layers above would literally collapse, and that's exactly what would happen in knowledge dependency layering. The layers above literally need the knowledge of abstractions in lower layers to make any sense.

ALA's ripple effects are already under control because the only dependencies are on abstractions, which are inherently stable, and furthermore, those abstraction must be more abstract. However, to make these dependencies even less likely to cause an issue during maintenance, we try to make the abstraction layers discrete, and separated by approximately an order of magnitude. In other words each layer is approximately an order of magnitude more abstract than the one above it. More abstract means more ubiquitous, so the layers contain abstractions which have greater scope, and greater potential reuse as you go down the layers. 

We won't need many layers. If you think about abstraction layers in the real world, we can get from atoms to the human brain in four layers. Remember the creativity cycle early in this article. We only need to go around the cycle four times to make a brain: Atoms, Molecules such as proteins, Cells such as neurons, neural nets, and finally the brain itself.   

==== The four layers

We start with four layers. They have increasing scope as you go down. This type of layering was described by Meiler Page-Jones. Meiler Page-Jones’ names for the four layers are: "Application domain", "Business domain", "Architecture domain", and "Foundation domain". 

image::Layers.png[Layers diagram, title="Four ALA layers", width=75%]

////
[ditaa,file="Diagram-03.png"]
--
Specialized
  
  |       Application layer        |
--+--------------------------------+--
  |   Domain Abstractions layer    |
--+--------------------------------+--
  |  Programming Paradigms layer   |
--+--------------------------------+--
  |         Language layer         |
  V                                v
  
Increasing abstraction            Dependencies
Increasing ubiquity
Increasing reuse
Increasing stability
--
////




ALA uses slightly different names: Application layer, Domain Abstractions layer, Programming Paradigms layer, and Language layer.

===== Application layer

The top layer has knowledge specific to the application, and nothing but knowledge specific to the application, i.e. representing your requirements.

A simple Application might wire a grid directly to a table. When Business logic is needed, any number of decorators (that do validation, constraints, calculations, filtering, sorting, etc.) can be inserted in between the grid and the table by changing the wiring of the application. 

===== Domain abstractions layer

Knowledge specific to the domain goes in this layer. A domain might correspond to a company or a department. As such, teams can collaborate on the set of abstractions to be provided there.

Applications have knowledge dependencies reaching into this layer. 

===== Programming Paradigms layer

All knowledge specific to the types of computing problems you are solving, such as execution models, programming paradigm interfaces and any frameworks to support these, is in this layer.

The Programming Paradigms layer will abstract away how the processor is managed to execute different pieces of code at the right time. Execution models are covered in detail in another article here TBD.

This layer is also where we arrange for our domain abstractions to have common simple connections instead of having a specific language for each pair of modules that communicate. The Programming Paradigms layer abstracts away ubiquitous communications languages (which we have been referring to as programming paradigms in this article.) 

Let's use the clock as a real world example. (This is the same clock example we used in section 2.9 when introducing the role abstractions play in the creative process.) One of the the domain abstractions for clocks is a cog wheel. Cog wheels communicate with one another. But they don't do it with communications languages specific to each pair, even though each pair must have the correct diameters and tooth sizes to mesh correctly. The cog abstraction just knows about the common paradigm of meshing teeth, a more abstract language in this lower layer. This language is analogous to a programming paradigm. With it, the clock abstraction (which is in the highest layer) can then instantiate two cogs and configure them to mesh. The concept of cog thus remains an abstraction and instances of it are composable. The clock, which already knows that two instances of cogs are needed, also knows where they will be fitted and what their diameters must be. The knowledge in the clock abstraction is cohesive. 

===== Language layer

The language layer is included to show what is below the other three layers. It is not hardware as you would find in many other layering schemes, nor is it a database, because it is not run-time dependencies we are layering. The lowest layer has the remaining knowledge you need to understand your code, that of the languages, libraries and any very generic APIs you may use.

The hardware and database do have a place, but we will cover it later. Being a run-time dependency, it will be well off to one side and slightly higher up.

===== Domain Abstractions API

The boundary between the application layer and the domain abstractions layer is an API that supports the solution space of your requirements (within the constraints of your domain).

The scope of the Domain Abstractions layer defines the expressiveness available to the application. The greater the scope (or bigger the domain), the more applications are able to do. The cost is expressiveness. The applications will have to be longer to specify what is to be done. Conversely, a smaller domain allows less versatility in the applications, but there is greater expressiveness, which means you write less code. 

===== Possible extra layers

The domain is an approximation of all the potential applications and all the modifications you are likely to make. If the domain is large because it is enterprise wide, you could have an additional layer for small domains. The enterprise domain would include enterprise wide abstractions such as a person identity, and the smaller domains would add additional, more specific abstractions, such as a customer (by composition).

If the applications are large and themselves need to be composed of features, an additional layer that supports plug-in style abstractions may work well. Plug-in abstractions may actually be instances of domain abstractions, such as a settings Menu, or a customer Table. A feature can then add settings to the menu, or columns to the table that remain unknown to any other features.

===== Programming Paradigms API

The boundary between all higher layers and the Programming Paradigms layer is another API. It separates the domain knowledge from the programming paradigm implementation knowledge. It almost always takes care of the ‘execution flow’, the way the computer CPU itself will be controlled to execute all the various parts of the code and when, often using a framework. On the other hand, the Programming Paradigms layer doesn’t necessarily have any code at all. Remember that the layers are ‘knowledge dependencies’, not run-time dependencies, so the paradigm could be a ‘computational model’ that just provides the knowledge of patterns of how to constracut the code in higher layers. The decisions about use of the patterns and about the way the code is executed have already been made and exist in the Programming Paradigms layer.

===== Rate of change of knowledge

The knowledge in each of the four layers has different change rates. 

* The Language layer contains knowledge that will likely change only a few times in your career. 

* The Programming Paradigms layer knowledge changes when you move to different computing problems types, or discover different approaches to solving a broad range of problems. For example, if you have not yet used an event driven execution model or state machines in your career, and you move into the embedded systems space, you will very likely need to have those skills.

* The Domain Abstractions layer has knowledge that changes when you change the company you work for. It will change at the rate that the company's domain is changing, or is becoming better understood. If your company uses lean principles, one of the things you want to do is capture knowledge for reuse. This is the whole point of the Domain Abstractions layer, it is a set of artefacts that capture the company's reusable knowledge. 

* The Application layer has the fastest changing knowledge, the knowledge that changes at the rate that an application gets maintained.

=== No separation of UI

In ALA we don't separate the UI unless there is a reason to do so. The amount of knowledge in the UI that comes from a particular application's requirements is usually quite small and that knowledge is usually quite cohesive and coupled with the business logic of the feature it belongs with. For example, the layout of the UI is a small amount of information, and the bindings of the UI elements to data are a small amount of information. So all that cohesive knowledge is kept together, encapsulated inside a feature. Instead, the UI is composed from Domain UI abstractions. Being domain specific, these abstractions have a little more knowledge to them than generic widgets. For example, their domain knowledge may include style, functionality and suitability to their domain context. For example, a softkey or menu item will have an appearance, functionality and suitability to the way UIs are designed in the domain. Using one in a specific application only requires a label and a binding to an action. They will also provide consistency in the domain.

If there is an actual requirement to have different UIs, say a command based UI and a GUI based UI, then you just abstract the UI abstractions further until they can be used either way. The UI abstractions still remain an integral part of the application.

In the example project for this pass, we will for the first time use multiple programming paradigms, a usual thing in real ALA projects.


=== Complexity 

Earlier in this section there was a reference to some writing by Dijkstra on complexity <<Dijkstra1,here>>, where he said complexity need not increase any more than proportional to program length.

But in an early section of this article, we said that ALA achieves constant complexity <<ComplexityGraph1,here>>. 

anchor:ComplexityGraph2[]

[chart,line,file="complexity_curve.png", opt="title=Complexity,x-label=KLOC,legend=right"]
--
//Big ball of mud
1,	10
2,	20
5,	50
10,	100
20,	200
50,	500

//Loosely coupled
1,	10
2,	14
5,	22
10,	32
20,	45
50,	71
100,100
200,141
500,224
1000,316

//ALA
1,	10
2,	11
5,	12
10,	13
20,	13
50,	15
100,16
200,17
500,19
1000,20

//Code writer's brain limit
1,	100
2,	100
5,	100
10,	100
20,	100
50,	100
100,100
200,100
500,100
1000,100

//Code reader's brain limit
1,	50
2,	50
5,	50
10,	50
20,	50
50,	50
100,50
200,50
500,50
1000,50
--

Now that we have a better understanding of the ALA structure, we can explain how it manages complexity to be constant. 

These graphs are qualitative in nature based on both experience and the following logical explanation of why ALA should have this consequence.

The design-time view is a static logical view containing abstractions. The only relationship between abstractions is an instance of, which I have called a knowledge dependency on an abstraction in a lower layer. Even knowledge dependencies on say fifty different domain abstractions (a typical number) are not difficult to familiarize yourself with. These knowledge dependencies become just like any other line of code - part of the domain language as it were. So, in effect, the entire system consists of completely standalone abstractions, as if each one was a small stand-alone program. If every abstraction contains say 200 lines of code, then the most complex the software gets is that of 200 lines of code. Complexity is a constant. 

Even if one abstraction is complex, say it conceals a piece of legacy code using a facade pattern, that doesn't affect the complexity of any of the others.


=== ALA is a logical view

If the system is deployed on multiple machines (this is the subject of the physical view), the ALA abstractions, layers and diagrams all remain identical. A simple application diagram connecting a temperature sensor to a display field does not change if the sensor happens to be on a Mars Rover and the display field is at JPL. 

Ideally, the performance view also does not affect the ALA logical view. This is a many faceted problem that we will return to later.

ALA usually works very well with aspects of the development view as discussed elsewhere. For example, the fact that domain abstractions have zero coupling greatly helps the allocation of teams. The teams need only cooperate on a common understanding of the programming paradigms used.   

=== Abstractions own their outputs

In traditional architecture, functions, modules or classes obviously have inputs and outputs.

In ALA, outputs to abstractions in a lower layer should be done in the same way as they have always been done.  Input from a lower layer will require a dependency inversion such as the observer pattern.

Inputs and outputs to peer abstractions need to be done differently.

Inputs that are static functions of a module are fine, but should be clearly grouped with any other inputs that would be expected to come from the same peer. 

Inputs and outputs of a class should not be in the class's main interface.  The class's interface should only be used by the abstraction in a higher layer to instantiate and configure the abstraction. This is the interface segregation principle. A class should have a new interface for each I/O port that can be wired to a peer. These interfaces are abstractions from a lower layer and should represent a programming paradigm.

Programming languages encourage outputs to peers to be implemented as direct function calls, or method calls on an interface owned by another abstraction. If you are using an asynchronous event driven design, it is common for an output to be written like this:

 Send(receiversEventID, receiverID, priority);
 
In ALA, sending an event would be written similar to this:

 Send(senderEventID, senderID, senderPortID)
 
The abstraction shouldn't know the (shared) event, the destination or the priority. An abstraction in a higher layer has the application specific knowledge about the wiring and will know the destinations, the priority, and the receiver's event.

In general, classes, modules, components, functions should own their outputs. Say what this output is in term of its own abstraction. 'This has happened'. 'Here is my result'.

Outputs that will use synchronous function calls or methods calls must similarly be wirable abstract outputs that don't know anything about where they go. Function calls could go via a callback or a signal & slot library. Whole interfaces of methods go via an abstract interface from a lower layer, injected in by a higher layer abstraction.

Note that inputs and outputs are not necessarily different ports. We may want to wire both input and outputs between two abstractions with a single wiring operation. This is the Interface Segregation Principle. The general case is that a single wiring operation wires a pair of interfaces that are logically one interface. One contains methods going in one direction and the other contains methods going in the other.   

=== Diagrams vs text

You may have noticed a lot of use of diagrams in our discussion so far and wondered if diagrams are a fundamental part of ALA. They are not.

According to one school of thought, diagrams and text are equivalent representations and it is a matter of preference which you use in a given situation. In ALA the difference between diagrams and text is seen in this way. Text is suitable for 'wiring' a structure that is linear or a small tree. A diagram is suitable for a network.

By default, text wires together abstractions in a linear chain using spaces. Sometimes periods or -> or other symbols are used instead.  Text can also handle small tree structures using brackets, usually () or {}. The brackets work for the compiler, but the brain doesn't see them so well, so we typically also use indenting. For our purposes here we will just talk about the indenting method, like Python. When the tree gets too large, there will be deep indenting and a large document. Typically, when this happens, we break the tree into subtrees and then use symbolic references or identifiers to connect the subtrees into the main tree. When this happens, we start to have to follow those indirections to see the full structure. If the identifiers don't represent good abstractions, this will be a problem. 

If the structure is a network rather than a tree, representing it as text starts to require even more symbolic references. Again, if those symbolic references are not abstractions, the reader will have to follow the indirections to understand the text. But with wiring, these references are not abstractions, they are just composition (composing things together in an unstructured way). We would prefer not to even give them a name.   

This is where diagrams do what text cannot. Two arbitrary points can be connected by a line in a visual way, so avoid a symbolic reference causing an indirection. You can 'follow the wiring' without any effort at all. And the connection can be anonymous, just as spaces or dots are in text.

Diagrams and text do not do the same job. Ask any electronics engineer to do his work without a schematic, and work with a list of nodes in text form instead. They could no longer reason about the design. Just try to reason about a state machine by looking at a text list of transitions. Following the indirections in text form gets totally in the way of understanding the machine. 
In ALA, when you start representing your requirements by composing domain abstractions, they usually, but not always, have a network structure. That's what causes diagrams to come into ALA.


=== Features

You may have noticed throughout this article the word 'features' being used quite often instead of 'Application'. When the application is large, we can think of it as a composition of feature abstractions. This is exactly what happens in natural language in the domain when describing requirements. 'Features' is just the word we give the natural abstractions in the requirements, without even realizing it. Just go with this in the software itself.    

=== Composability and Compositionality

We have referred to the property "composability" a few times in the article. By composability, we refer to the ability to compose an infinite variety of applications by combining a finite number of domain abstractions in different ways.

This is an important property in ALA. Once the meanings of a finite number of abstractions together with their rules of composition are known, the reader is able to understand an infinite number of constructions at first reading.

Composability uses the Principle of Compositionality which states: In mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. 

In software engineering, it is described by a pattern called "Abstract Interactions" or "Configurable Modularity" by Raoul de Campo and Nate Edwards - the ability to reuse independent components by changing their interconnections, but not their internals. It is said that this characterises all successful reuse systems, and indeed all systems which can be described as "engineered". 

ALA has these properties using abstractions and paradigm interfaces.

As mentioned earlier, there are other software systems that have composability, usually using data-flow paradigm, such as RX (Reactive Extensions). Most composability systems are restricted to a single paradigm. For ALA to have the correct level of expressiveness when composing domain abstractions to represent the requirements, a variety of connection paradigms are needed.

We can make an analogy with Lego bricks. Some Lego parts have the familiar little stud and tube connectors. Some will support axles and holes, either tight or loose. These different ways of connecting Lego parts are analogous to different programming paradigms. 

If the domain were for building small toy machines, the non-ALA method would start with the imagined machine and decompose it into parts specific to that one machine. But it's better to invent mechanical paradigms, then create a finite number of parts that can be connected in an infinite number of ways, not only for the initial machine, but all its variants in the future, and furthermore other machines in the domain.

=== Horizontal domain partitions

Say you are implementing a particularly large domain abstraction such as a 'Table', or are implementing a complicated programming paradigm. We would like to break these up into smaller components. Do we introduce a fractal type of structure to deal with this? Should we have hierarchical layers within layers contained completely inside the Table abstraction?

The astute reader will have noticed the non-ALA thinking in the statement "break these up into smaller components". In ALA we don't decompose a large abstraction into components, we compose it from abstractions, which if necessary we invent as we go. These new abstractions will have a scope or level of ubiquity, stability and reuse that corresponds to one of the existing layers. So there should be no hierarchical or fractal structures in ALA.

However, the domain that these new abstractions are in won't be the same domain as the one that provides for the writing of Application requirements. For example, the implementation of the Table abstraction will need to be connected to another abstraction in the domain of databases. One of the abstractions in that domain will know about a particular database, say SQL Lite. A polymorphic interface should exist between the two. That interface, being more abstract than either the Table or the SQL Lite abstractions, will be in the next layer down, where both the Table and the MySQL abstractions can have a knowledge dependency on it. Of course the SQL abstraction will actually be further composed of an adapter and a real database. 

Some application domain abstractions are complicated. Examples of these are abstractions requiring a connection to an actual database, actual hardware, the Internet, etc. Implementing these will typically wire out horizontally into other technical domains. You can visualise them going in multiple directions, which is exactly the idea of Alistair Cockburn's hexagonal architecture.

[plantuml,file="diagram-22.png"]
----
@startdot
digraph foo {
edge [color=green]
size="2!"
fontsize=8
fontname=Ariel
labeljust=l
subgraph cluster_1
{
label="Features layer"
Feature
}
subgraph cluster_2
{
label="Domain Abstractions layer"
Input
Table
}
subgraph cluster_3
{
label="Programming Paradigms layer"
I
IDataModel
HAL
}
subgraph cluster_4
{
label="Database configuration"
Config
}
subgraph cluster_5
{
label="Database domain"
SQLLite
}
subgraph cluster_7
{
label="Hardware domain"
ADCdriver
}
Feature -> Input
Feature -> Table
Input -> I
Table -> I
Input -> HAL
Table -> IDataModel
Config -> SQLLite
ADCdriver -> HAL
SQLLite -> IDataModel
}
@enddot
----



A communications domain using a OSI model may end up with a whole chain of communications domain abstractions going sideways:

[plantuml,file="diagram-23.png"]
----
@startdot
digraph foo {
edge [color=green]
fontsize=10
fontname=Ariel
labeljust=l
subgraph cluster_1
{
label="Application layer"
Application
}
subgraph cluster_2
{
label="Domain Abstractions layer"
A
B
}
subgraph cluster_3
{
label="Programming Paradigms layer"
I
XML
REST
TCP
IP
ICMP
Ethernet
}
subgraph cluster_4
{
label="Network configuration"
ConfigComms
}
subgraph cluster_5
{
label="Network domain"
Presentation
Session
Transport
Network
Datalink
Physical
}
Application -> A
Application -> B
A -> I
B -> I
ConfigComms -> Presentation
ConfigComms -> Session
ConfigComms -> Transport
ConfigComms -> Network
ConfigComms -> Datalink
B -> XML
Presentation -> XML
Presentation -> REST
Session -> REST
Session -> TCP
Transport -> TCP
Transport -> IP
Network -> IP
Network -> ICMP
Datalink -> ICMP
Datalink -> Ethernet
Physical -> Ethernet

}
@enddot
----

The technicalities may be incorrect but the diagram gives the idea of how the OSI 'layers', which are just run-time dependencies, would fit into the ALA layers. 

=== No hierarchical design

ALA does not use any form of hierarchical structure. Instead it uses abstraction layers, together with "Horizontal domain partitions" discussed earlier.


=== Product owner perspective

TBD



=== Reuse

TBD


=== Documentation

TBD


=== Symbolic indirection

TIP: Avoid use of symbolic indirection without abstraction

When we start assembling requirements from abstractions, a topic that we will cover in coming sections, we will be using symbolic indirection, such as function calls or the new keyword with a class name. Unless a symbolic indirection is to an abstraction, they are for the compiler to follow at compile-time, not for the code reader to follow at design-time. Understanding the code relies on allowing the reader to read a small cohesive block of code. The reader should never have to follow the indirection somewhere else. If you don't achieve this, and abstraction is the only way you can, then any decoupled architecture will be _more_ difficult to read. 

Abstraction allows indirection while allowing the reader to continue reading on to the next line. The importance of this property cannot be overstated. As soon as we start thinking in mere programming language terms of modules, components, interfaces, classes, or functions, the abstraction will start to be lost. These other artefacts may have benefits at compile-time (the compiler can understand them), but that is not useful at design-time unless they are also good abstractions.  

It would be nice if your compiler could tell you that you have a missing abstraction, just as it does for a missing semicolon, but alas, they are not capable of understanding abstractions yet. So it is still entirely up to you.

Abstraction is almost a black and white type of property. It's either there or it isn't. If the reader of your code does not have to follow the indirection, you have it. 

Footnote: When the reader of your code meets your abstraction for the first time (usually a domain abstraction in a domain they have recently come into), ideally their IDE will give them the meaning in a little pop-up paragraph as their mouse hovers over any of its uses. Depending on the quality of the abstraction, after a single exposure, their brain will have the insight, like a light coming on, illuminating a meaning. The brain will form a new neuron to represent the concept. Since the reader will hopefully remain in the domain for some time, this overhead to readability shouldn't be large.

=== Quick tip.

Whenever I have only two minutes to give advice on software architecture, I use this quick tip. The tip is ALA reduced to its most basic driving principle.

Ask your modules, classes and functions:
[TIP]
====

[green]#*What do you _know_ about?*#
====

The answer should always be "I just know about...".

The anthropomorphization helps the brain to see them as autonomous entities. The word 'knows' is carefully chosen to cause a 'design-time' perspective. 

. It's a restatement of the SRP (Single Responsibility Principle). Every element should know about something coherent. Furthermore, no other elements should know that.

. An element may know about a single hardware device.

. An element may know about a user story.

. An element may know about a protocol.

. An element may know an algorithm.

. An element may know how to do an operation on some data, or the meaning of some data, but not both.

. An element may know a composition of other elements.

. An element may know where data flows between other elements.

. No element should know the source or destination of its inputs and outputs.



=== Example project - a real device

Unlike our previous example projects, this project is a real device and had previously been implemented without any knowledge of ALA. So this example serves to make comparisons between ALA and conventional software development. The original software was around 200 KLOC and took 3 people 4 years to write. 

The actual device is used by farmers all over the world. It can weigh livestock and keeps a database about them for use in the field. It connects to many other devices and has a large number of features: 

image::Tru%20Test%20XR5000%20Weigh%20Scale%20Indicator.jpg[XR5000 image, title="Livestock weighing indicator", width=75%]

The architecture in the original software, was somewhat typically organised into modules and patterns by its developers. Also somewhat typically, it had ended up with a high cost of modifiability - a big ball of mud. After the first release, the first incremental requirement was a 'Treatments' feature, which involved several new tables, new columns in existing tables, new data pages, new settings pages and some new business logic. This feature took a further 3 months to complete (actually 6 calendar months), which seemed out of proportion for the size of the feature. Somehow the Product Owner and managers seemed to have a sort of intuition that if similar things had been done before, such as menus or database tables, those things were already done, and the only new work was in the specific details of the new requirements. Those requirements could be communicated in a relatively short time, say of the order of one hour or one day if you include follow up discussions of abnormal scenarios.  So 6 months did not go down well. ALA, of course, works in exactly this intuitive way that managers hope for. All the things already done are sitting there in the domain abstractions, waiting to be reused, configured and composed into new requirements.

==== Iteration zero

During the development, there had a been a high number of changes required to the UI. It occurred to me at the time that the underlying elements of the UI were not changing. It was mainly the details of layout and navigation around the device's many pages that were changing. The same could be said about the data and business logic. Only details were changing. 

I took to representing the new designs using box and line drawings representing both the UI layouts and the navigation flows. I realized these diagrams were potentially executable, and wondered how far I could go representing the data and business logic in the same way. I decided to try to represent all of the functionality of the indicator in just one  diagram.

It took two weeks to complete the diagram. I used Xmind because it laid itself out. I found that any drawing package that needed you to stop and do housekeeping such as rearranging the layout got in the way so much that you would lose your flow. Xmind allowed me to just enter in the nodes and it would automatically wire them in as either peers or chains and lay them out. The one disadvantage was that Xmind only does trees, so any cross tree relations had to be done manually, but this was also very quick in Xmind once you were used to it. I just let the cross wiring form arcs across parts of the tree.

Progress was extremely rapid once you had the abstractions and paradigms you needed. And many of them were obvious: softkeys, pages, grids, menus, actions, navigate-action, tables. etc. The programming paradigms would pop into play as needed. After the obvious UI-layout and navigation-flow ones came data-flow and data-flow of table types, events, and schema. The user of this device could set up custom fields, so the schema itself partially came from another  table. At times I would get stuck not knowing how to proceed. The longest of these blocks was half a day. But every time the required abstractions or programming paradigms would firm up, and in the end anything seemed possible.

The diagram itself took shape on the right hand side of the Xmind tree. On the left side I had the invented domain abstractions and paradigm interfaces, with notes to explain them. The right side was mostly just a set of relatively independent features, but there was the odd coupling between them such as UI-navigation lines that were also present in the requirements.

The diagram contained around 2000 nodes (instances of the abstractions), which is about 1% of the size of the total original code. There were about 50 abstractions, and several paradigm interfaces.

Part of the diagram is shown below (laid out more nicely in Visio)

image::All%20Animals%20Screen%20V3.png[Application diagram for the All Animals View feature, title="Application diagram for the All Animals View feature"]

As I did the diagram, I deliberately left out anything to do with the aforementioned Treatments feature, so that I could see how easy it might have been to implement once the domain abstractions for the rest of the requirements had matured. So after the diagram was completed, I added the Treatments feature. This involved adding tables, columns to existing tables, a settings screen, a data screen, and some behaviours.  No further abstractions needed to be invented. The incremental time for the diagram additions was of the order of one hour. Obviously testing would be needed on top of that, and the 'Table' abstraction would need additional work so it could migrate itself, a function it had not needed up until this point. Although somewhat theoretical, the evidence was that we could get at least an order of magnitude improvement in incremental maintenance effort.

At first the diagram seemed too good to be true. It had an elegance all of its own. It apparently captured all of the requirements, without any implementation at all, and yet seemed potentially executable. And if it worked, application modifications of all the kinds we had been doing were going to be almost trivial.

The burning question on my mind was, is it simply a matter now of writing a class for each of these abstractions and the whole job is done?

==== Translating the diagram to code

We hired a C++ student and proceeded with a 3-month experiment to answer this question.

It was a simple matter to translate the diagram into C++ code that instantiated the abstractions (classes), wired them together using dependency injection setters, configured the instances using some more setters, and used the fluent interface pattern to make all this straightforward and elegant. Part of the code for the diagram sample above is shown below to give you a feel for what it looked like.

....
m_animalListScreen
	->wiredTo((new Softkeys())
		->wiredTo((new Softkey())
			->setTitle("History")
			->wiredTo(new Navigate(m_animalHistoryScreen))
		)
		->wiredTo((skeyOptions = new Softkey())
			->setTitle("Options")
			->wiredTo(new Menu()
				->wiredTo(new Navigate("Session...", m_sessionSummaryScreen))
				->wiredTo(new Navigate("Settings...", m_settingScreen1))
			)
		)
	)
	->wiredTo((searchField = new TextDisplayField())
		->setLabel("Search")
		->setField(VIDField = new Field(COLUMN_VID))
	)
	->wiredTo(new Grid()
		->wiredTo(columnOrder = new ColumnOrder())
		->setRowMenus((new EventHandler())
			->setEvent(EVT_KEY_ENTER)
			->wiredTo(new Menu()
				->wiredTo(new Navigate("View information for this animal", m_animalSummaryScreen))
				->wiredTo((new Action("Delete Record", AnimalsTable::DeleteRow))->wiredTo(AnimalsTable))
			)
		)
	);
....

==== Writing the classes

We knew we wouldn't have time to write all 50 classes, so we chose to implement the single feature shown below as a screen shot. 

image::XR5000ScreenShot.jpg[All Animals screen shot, title="All Animals view in the weighing indicator", width=75%]

The student's job was to write 12 abstractions out of the 50. These 12 were the ones used by that feature. The initial brief was to make the new code work alongside the old code (as would be needed for an incremental legacy rewrite), but the old code was consuming too much time to integrate with, so this part was abandoned. 

The learning curve for the student was done as daily code inspections, explaining to him where it violated the ALA constraints, and asking him to rework that code for the next day. It was his job to invent the methods he needed in the paradigm interfaces to make the system work, but at the same time keep them abstract by not writing methods or protocols for particular class pairs to communicate. It took about one month for him to fully 'get' ALA and no longer need the inspections.  

// image:All%20Animals%20Screen%20V3.svg[]

The student completed the 12 classes and got the feature working in the device. The feature included showing data from one database table in a grid, sorting, searching, softkeys, and a menu.

Interestingly, as the student completed certain abstractions that allowed parts of other features to be done, he would quickly go and write the wiring code and have the other features working as well. For example, after the softkeys, actions, navigate, and page abstractions were done, he went through and added all the softkey navigations in the entire product as this only took minutes to do. 

We wanted more funding to retain the student until we had enough to do the treatments feature, and indeed all 50 abstractions with the hope of making this implementation the production code and improving our ongoing maintenance effort. But that was not to be, despite the promising result.

We have about a quarter of a data point. Some of the abstractions done were among the most difficult, for example the Table abstraction, which had to use SQL and a real database to actually work. So it is not unreasonable to use extrapolation to estimate that the total time to do all 50 abstractions would be about one person-year. That compares with the original 12 person-years. 

It seems that classes that are abstractions are faster to write. This seems intuitive because you don't have any coupling to worry about. More importantly, the two phase design-then-code methodology of ALA allows the developer not to have to deal with large scale structure at the same time as writing code. This frees the developer to go ahead and write the code for the local problem.

I believe it is beneficial for each developer to be trained to be both an architect and a developer, but just don't ask them to do both at the same time.

This practical result combined with the theory outlined earlier in this article suggests there ought to be a large improvement in incremental maintenance effort over a big-ball-of-mud architecture.






== Pass six - ALA compared with...


=== Physical boundaries

I was listening to a talk by Eric Evans where he said that Microservices works because it provides boundaries that are harder to cross. We have been trying to build logical boundaries for 60 years, he said, and failed. So now we use tools like Docker that force us to use say REST style interfaces in oder to have physical boundaries. I have also heard it suggested that using multiple MCUs in an embedded system is a good thing because it provides physical boundaries for our software components. And I think, really? Is that the only way we can be create a logical boundary? I can tell you that multiple MCUs for this reason is not a good idea if only because all those MCUs will need updating, and the mechanisms and infrastructure needed to do that make it not worth it. Unless there is a good reason, such as to make different parts of your code independently deployable, the extra infrastructure required for physical boundaries that are just logical boundaries is not necessary. Furthermore, physical boundaries, like modules do not necessarily make good abstractions. The only boundary that works at design-time is a good abstraction. So ALA achives it's design-time boundaries by using abstractions.

=== TDD

It is said that TDD's main advantage is not so much the testing, but the improvement in the design. In other words, making modules independently testable makes better abstractions. This is probably true, but in my experience, TDD doesn't create good abstractions nearly as well as pursuing that goal directly. The architecture resulting from TDD is better but still not great.

=== Observer pattern and dependency inversion

TBD



anchor:ComparisonHexagonal[]

=== Hexagonal Architecture (ports and adapters)

In the previous section we intimated that the sideways chains of interfaces going out in horizontal directions were the same as hexagonal architecture. While ALA shares this aspect of hexagonal architecture, there is still an important difference.

ALA retains domain abstractions of the UI, Database, communication and so on. For instance, in our XR5000 example, we had a domain abstraction for a persistent Table. We had domain abstractions for UI elements such as Page, Softkey etc. We don't just have a port to the persistence adapter, we have an abstraction of persistence. We don't just have a port for the UI to bind to, we have abstractions of the UI elements. The implementation of these abstractions will then use ports to connect to these external system components. Why is it important that we have domain abstractions of these external components?

. The Database and the UI will have a lot of application specific knowledge given them as configuration. Remember the creativity cycle. After instantiation of an abstraction comes configuration. The database will need a schema, and the knowledge for that schema is in the application. The Softkey UI elements will need labels, and that knowledge is in the application. By making domain abstractions for persistence and UI, the application can configure them like any other domain abstraction as it instantiates and wires up the application. To the application, these particular domain abstractions look like wrappers of the actual database and UI implementations, but they are more like proxies in that they just pass on the work. 
+
The Persistence abstraction then passes this configuration information, via the port interface to the actual database. The Softkey abstraction then passes its label, via the port interface, to the softkeys. Otherwise the Application would have to know about actual databases and actual softkeys.
+
If you need a design where the UI can change, you just make the UI domain abstractions more abstract. A softkey may be a command abstraction. It is still configured with a label. But it may be connected to a softkey, a menu item, a CLI command, a web page button, or a Web API command.

. From the point of view of a DSL, it makes sense to have concepts of UI and persistence and communications in the DSL language. The application is cohesive knowledge of requirements. The UI and the need for persistence are part of the requirements. In fact, for product owners communicating requirements, the UI tends to be their view of requirements. They talk about them in terms of the UI. Many of the product owners I have worked with actually design the UI as part of the requirements (with the backing of their managers, who are easily convinced that software engineers can't design UIs. PO can't either, but that is another story.). The point here is that the UI layout, navigation, and connection to business logic is all highly cohesive. We explicitly do not want to separate that knowledge. 
+
As a restatement of an earlier tenet of ALA, it is much better to compose the application with abstractions of Business logic, UI and persistence than to decompose the application into UI, persistence and business logic.

. We want the application to have the property of composability. We have previously discussed how that means using programming paradigm interfaces for wiring up domain abstractions. By using domain abstractions to represent external components, the abstractions can implement the paradigm interfaces and then be composable with other domain abstractions. For example, the Table domain abstraction which represents persistence may need to be connected directly to a grid, or to other domain abstractions that map or reduce it. Indeed, the Table abstraction itself can be instantiated multiple times for different tables and be composed to form a schema using a schema programming paradigm interface. I have even had a table instance's configuration interface wired to a another Table instance. (So its columns can be configured by the user of the application.)     

. The fourth reason why it is important for the application to not directly have ports for external components of the system is that we don't want the logical view of the architecture to become just one part of the physical view. If there is a communications port that goes to a different physical machine where there is more application logic, the application's logical view should not know about that. It may be presented as an annotation on the application (lines) connecting certain instances, but it shouldn't split the application up. At the application level, the collaboration between parts instantiated on different machines is still cohesive knowledge and belongs inside one place - the application.  


=== Layer patterns

==== MVC

TBD

==== Application, Services, Drivers, Hardware

TBD

=== Factory method pattern

Let's say you have a nice Table domain abstraction that is perfect for the requirements you have in your domain. At run-time, the Table abstraction must be wired via a polymorphic interface to a particular database, and the database must be instantiated. We don't want the Application, the part that composes domain abstractions, to know anything about all this. We want the Application to be able to just use a Table as if it is a self-contained abstraction.

Similarly, in the partitioned off 'Database' domain, there will usually be some knowledge in the top layer to configure a particular database. This knowledge knows nothing of the Application and is not responsible for creating the Tables for it.   

Table knows about the polymorphic interface, but doesn't know it's for a connection to a real database. This interface could have a method that the Table instance calls when it is instantiated. Table only knows that it has to call this method. However, there is not yet any provider of this interface in place, so we cannot get the database connected up this way. But we do want to wait until the first Table abstraction is instantiated, otherwise we would not need to spin up a database at all.

Instead, Table knows to instantiate a Factory design pattern object. To the Table, this action it must do is logically just part of the same polymorphic interface. It doesn't even know it is a factory. If the interface is called 'IOutsideConnection', the factory could be called something like OutsideConnection and the method inside it called getAnOutsideConnection. It could be a static class or a singleton. Table just saves the object that it gets in the interface reference. 

 IOutsideConnection outside = (new OutsideConnection()).getAnOutsideConnection();

If it is the first time the factory object is used, it could, for example, invoke an object in the top layer (via dependency inversion, the object has already registered itself with the factory) that contains the configuration knowledge about the database. The object in the top layer instantiates a database domain abstraction to give to the Table.


=== Interface segregation principle

TBD

=== Open Closed Principle and decorators

TBD


=== Bridge pattern 

TBD


=== Architecture styles

I am not an expert at these so called 'Architectural styles'. Any feedback about the accuracy of the following comparisons would be appreciated.

==== Presentation, Business, Services, Persistence, Database

TBD

==== Presentation, Application, Domain, Infrastructure

The middle two layers appear to be the same as ALA's. The Presentation (UI) only has run-time dependencies on the Application, and the Domain layer only has run-time dependencies on the Infrastructure (Persistence etc), so these layers are not present in ALA. 

Instead Presentation is done in the same way as the rest of the application, by composing and configuring abstractions in the domain. The meaning of composition for UI elements (typically layout and navigation-flow) is different from the meaning of composition in the use-cases (typically work-flow or data-flow).

In ALA, the foundation layer is also done in the same way as the rest of the application, at least a little. Domain abstractions that represent say a persistent table are in the Domain layer. The composition and configuration of them again goes in the Application layer. This time the meaning of composition is, for example, columns for the tables and schema relations.  

If the implementation of any domain abstraction is not small (as is the case with the persistent Table abstraction mentioned above, which will need to be connected to a real database), it will be using other abstract interfaces (in the Programming Paradigms layer) connected to its runtime support abstractions in a technical domain, the same as in Hexagonal Architecture.

==== Object Oriented Programming

From my reading, it seems that the most characteristic feature of OOP is that when data and operations are cohesive, they are brought together in an object. Others may see it as enabling reuse, inheritance, and still others may see it as polymorphism. New graduates seem to be introduced to polymorphism in inheritance and not be introduced to interfaces at all, which is a shame because the concept of interfaces is much more important. 

I have never been an expert at Object Oriented Design as I found the choice of classes difficult and the resulting designs only mediocre. But I think the most fundamental and important characterising feature of OOP is under-rated. That is the separation of the concepts of classes and objects. This separation is not so clearly marked when we use the terms modules or components. The separation is fundamentally important because it's what allows us to remove all dependencies except knowledge dependencies. In the way described earlier in this article, you can represent the knowledge of most dependencies as a relationship between instances completely inside another abstraction. What OOP should have done is represent relationships between objects completely inside another class. The problem is that OOP doesn't take advantage of this opportunity. Instead, it puts these relationships between objects inside those objects' classes, as associations or inheritance, thereby turning them into design-time dependencies, and destroying the abstract qualities of the classes. Abstractions, unlike classes, retain their zero coupling with one another.

ALA addresses the problem by calling classes abstractions and objects instances. Abstractions differ from classes by giving us a way to have logical zero coupling, as if they were on different physical platforms. Instances differ from objects by having ports because their classes give them no fixed relationships with other objects.

Of course, when you are writing ALA code, abstractions are implemented using classes, but you are not allowed associations or inheritance. Instances are implemented as objects but with ports for their connections. A port is a pair of interfaces that allow methods in both directions. The interfaces are defined in a lower layer.
 
In ALA, the UML class diagram completely loses relevance. Because classes have no relationships with each other, bar knowledge dependencies, a UML diagram in ALA would just be a lot of boxes in free space, like a pallet of things you can use. You could show them in their layers and you could even draw the downward composition relationships that represent the knowledge dependencies, but there would be no point to this except in explaining the concepts of ALA. When you are designing an actual system, the real diagram is the one inside of an abstraction, especially the uppermost one, the application. It shows boxes for instances of the abstractions it uses, with the name of the abstraction in the box, the configuration information for those instances, and of course the lines showing how they are wired together. The names inside the boxes would not even need to be underlined as in UML, because the boxes in such diagrams would always be instances. 

Such a diagram is close to a UML object diagram. However, a UML object diagram is meant to be a snapshot of a dynamic system at one point in time. In ALA, any dynamic behaviour is captured in a static way by inventing a new abstraction to describe that dynamic behaviour. Thus the design-time view is always static. So the object diagram is static. The application class specifies a number of objects that must be instantiated, configured, and wired together to execute at run-time. Since the structure is always static, ideally this would be done by the compiler for best efficiency, but there is no such language yet. So, in the meantime, it is done at initialization time. The object diagram can be fairly elegantly turned into code using the fluent coding style shown in the XR5000 example.

=== DSLs

We compared ALA and DSLs in the quick overview <<DSL1, here>> 

ALA includes the main idea of DSLs in that the fundamental method "represent[s] requirements by composition of domain abstractions". It shares the DSL property that you can implement a lot more requirements or user stories in a lot less code. 

But ALA only tries to be a light-weight way of telling ordinary developers how to organise code written in your underlying language. Although the domain abstractions do form a language and the paradigm interfaces give it a grammar, ALA doesn't pursue the idea of a language to the point of textural syntactic elegance. Instead, you end up with explicit wiring methods to combine domain entities, or plain old functional composition, or some other form of composition in the wider sense of the word. Often, the text form is only a result of hand translation of an executable diagram. ALA certainly doesn't overlap with DSLs to the extent of an external DSL, nor does it try to sandbox you from the underlying language. It therefore does not require any parsing and doesn't need a language workbench, things that may scare away 'plain old C' developers.

Like DSLs, ALA can be highly declarative depending on the paradigm interfaces being used to connect domain abstractions. It is better to have the properties of composition and composability in the your domain language even if they may not be in a perfectly elegant syntactic form. ALA may end up composing abstractions with calls to wireTo methods instead of spaces or dots. But often a diagram using lines is even better than spaces and dots.  

In DSLs, it is important that different languages can be combined for different aspects of a problem. For example, a DSL that defines State machines (the state diagram) and a DSL for data organisation (Entity Relationship Diagram) may be needed in the same application. You don't want to be stuck in one paradigm. ALA recognises this importance by having paradigm interfaces that are more abstract than the domain abstractions. 

DSLs probably work by generating a lot of code from templates whereas ALA works by reusing code as instances of abstractions. Both of these methods are fine from the point of view of keeping application specific knowledge in its place, and domain knowledge in its place. Howver, the distinction between ALAs domain layer and programming paradigms layer is probably not so as clearly made in the implementation of the templates.   

It is an advantage of DSLs that they can sandbox when needed. An example from the wiring pattern earlier is that the ports of instances do not need to be wired. Therefore, all abstractions need to check if there is something wired to a port before making a call on it. Enforcing this is a problem I have not yet addressed.

A possible solution, albeit inferior to a real DSL that would tell you at design-time, might be that when there are tools that generate wiring code from diagrams, they automatically put stubs on all unwired ports. These stubs either throw an exception at run-time, or just behave inertly. 

ALA is different from external DSLs. ALA is just about helping programmers organise their code in a better way. It doesn't try to make a syntactically elegant language, as a DSL does. Certainly an external DSL will end up representing requirements in a more elegant syntax. But that is not the most important thing in ALA. The most important thing is the separation of code that has knowledge of the requirements, which will cause the invention of abstractions that have zero coupling (because the coupling was really in each requirement - that is why a requirement is cohesive). ALA also avoids taking the average imperative language programmer out of their comfort zone. It does not require a language workbench and does not sandbox you from the underlying language.

ALA probably does fit into the broadest definition of an internal DSL. However, again, it does not target syntactic convenience in the expression of requirements so much as just separating the code that knows about those requirements from the code that implements them. An internal DSL usually aims to have a mini-language that is a subset of the host language, or it tries to extend the host language through clever meta-programming to look as if it has new features. ALA is about abstraction layering. It is about this design-time view of knowledge dependencies: what abstractions in lower layers are needed to understand a given piece of code.


=== Dependency injection

==== Similarities

In ALA you inject run-time required objects via setters.

==== Differences

ALA uses explicit wiring, never automatic wiring. For one thing, the wiring is required to compose from a pallet of domain abstractions. But secondly, and more importantly, you do not want the knowledge that the wiring represents to disappear into the abstractions themselves, not even as meta-data. That would destroy the abstractions.

In ALA, the explicit wiring can't be XML or JSON, even if it can be modified at run-time. Usually, because a network structure will be required, the explicit wiring must be a diagram. However, it can be a projection editor, so that the structure is entered in text form (preferably not XML or JSON) and live viewed in graphical form.  

In ALA, abstraction pairs don't have their own interfaces for their instances to communicate. So we don't have the situation where class A has a dependency on class B, and so an object of class B (or one of its subclasses) is injected into class A. Similarly, we wouldn't have the situation where class A requires an interface that is implemented by class B.

In ALA the dependencies can only be on paradigm interfaces, which are a whole abstraction layer more abstract. So we need to be thinking that if class A accepts or implements a certain paradigm interface, there could be any number of other abstraction instances that could be attached. Furthermore, we could build arbitrarily large assemblies - composability. Or we don't have to connect an instance at all. So it doesn't really make sense to call what we are injecting 'dependencies'. We just think of it as wiring things up, like electronic components.


=== Component Based Software Engineering

ALA uses many of the same methods found in component based engineering or the Components and Connector architectural style.


===== Similarities

* Components are Abstractions.

* Reusable software artefacts.

* Connection ports for I/O.

* Composability

* Both instantiate components, specialize them by configuration, and compose them together to make a specific system.

* ALA's 3rd layer has interfaces used to wire abstractions in the 2nd layer, so at a lower level (more abstract) level. They represent something more like programming paradigms. The equivalent pattern in components engineering is "Abstract Interactions".  

* The architecture itself is composed of a generic part and a specific part. The general part is the ALA reference architecture itself and the components or the connectors architectural style. The specific part is the wiring diagram of the full system.

===== Differences

* Component based engineering technologies such as CORBA primarily solve for platform and language interoperability in distributed system whereas ALA brings some of the resulting concepts and properties to everyday small-scale, non distributed development as well, where the only separation is logical.

* In ALA there is perhaps more particular emphasis on making components clearly more abstract than the systems they are used in, and making the interfaces clearly more abstract than the components. The components are pushed down a layer and the interfaces down to a layer below that. Then all dependencies must be strictly downwards in these layers. In component based engineering, this structure is not necessarily enforced. If the components are just a decomposition of the system, then the system, components and interfaces may all be at the same level of abstraction, making the system as a whole complex.

* ALA depends on the 'abstractness property' of components to get logical separation, and so calls them 'Abstractions' and not components to help them retain that property. Even if there will only be one use and one instance, it is still called an abstraction. This keeps them zero coupled and not collaborating with other abstractions they will be wired to.

* ALA layers are knowledge dependency layers.  Components may still be arranged in layers according to run-time dependencies, such as communication stacks. In ALA run-time dependencies are always implemented as explicit wiring inside another higher layer component.

* ALA's top layer must be a straight representation of the requirements, whereas components may tend to be decomposed pieces of the system.

* ALA's 2nd layer of components are designed for expressiveness of user stories or requirements, and provide DSL-like properties. ALA puts emphasis on the 2nd layer of components having the scope of a domain as the means of explicitly controlling the expressiveness of the pallet of components.

* ALA is not fractal. In ALA the components of components are abstractions that become more abstract and thus ubiquitous and reusable. ALA therefore uses abstraction layers rather than hierarchies.

* ALA forces decisions about which abstraction layers the software artefacts go into, and then controls knowledge (semantic) dependencies accordingly.

* ALA tries to make the abstraction layers discrete and separated by a good margin. 

* ALA puts greater emphasis on wiring being able to represent any programming paradigm that suits the expression of requirements, and the use of many different paradigms in the same wiring diagram.

* ALA emphasises the cohesion of functional parts of a system such as UI, logic and Data, by bringing them all together in one small diagram using domain level components

* Instead of 'required' interfaces, in ALA they are called 'accepts' interfaces. This is because the abstractions are more abstract and composable, so, as with Lego blocks, there isn't necessarily a connection to another instance.

==== Domain Driven Design

Domain Driven Design's "Bounded Contexts" and ALA's Domain Abstractions layer have the same goal, that of encapsulation of the domain specific knowledge.

Domain driven design appears to concentrate on common languages to allow  pairs of elements to communicate, which ALA explicitly avoids. ALA tries to abstract the languages so that they are more abstract and fundamental than the domain, and more like programming paradigms.

TBD Discuss with a DDD expert the comparison between ALA and DDD.

=== Microservices

TBD


=== Hexagonal Architecture (Ports and Adapters)

ALA includes the basic idea of hexagonal architecture, but with modification using the Bridge Pattern to keep cohesive knowledge belonging to the application from being split. This was explained in an earlier section of this article. <<ComparisonHexagonal>>

=== Architecture evaluation methods

Methods such as ATAM tell us how to evaluate an architecture for quality attributes such as maintainability, for instance by giving it modification scenarios to test how difficult the modifications would be to implement. There are several scenarios based methods to do this such as ATAM. Using this we could, theoretically, iteratively search over the entire architecture design space to find a satisfactory solution. It's a bit analogous to numerically solving for the maxima of a complex algebraic formula. In contrast, ALA is analogous to an 'algebraic solution'. If the desired quality attributes, and all the software engineering topics listed above are the equations, ALA is the algebraic solution. It simplifies them down into a parameterised template architecture, ready for you to go ahead and express your requirements.

=== Reactive Extensions

In ALA, when you wire together 




=== WPF's XAML

TBD

=== Functional programming

TBD

=== Functional programming with monads

TBD

=== Functional Reactive Programming

TBD

=== Multi-tier Architecture

TBD

=== Onion Architecture

TBD

=== Clean Architecture

TBD

=== Example project - A real application

Like the previous example, this was originally a real application written in a conventional way, and consequently ending in a big ball of mud. Because maintenance had become so difficult it was decided to  rewrite it using ALA. This gives us a good basis for comparison between conventionally written software and ALA.

TBD




== Pass seven - Surrounding Topics

=== Recursive abstractions

ALA enforces a strictly layered (non-circular) knowledge dependency structure. It encourages a small number of  abstraction layers at discrete well separated levels of ubiquity as a framework for the knowledge dependencies. This would appear to exclude the possibility of the powerful abstraction composition technique of recursion, where the meaning of an abstraction is defined in terms of itself. (Or an abstraction implementation may need knowledge of another abstraction, which in its turn has an implementation that needs knowledge of the first abstraction. This appears to require circular knowledge dependencies.

Circular knowledge dependencies happen all the time in functional programming where recursion replaces iteration. This is generally when a function needs to call itself or a class needs to use 'new' on itself. For example, a recursive descent compiler will have a function, 'statement', which will implement specific statements such as 'compound statement', 'if statement' and so on, and in those there will be a recursive call to the function, 'statement'. The following Syntax diagram represents part of the implementation of function 'statement'.

[plantuml,file="diagram-24.png",title=Syntax Diagram showing implementation of statement using recursion]
--
@startdot

digraph foo {
node [fontname="Arial"]
// nodesep = 10
graph [rankdir=LR]
node [group=main]
Statement1 OpenBrace 
node [group=""]

OpenBrace [label="{"]
CloseBrace [label="}"]
OpenBracket [label="("]
CloseBracket [label=")"]
Semi [label=";"]
End1 [style=invis]
End2 [style=invis]
Statement1 [label=statement]
Statement2 [label=statement]
Statement3 [label=statement]

Statement1 -> OpenBrace -> Statement2 -> CloseBrace
Statement2:e -> Semi:e
Semi:w -> Statement2:w
CloseBrace -> End1 
{rank=same Statement2 Semi}

edge [len=0.1]
Statement1 -> if -> OpenBracket -> expression -> CloseBracket -> Statement3 -> End2
}
@enddot
--

In ALA, we want to preserve the idea of clear layers defining what knowledge is needed to understand what. Resolving this dilemma could get a bit philosophical. Since abstractions are the first class artefacts of the human brain, it may be best to think about how the brain does it. The brain must actually have two abstractions with the same name but at different levels. The first is analogous to a 'forward declaration' in a language to allow a compiler to know about something that will be referred to in a more abstract way before it finds out about it in a more specific way.

By this analogy, ALA sometimes requires the concept of a forward-declared-abstraction, something that is clearly more abstract than the concrete implementations. Therefore, we can put this forward declaration in the next layer down, just as we would a paradigm interface. In the recursive descent compiler example, we would first have the abstract concept of a statement, meaning a unit of executable code as an interface in a lower layer. Then the specific abstractions, compound statement, if statement and so on are in a higher layer. They both provide and accept the interface.

Another language example is that an expression is composed of terms, a term is composed of factors, and a factor can be composed of expressions (enclosed in round brackets). If we model these compositions as direct knowledge dependencies, we would have too many layers - and they would not be becoming more abstract as we go down. The existence of the recursion at the end reinforces that. It seems that all three, expressions, terms and factors, should have abstract interface versions at a lower level.

Not all cases of recursion would require these interfaces. If, for example, in your old way of doing things there is a long chain of function calls, with the last one calling the first one, all of them are probably run-time dependencies, not knowledge dependencies at all. So in ALA, they should all be changed to be wirable, and wired together by an abstraction in a higher layer. The paradigm interface that is used to allow them to be wired  may be,for example, data-flow. So recursion does not necessarily require different interfaces for each different abstraction involved in the circular dependency.

////
With that in mind, we return to expression - term - factor - expression example and ask ourselves if there are really knowledge dependencies involved at all. Do we think have to know about terms to understand what an expression is? The answer is probably no. Besides, adding terms is only one way of making an expression. What if we think of expressions, terms and factors as language elements that are wirable using a paradigm interface called 'can consists of'. That's probably more like how our brains think of it. We could even draw a diagram of the language using 'can consiste of' relationships between elements. The result is an Abstract Sytax tree instead of the Sytax diagram above.
////

=== Abstraction of Port I/O properties

This is an advanced topic that allows abstractions to be written without knowing details of the implementation of the communications. The idea is for the language to support logical or abstracted I/O ports that work for any type of technical communication properties such as described in the sections below. If we allow these properties to be binded late, say at compile-time, they can be changed independently of the domain abstractions. This allows tuning of performance or physical deployment of the abstractions to different processes or hardware. 

I have been looking into how this could be accomplished using a conventional language, but it seems quite hard.

==== Push or Pull

Say an abstraction has a single logical input that can be wired to and a single logical output that can be wired from. Both the input and the output could be used in either a push or a pull manner.

For the input, push means we will be called with the data.  Pull means we will call out to get the data.

For the output, push means we will call out with the data, and pull means that something will call us to get the data.

There are four combinations possible:

* push push : push through
* pull pull : pull through
* push pull : internally buffer the input or output
* pull push : active object

Let's imagine we have a function that processes the data inside the abstraction.

The four combinations would require the function to run as a result of a function call from the input, or a function call from the output. The function result may be put into an internal buffered or be pushed out. The function may need to receive its input from an input buffer or by pulling. The function may need to run via a 3rd input that is polled or called by a timer.

We could conceivably write an abstract I/O class with an output interface and an input interface and a configuration interface that allows it to be configured late on how to do the I/O. This abstract I/O object would call the function to do the work at the right time according to its configuration.

==== Synchronous or Asynchronous

==== Buffered or unbuffered

==== Shared memory or messaging

==== Exposed state plus notification

==== Synchronous Request/Response


=== Working with legacy code

In old (non-ALA) legacy code, abstractions, if they ever existed, have usually been destroyed by coupling. If there is no model left by the original designer, or it is out of date, I first create one. I usually have to 'reverse engineer' the model by doing many 'all files searches' and trying to build a mental picture of how everything fits together. It can quickly become mentally taxing if the user story is non-trivial. So I build a UML class diagram from the searches (their one useful application) as the background (using light lines), and a tree of method calls for the specific user story on top of it (using heavier lines). These diagrams can end up looking pretty horrific, because the knowledge of the user-story has become so scattered, especially when inheritance is involved. The tree of method calls will come into the base class but leave from a subclass method.

This process can take several hours to a day for a single user story.
Once the code for the single user story is understood, some acceptance tests are put in place for it, by putting in insertion points as close as practical to the inputs and outputs for the user story. 

The next step is to factor out the method call tree for the user story into a new abstraction. This typically contains a sequence of new abstract activities or data transformations. These new abstractions are pitched at the domain level. Sometimes, if in C#, I will use Reactive Extensions. The user story may become a single RX sequence. The abstractions are then implemented, with tests, by copying and pasting useful code snippets from the original classes into the new abstractions. The old classes are marked for deprecation. 

Conversion of user stories takes place iteratively.  



=== Writing tests architected in ALA

TBD

=== Debugging ALA programs

Because in ALA you can get multiple instances of the same class used in multiple places, and multiple implementations of the same interface used in different places, debugging is easier if the instances are able to identify themselves. For this reason I tend to have a local property in every class called Name. The property is immutable and set by the constructor.

=== ALA language features

One of my first hobby programming projects was a compiler for a C-like high level language for embedded systems. At the time I had lots of energy to write the compiler and optimize the object code (written in itself, the performance of both compiling and of object code execution beat the first C compilers to later appear by around a factor of ten) but I lacked a lifetime of experience to design a language. Forty years later, I feel as if it's partially the other way around, at least for language feature that would support good architecture. The language I should have implemented way back then should have been an ALA language - one that supported ALA architecture by having the needed constraints.

It would have had Abstractions and Instances as first class elements. The name Abstraction is to reinforce the obvious use of the only type of element that the brain uses at design-time for any kind of separation of concerns.

It would support a single type of relationship - a knowledge dependency. You would have to define your four layers, and keep them in separate folders so you would be forced to decide at what abstraction level any given piece of design-time knowledge would go. Of course, it would only allow knowledge dependency relationships from one layer to a lower layer. If you wanted to add an extra layer to the chain of dependencies, that would be a bad design decision. For example, if your application is getting too large, you could create a layer between it and the domain abstractions layer called 'plug-ins'. 

Instances would work like components in that they would have ports for I/O. Like interfaces, ports are defined in a lower layer. The only way of instantiating abstractions and connecting them together is inside an abstraction in a higher layer.

Abstractions would support multiple ports of the same interface. Current languages have the difficulty that you can only implement one interface of a given type, which we had to workaround by having connector objects.

Ports would support late configuration of all communication properties such as push, pull, asynchronous, synchronous (explained above) without changing the Abstraction. 

Such a language would overcome many of the problems of current languages that encourage non-ALA compliant practices. But the invention of good abstractions in the first sprint of any green-field project would still be a skilled phase requiring an innate ability to abstract.

////
TBD: Direct representation of diagrams as text. Can use symbolic references (that represent what would be lines on the equavalent diagram) within a  local scope only. The target of such symbolic references are not abstractions and therefore do not need to be moved to a lower layer. Typically there will be at most 2 or 3 connections to the element. e.g. local variables, a local struct definition, or a local function that you would ideally liked to have been anonymous. In fact in the equivalent diagram, these elements would be anonymous.
////




= Feedback

Any feedback about this article is welcomed. Please send to johnspray274<at>gmail<dot>com

