:imagesdir: images

== Chapter six - ALA compared with:

(TBD: This chapter needs a review pass - some of thinking is out of date.)

In this chapter, our perspective is to compare ALA with existing programming paradigms, principles, styles, and patterns.

The idea is to understand ALA in terms of similarities and differences with something you may already understand.

A programming paradigm, principle, style, or pattern generally takes a long time for the average developer to master. It then only makes an incremental improvement to software quality, if any. ALA is a reference architecture that combines the best of these existing programming paradigms, principles, styles, and patterns into one coherent idea that you can learn. 

Some programming paradigms, principles, styles, or patterns are completely contrary to ALA. The most prominent examples are the UML class diagram, the idea of decomposition of a system (which includes all patterns like MVC), the idea that indirections are hard to trace, and the idea of loose coupling, the idea of dependency management without dustinquishing between good and bad dependencies, and conventional layering patterns (sometimes called stacks) where the layers are driven by bad dependencies.   


=== Monads

In chapter 3, we summarised a comparison with functional programming and composition with monads. We said that ALA is essentially composition of objects, whereas monads is composition of functions. 

Monads use objects under the covers. The objects are delegates (roughly object/function references), closures (objects of compiler generated classes that capture local scope variables), and other classes that deferred monads use under the covers. Because ALA uses objects made from explicit classes with ports, it is easier to understand, easier to debug, and more powerful. 

ALAs domain abstraction classes can have arbitrary ports with arbitrary programming paradigms. So where monads will do the dataflow parts of a program, ALA will do the entire program. The application code in both case the ALA case and the monads case is pure functional code.

Now we are going to go under the covers of monads to get a concrete understanding of how ALA works compared to monads in terms of the code differences.

Often abstract concepts can be explained in terms of how they are syntactical sugar for a longer imperative form of the code, especially for developers who are not so much mathematicians, but understand imperative object oriented code. This is hard to do for monads, especially deferred monads, because there is so much code under the covers. So most explanations of monads do not attempt to explain them in terns of syntactical sugar for some equivalent code. They go for an abstract or mathematical explanation instead. But I am going to explain monads in terms of refactoring of equivalent imperative code.

Most of the code snippets in this section are demonstrated by small projects on Github here:

https://github.com/johnspray74[https://github.com/johnspray74]


==== Composing functions

Let's start by composing three functions in imperative style.

In these examples, we will always feed the number 42 into the first function, just to make it easy for you to see thhe source of the composed function chain:

[source,C#]
....
int result = function3(function2(function1(42)));
....

Composing functions in this way is not fun because of all the nested brackets. So we can write it like this:

[source,C#]
....
int r1 = function1(42);
int r2 = function2(r1);
int result = function3(r2);
....

This form has the disadvantage of creating extra variables, which makes the entire scope more complicated because anywhere in the scope can potentially use any variable. However, the variables can be used to give meaningful names to the data, which is an advantage.

Another way we can compose the functions is by having a composing function that takes a value and a function and returns a value:

[source,C#]
....
int Compose(int x, Func<int,int> f)
{
    return (f(x));
}    
....

So now we can compose the functions like this:

[source,C#]
....
int result = Compose(Compose(Compose(42, function1), function2), function3);
....

Well there doesn't seem like there is any point to that. But let's persevere a little longer with the idea of using a compose function. A number of very cool advantages are coming up.

First let's make the Compose function an extension method by adding a _this_ keyword:

[source,C#]
....
static class ExtensionMethods
{
    public static int Compose(this int x, Func<int,int> f)
    {
        return f(x);
    }
}
....


Now we can compose the three functions using this syntax:

[source,C#]
....
int result = 42.Compose(function1).Compose(function2).Compose(function3);
....

That syntax is called fluent syntax. It gets rid of the brackets that we had in the imperative version, and the composition now goes from left to right. We can now easily add more functions to the chain. And if it gets too long for one line, we can easily break it over multiple lines. Fluent syntax is our first advantage of using a Compose function. 

[source,C#]
....
int result = 42
    .Compose(function1)
    .Compose(function2)
    .Compose(function3);
....

The three functions are chained in a way that it looks like a dataflow programming paradigm instead of imperative. It is more declarative. We are just specifying what functions we want to compose in what order instead of imperatively executing code. We will always use the extension method form of the Compose function from now on.

The Compose function takes a function pointer or delegate as its parameter. If the function is only ever used once, we can make it anonymous and put the function code directly into the Compose call:

[source,C#]
....
int result = 42
    .Compose(delegate(int x){return x+1;)
    .Compose(delegate(int x){return x*10+1;})
    .Compose(delegate(int x){return 1/x;});
....

Doing it with delegates like that is somewhat verbose, so the next step is to change the syntax to lambda expressions.

[source,C#]
....
int result = 42.Compose(x => x+1).Compose(x = x*10+1).Compose(x => 1/x);
....

So that's our third advantage of using a Compose function. We can use lambda expressions right in the parameter of Compose instead of creating separate named functions. Named functions are good if the function is a good abstraction in a lower layer. If a function is only used once ever, it is unlikely to be a good abstraction. So now the name becomes just a symbolic wiring between two points in the code. Symbolic wiring are bad because we have to search for teh two points. 

You can see that although this last form is just syntactic reworking of the original imperative code, it now looks even more like a dataflow programming paradigm than an imperative paradigm. By dataflow, we mean that we are thinking of it in terms of piping data from lambda expression to lambda expression rather than executing through statements.

So far we know that under the covers of the Compose function, the execution flow still follows the dataflow. Since we are just describing a dataflow with our code, it is possible for the execution flow under the covers to work in a completely independent different way. We can for example, implement deferred execution, where the Compose function builds an executable structure which can be run later. Later we will do even more powerful examples of an independent execution model such as asynchronous execution. These possibilities are our forth advantage of using a Compose function.

==== Deferred execution

The Compose function we had in the previous section evaluated the functions immediately and returned a result directly. If we write a deferred version, the Compose function will return a new function that represents the entire calculation, but can be called later. Here is a deferred version of the Compose function:


[source,C#]
....
static class ExtensionMethods
{
    static Func<int> Compose(this Func<int> f1, Func<int,int> f2)
    {
        return x => f2(f1());
    }
}
....

Note that in the imperative world we would say we are really passing in two pointers to functions, and returning a pointer to a function, but in the functional world this is said to be just composing functions. 

The Compose function returns a structure made up of delegates and closure objects:

image::ComposeClosure.drawio.png[title="Object diagram of the structure returned by the deferred version of the Compose function"]

The closure object has two fields, which are delegates to the source and the functions. A delegate is a pointer to an object together with a method in that object. A closure is an object made from a compiler generated class with a single method and one or more fields which were copied from the local scope when the closure was created.

We can use the deferred version of Compose like this:

[source,C#]
....
Func<int> composedFunction = 42.ToFunc<int>
    .Compose(x => x+1)
    .Compose(x = x*10+1)
    .Compose(x => 1000/x);
....

This doesn't call any of the lambda expressions. That can be done at any time later with:

[source,C#]
....
int result = composedFunction();
....


Deferred function composition generally returns a large object structure containing delegate objects and closure objects. Here is what the object structure for the composition above looks like:

image::ThreeComposedFunctionsClosureDiagram.drawio.png[title=Object diagram of expression composing three functions using deferred Compose function]

We will generally want a deferred version of a Compose function because then you have the option of using it immediately as if it wasn't deferred, or running it at a later time. Even if you run it immediately, the way a deferred composed function runs can be more efficient with use of memory. 


==== Functions that need logic between calls

The next step is a powerful refactoring we can do when we use a Compose function that we can't do when we use imperative code.

First we'll give four examples of imperative code. In each case, the functions we are composing are not returning a simple value that can be fed directly to the next function. They are returning a variety of different things, but in every case we need a little bit of extra code to handle what the function returns before feeding it to the next function.


===== Example 1

In this first example, we may need to allow for the fact that functions can throw an exception, or return null, or Maybe or even -1. For example the function may contain a divide by zero. In these situations, we would commonly have to add an if statement or a try statement so we don't call the rest of the functions in the case of a no value. In C code, -1 is often used for this purpose, so let's use that because it's very simple:

[source,C#]
....
// procedural composition of functions that can return -1 or null

int result1 = function1(42);
if (result1 != -1)
{
    int result2 = function2(result1)
    if (result2 != -1)
    {
        int result3 = function3(result2);
        if (result3 != -1)
        {
            DoSomething(result3);
        }
    }
}
// fall through means something returned -1
....

Note that every composed function involves an extra level of indenting, which is really nasty.


===== Example 2

We may have functions that return many values, such as an array, a list, an IEnumerable or an IObservable. We then want to feed all the individual values into the next function, and then recombine the results. For example, the function may be given customers one at a time and returns a list of their orders, which we want to join back into a single list of orders.


[source,C#]
....
// procedural composition of functions that return a list

var results1 = function1(42);
List<int> combinedList1 = new List<int>;
foreach(result1 in results1)
{
    var results2 = function2(result1)
    List<int> combinedList2 = new List<int>;
    foreach (result2 in results2)
    {
        var results3 = function3(result2)
        combinedList2.Append(results3);
    }
    combinedList1.Append(combinedList2);
}
List<int> result = combinedList1;
....

Again note the extra level of indenting for every composed function.

===== Example 3

The functions may contain delays, or they may wait for input or output. So each function may return a Task, future or promise object. The glue code needs to wait for the future object to have the result before calling the next function: 

[source,C#]
....
// procedural composition of functions that return a future object

TaskCompletionSource<int> tcs = new TaskCompletionSource<int>();
Task<int> result1 = function1(x);
result1.ContinueWith((t)=>
{
    Task<int> result2 = function2(t.Result)
    result2.ContinueWith((t)=>
    {
        Task<int> result3 = function3(t.Result);
        result3.ContinueWith((t)=>tcs.SetResult(t.Result));
    });
});
Task<int> result = tcs.Task;
....

Again notice the nasty indenting required for every function we want to chain.

===== Example 4

There can be many other motivating examples. In fact we can do almost anything we like between the function calls as long as we are always doing the same thing or something we can factor out as an abstraction. Let's do one more example just to show that we can do something fairly arbitrary. Let's say we always want to do modulo 360 arithmetic. And let's throw in a rotation counter as well:

[source,C#]
....
// procedural composition of functions that can return null

int rotations = 0;
int result1 = function1(42)
rotations += result1 / 360;
result1 = result1 mod 360;
int result2 = function2(result1)
rotations += result2 / 360;
result2 = result2 mod 360;
int result3 = function3(result2)
rotations += result3 / 360;
result3 = result3 mod 360;
....

==== The monad pattern


The pattern in all the examples above is that the functions being composed were returning something that couldn't directly be fed to the next function. We needed some extra logic code after each function call to handle that. That logic code was always the same.

What we want to do is refactor out the common logic code. We want to get back to that simple declarative idea of just composing functions. With the imperative code, this is hard to do. But when we use a Compose function, we can easily just refactor that common code into the Compose function.

This is our forth big advantage of using a Compose function. The refactoring is called the monad pattern. For monads the Compose function is called Bind. It also goes by other names such as ==>, flatmap, and SelectMany. What it's called depends on the programming language and the type which the Bind function takes and returns. From now on we will call the Compose functions that contain this extra common code _Bind_.

You might have noticed that in our immediate example of Compose above, the type that Compose takes and returns was a value. But in the deferred versions, the type that Compose takes and returns was a Func<T>. For Bind, this type that the Bind function takes and returns will generally be an interface, which we will refer to as just _Interface<T>_. This interface is also called the _monad type_. The interface can be anything we want in order to support that common code. It can be used to transmit any extra information that the common code needs. It can be an actual interface, such as IEnumerable<T>, or IMaybe<T>, or it can be a class (which we can think of as an interface with only one implementation) such as Task<T>.

Bind always takes an interface and returns the same interface. (Well in the case of the -1 monad, which we will do first, the 'interface' we use is integer, but we use that integer for two different things, so we can still think of it as an interface.) So you can chain Bind calls together using that same fluent syntax we used with Compose. The interface carries through anything the additional logic code needs, right through the chain. 

The interface is usually generic, so takes a type as a parameter, e.g. IEnumerable<T>. The Bind function takes an Interface<T> and returns an Interface<U>. The functions that are composed take a T and return an Interface<U>. Notice that the generic type can change between the input and the output of the Bind function.

Consider the types that the Bind function is using along the chain. It's sometimes difficult to tell what the types are. That's one of the problems with monads. What is the type of anything in this statement:

[source,C#]
....
var I4 = source.Bind(function1).Bind(function2).Bind(function3);
....

The compiler knows, so we should be able to know. Let's expand that fluent version, and also put function types in place of the functions themselves:

[source,C#]
....
Interface<T> I1 = source;
Interface<U> I2 = I1.Bind(func<T, Interface<U>>);
Interface<V> I3 = I2.Bind(func<U, Interface<V>>);
Interface<W> I4 = I3.Bind(func<V, Interface<W>>);
....

As you can see, while Bind always takes an interface and returns the same interface, the internal type may change along the way. In our examples we wont be changing teh type much, but remember that you can.

Here is a diagram of the monad pattern.


image::MonadPattern.png[title=The monad pattern]


As you can see, monads are a 2-layer pattern. The two layers of monads correspond roughly with ALA's application and programming paradigms layers. The code that uses Bind to compose functions, and the lambda functions themselves are in the application layer. The Bind function and the Interface<T> are in the programming paradigms layer. (Often monads come with a set of more specialized functions such as Sort, Filter or Sum. These would go in the equivalent of the domain abstractions layer. These functions either use Bind, or do the equivalent logic as Bind themselves.)

So monads allow you to compose on two abstraction layers at the same time, the green and the yellow parts. In the higher layer you have the functions that you are composing to build a specific application. In the lower layer, the Bind function contains that common code. Everything is more abstract and more reusable in the yellow layer.

The functions that are being composed take a T and return an Interface<U>. It is tempting to think that the Bind function simply returns the Interface<U> that is returned by the function, because they have the same type. But that is not usually the case. Bind usually creates a new object that implements Interface<U>, and then combines information from both the input Interface<T> and the output of the function, Interface<U>, to provide the output Interface<U>. That's what the diagram is trying to convey.

In many explanations of monads, they call the interface the monad type, a wrapped type, a container type, a type in a box, an amplified type, or just the notation M T. I don't think any of these forms are helpful in explaining monads. The wrapped, container and box terms don't work well for deferred monads, which don't actually contain a value. They contain a means of getting a value. For example, the deferred version of a list is IEnumerable. If our function returns an IEnumerable, that's not really a container. Generally we can just think of it as an Interface<T>

The term _amplified_ just introduces another seemingly abstract concept which is unnecessary. And the term Monad type or the notation M T seems a bit circular - as if we are going to explain monads in terms of monads. So I prefer to think of the thing that the Bind function takes and returns as an interface. It sometimes has one implementation, such as Task or List, but often it has more than one implementations such as IMaybe or IEnumerable. Usually the Maybe monad use IMaybe with two implementations, one for when there is a value and one for when there is no value. 

The monad pattern requires three things: 
* Interface<T>
* a constructor or method for making ordinary values of type T into an object that  implements Interface<T>
* a Bind function that takes an Interface<T>, returns an Interface<U>, and takes a function of the form Func<T, Interface<U>>.

The interface always provides a way of getting the values (or values) out of the monad object.

Bind can pipe any extra information or capability we want through the interface. We could, for the sake of a silly example, pipe through an audio stream if we really wanted to. The bind function would take care it. The lambda expressions would not even have to know about it. But they could generate little sound segments to be inserted into the audio stream. 
Now we are going to look at the code in the Bind function for each of our imperative examples.

These bind functions are demonstrated by small projects on Github here:

https://github.com/johnspray74[https://github.com/johnspray74]



The first examples of Bind that we will look at are _immediate_ execution. Immediate means the Bind function actually calls the functions, executes the common code, and returns the result directly. We will do them first because they are very simple, and because we want to understand the difference between immediate monads and _deferred_ monads.

Many explanations of monads fail to explain that you can make immediate or deferred versions of Bind, which leads to confusion about what monads are. Monads are not necessarily about deferred execution, although they often are. They are also not solely about fluent syntax, which can be used for other things as well. 

So let's looks at the very simple immediate monads. We are going to start with the MinusOne monad: 




===== immediate monad: MinusOne

In this case the extra information that goes through the interface is the 'no value', represented by -1. When there is a value it must be a positive integer. Because the integer type can handle both -1 and positive values, the interface in this case can be thought of as just the type integer. This is an unusual case, but it results in a simple Bind function, so we will start with it. 

First let's see what the imperative code becomes when we use our new Bind function.

Application layer code
[source,C#]
....
// composition of functions that might return -1

int result = 42.Bind(x => x+1).Bind(x = x*10+1).Bind(x => x==0 ? -1 : 1/x);
....

Compare this with the imperative version above which had to use nested if statements.

It's almost the same as when we used Compose except that we are using Bind instead of Compose. Also the composed functions are now allowed to return -1 if they can't compute a result. The function that uses a division is an example.


Here is the Bind function:

Monad layer code
[source,C#]
....
public static class MinusOneExtensionMethods
{

    public static int Bind(this int source, Func<int, int> function)
    {
        return source == -1 ? -1 : function(source);
    }
}
....

You can see that if any function in the chain returns -1, the rest of the functions are skipped and the final result is -1.

That is all there is to our first monad.


===== Immediate monad: IMaybe

Using minus one to represent a no value is not used outside the C world, and has limited use with only positive integers. The general solution is the IMaybe<T> monad:

The IMaybe version is similar to the -1 version. However Bind in this case requires an IMaybe and returns an IMaybe, and the functions that we compose together also return an IMaybe:

Application layer code
[source,C#]
....
IMaybe<double> combinedFunctions = 42.ToMaybe()
.Bind(x => new MaybeSomething<int>(x+1))
.Bind(x => new MaybeSomething<int>(x*10+1))
.Bind(x => x==0 ? new MaybeNothing<double>() : new MaybeSomething<double>((double)1/x) );
....

Notice that we need to convert the starting value, 42, to a Maybe so that Bind can be used on it. That's the reason for the ToMaybe extension method. To be a monad, we need to supply this function. In monad land, it is sometimes calledthe unit function or the return function. 

The IMaybe interface itself consists of two getters, one called HasValue() that returns a bool to find out if a value is there, and the other called Value to get the actual value out if there is one. 

[source,C#]
....
public interface IMaybe<T>
{
    bool HasValue { get; }
    T Value { get; }
}
....

You would normally use HasValue first and only if it returns true would you use Value. HasValue is analogous to the MoveNext method in the IEnumerator interface, which you also have to call first before retreiving a value.

The Bind function uses its input IMaybe<T> to see if there is a value present or not. If there is nothing it doesn't even call the function. It just returns a new IMaybe<U> implemented by a nothing object. If there is a value, it gets the value and passes it to the function, then Bind returns the IMaybe returned by the function.


Monad layer code
[source,C#]
....


public static class MaybeMonadExtensionMethods
{
    public static IMaybe<T> ToMaybe<T>(this T value)
    {
        return new MaybeSomething<T>(value);
    }


    public static IMaybe<U> Bind<T, U>(this IMaybe<T> source, Func<T, IMaybe<U>> function)
    {
        return source.HasValue ? function(source.Value) : new MaybeNothing<U>();
    }
}


public class MaybeNothing<T> : IMaybe<T>
{
    bool IMaybe<T>.HasValue { get => false; }
    T IMaybe<T>.Value { get { throw new Exception("No value"); } }
}


public class MaybeSomething<T> : IMaybe<T>
{
    private T value;

    public MaybeSomething(T value) { this.value = value; }

    bool IMaybe<T>.HasValue { get => true; }
    T IMaybe<T>.Value { get => value; }
}
....


The monad consists of the IMaybe<T> interface, and the BInd and ToMaybe extension methods. These method use two concrete implementations of IMaybe, one for represnting no vale, and one for represeting a value.



===== Immediate monad: List


Application layer code
[source,C#]
....
// monad composition of functions that return a List

List<int> result = new List<int>(){42}.Bind(function1).Bind(function2).Bind(function3);
....

The functions each return a list. Here is the same application code with lambda expressions:

[source,C#]
....
var result = new List<int> { 0 }  
.Bind(x => new List<int> { x * 10 + 1, x * 10 + 2, x * 10 + 3 })
.Bind(x => new List<int> { x * 10 + 1, x * 10 + 2, x * 10 + 3 })
.Bind(x => new List<int> { x * 10 + 1, x * 10 + 2, x * 10 + 3 });
....

This is the list example. In this case Bind will receive a list as its input. It will feed all the values one by one to the function. Each call of the function will return a new list. Bind will append all the lists together and return the combined list:

image::ConsoleOutputListMonad.png[]


Monad layer code
[source,C#]
....
public static class ExtensionMethods
{
    public static List<U> Bind<T, U>(this List<T> source, Func<T, List<U>> function)
    {
        List<U> output = new List<U>();
        foreach (T t in source)
        {
            var List<U> functionOutput = function(t);
            output.AddRange(functionOutput);
        }
        return output;
    }
}}
....

Let's say the List<T> input were a list of students. Bind uses a for loop to get all the students one at a time. It passes each student to the function. Each call of the function returns a List<U>. Let's say this is a list of courses for the student. The bind function then joins all the separate course lists together to make a single list of courses of type List<U>, which it returns.

Note that the monad itself is designed to compose functions that return lists, and to flatten then the lists. Often we will want to just do a one-to-one mapping of the values in a list, or we will even want to aggregate the values in the list down to a single value such as Sum. Methods to do these are usually supplied along with the monad, but the monad itself just consists of the IMaybe<T> interface, the Bind function, and the ToMaybe function.


===== Immediate monad mod360

This is not strictly speaking a monad because the function doesn't return the same interface as the Bind function uses for its input and output. That's because in this case the function didn't need to know anything about the rotations. However it still shows how the monad pattern can refactor arbitrary common code around composed functions.

Application layer code
[source,C#]
....
// monad composition of functions that do some arbitrary code
// The second value in the Tuple is the number of rotations, which we initialize to 0.

Tuple<int,int> result = new Tuple(42,0).Bind(function1).Bind(function2).Bind(function3);
....


Monad layer code
[source,C#]
....
public static Tuple<int,int> Bind<T, U>(this Tuple<int,int> source, Func<int, int> function)
{
    int result = function(source.Item1);  // call the function
    return new Tuple<int,int> (
        result mod 360,   // normalize the angle
        source.item2 + result/360);   // count rotations
}
....


This time Bind takes a Tuple and returns a Tuple. The Tuple contains the angle between 0 and 359 and the rotations. Bind will do the mod 360 on the result returned by the function, and add any rotations. It returns a new Tuple with those two values.

Note that it was easy to get the starting 42 value into the Tuple needed by the Bind function by simply using 'new Tuple(42,0)'. So in this case we didn't need like a ToTuple method.

Those were the immediate versions of the monads. Let's now have a look at the deferred versions of these monads:


==== Deferred monads

If the monad is an immediate (eager) type, the value returned by the monad chain, I4, is the actual result. But if the monad is a deferred type, the value returned by the monad chain is a structure of wired up objects ready to be run to get the result. 

With deferred monads, depending on the monad, you might do things like the following to get the actual result out of the object structure.



[source,C#]
....
var result = CombinedFunction(0);
....

[source,C#]
....
if (result!=-1) { result }                   // -1 monad
if (result.hasValue) { result.value }        // maybe monad
result.ToList()                              // IEnumerable
foreach (var value in result) {...}          // IEnumerable
result.Subscribe((x)=>{....})                // IObservable
result.ContinueWith(result => result.Result) // task
await result                                 // task
result.Item0, result.Item1                   // tuple

....






===== Deferred monad MinusOne

For the deferred version of the MinusOne monad, we use Func<int> instead of an integer as the interface. The Bind function takes Func<int> and returns Func<int>:


[source,C#]
....
// deferred monad composition of functions that might return -1

Func<int> CombinedFunction = 
    42.Bind(x => x+1).Bind(x = x*10+1).Bind(x => x==0 ? -1 : 1/x);
}
....


The Bind implementation doesn't call the function, it returns another function that will do that later: 

Pull version
[source,C#]
....
namespace Monad.MinusOne
{
    public static class ExtensionMethod
    {
        public static Func<int> ToMinusOne(this int source)
        {
            return () => source;
        }

        public static Func<int> Bind(this Func<int> source, Func<int, int> function)
        {
            return () =>
            {
                int value = source();
                return value == -1 ? -1 : function(value);
            };
        }
    }
}
....

The lambda functions are turned into closure objects by the compiler. The returned object structure looks like the diagram below.

image::MinusOneDeferredPullMonadDiagram.drawio.png[title=Object diagram of expression using deferred/pull version of MinusOne monad"]

This structure is exactly the same as the one we showed above for the Compose function. The only difference is that for the three closures that are created by the Bind function, the closure method contains the common code, that is it checks it received -1 from the source before calling the function.

We got a little lucky with the implementation of the deferred/pull MinusOne monad. That is that we were able to use Func<int> as the interface that Bind takes and returns instead of using an actual interface with a function in it. That allowed us to use closures to implement the Bind and ToMaybe functions, just as we did for teh Compose function. From now on we won't be able to do that because the monads will be using an actual interface. 


===== Deferred monad MinusOne (push)

With deferred monads, we can do both pull versions and push versions, which we will do to properly understand teh nuances of each. Here is the application code that composes the three functions:

[source,C#]
....
// deferred monad composition of functions that might return -1

IMinusOneObservable<int> CombinedFunction = 
    42.ToMinusOne().Bind(x => x+1).Bind(x = x*10+1).Bind(x => x==0 ? -1 : 1/x);
}
....

The interface used is is IMinusOneObservable. Here it is:

[source,C#]
....
public interface IMinusOneObserver
{
    void Push(int value);
}
....


Pushing requires that the interface is wired in the same direction as the dataflow (although we could have chosen to use C# events instead). Since this interface is wired in the same directon as the dataflow, destinations implement the interface and sources will have a field of the type of this interface.

Bind can't be defined on this interface because it's the wrong way around. Bind takes and returns an interface that goes in the opposite direction of the dataflow. This allows the fluent syntax to go in the same direction as the dataflow. So what we do is have another interface just for Bind to use:

[source,C#]
....
public interface IMinusOneObservable
{
    void Subscribe(IMinusOneObserver observer);
}
....

All this interface does is give Bind a way to wire up the other interface. And yes these two interfaces are exactly analogous to the IObservable and IObserver interfaces in reactive extensions.


For the push version we don't have the luck we had in the pull version that allowed us to implement it with closures because we had to use the IMinusOneObserver interface. The Bind and ToMinusOneMonad methods both need to create an object from an explicit class that we need to define: 



Push version
[source,C#]
....
namespace Monad.MinusOne
{
    public static class ExtensionMethods
    {
        public static IMinusOneObservable ToMinusOneMonad(this int value) <5>
        {
            return new MinusOneStart(value);
        }

        public static IMinusOneObservable Bind(this IMinusOneObservable source, Func<int, int> function) <1>
        {
            MinusOne minusOne = new MinusOne(function);
            source.Subscribe(minusOne);
            return minusOne;
        }
    }




    class MinusOne : IMinusOneObservable, IMinusOneObserver <1>
    {
        private IMinusOneObserver observer; <2>

        private Func<int, int> function;

        public MinusOne(Func<int, int> function) <3>
        {
            this.function = function;
        }

        void IMinusOneObserver.Push(int value)
        {
            if (value == -1)
            {
                observer.Push(-1);
            }
            else
            {
                observer.Push(function(value));
            }
        }

        void IMinusOneObservable.Subscribe(IMinusOneObserver observer)
        {
            this.observer = observer;
        }
    }




    class MinusOneStart : IMinusOneObservable <5>
    {
        private int value;
        private IMinusOneObserver observer;


        public MinusOneStart(int value) { this.value = value; }

        void IMinusOneObservable.Subscribe(IMinusOneObserver observer)
        {
            this.observer = observer;
        }

        public void Run()
        {
            observer.Push(value);
        }
    }
}
....

<1> The Bind method just instantiates a class to do the work later. The Bind function also wires up the IMinusOneObserver interface using the Subscribe method.  

<2> IMinusOneObservable is implemented by sources. IMinusOneObserver is implemented by destinations. Since our MinusOne class, as part of a chain of operations, it is both a source and a destination, so it implements both. Once wired, the only reference between the objects is the reference from source to destination is the field called observer in the MinusOne class. 

<3> The constructor just needs to store the function.

<4> The Push method is the only part that runs when the monad object structure runs.

<5> The last thing to note is the usual method we need to get the 42 into the monad type so that we can start using Bind. In this case the monad type is IMinusOneObservable, so there needs to be a class that implements IMinusOneObservable. That class is MinusOneStart. The ToMinusOne extension method simply needs to instantiate this class.

Here is the object diagram of the resulting structure:

image::MinusOneDeferredPushMonadDiagram.drawio.png[title=Object diagram of expression using deferred/push version of MinusOne monad]

You can see that the three delegate-closure pairs we had in the pull version are replaced with an object of class MinusOne. The three objects are wired together in the direction of the data flow (left to right) using the IMinusOneObserver interface. The IMinusOneObservable was only used by the Bind function to effect the wiring. It is unused when the structure runs. The IMinusOneObservable interface at the end can be used to wire to an output object that implement IMinusOneObserver.

The 42 is stored in the object of the MinusOneStart class. This class has a run function which is used to start the structure executing. We start it from the source end because it is a push monad we are using. (This differs from the reactive extensions, which starts executing on Subscribe, so execution is actually initiated from the destination end.) In ALA push programming paradigms, we usually initiate dataflow at the source end.

You can start to see the ALA pattern to this structure. It is instantiating objects and wiring them together to build a structure to run later. IMinusOneObserver is the programming paradigm.

All the deferred monads we do from now on have this same structure. The push ones will be wired in the direction of dataflow, left to right, like this one is. The pull ones will be wired in the opposite direction of the dataflow, right to left. As I said, we were just lucky that the deferred pull version of this MinusOne monad that we did above was able to be implemented with closures because the monad type was Func<int> instead of a real interface. We will always need an explcit  class from now on. 

Next well do a deferred pull monad that uses a real interface<T>, the IMaybe<T> monad.



===== Deferred monad IMaybe (pull)

Here is application code to use the deferred/pull implementation of the maybe monad.


[source,C#]
....
IMaybe<double> combinedFunctions = 42.ToMaybe()
.Bind(x => new MaybeSomething<int>(x+1))
.Bind(x => new MaybeSomething<int>(x*10+1))
.Bind(x => x==0 ? new MaybeNothing<double>() : new MaybeSomething<double>((double)1/x) );
....

It looks the same as the immediate version. But it returns an IMaybe that's an object structure instead of returning one of the two concrete Imaybe value objects. 

First we define the IMaybe interface, which is the same as for the immediate version above. The MaybeNothing and MaybeSomething classes are also the same as before.


[source,C#]
....
    public interface IMaybe<T>
    {
        bool HasValue { get; }
        T Value { get; }
    }

    public class MaybeSomething<T> : IMaybe<T>
    {
        T value;

        public MaybeSomething(T value) { this.value = value; }

        bool IMaybe<T>.HasValue { get => true; }
        T IMaybe<T>.Value { get => value; }
    }



    public class MaybeNothing<T> : IMaybe<T>
    {
        bool IMaybe<T>.HasValue { get => false; }
        T IMaybe<T>.Value { get { throw new Exception("No value"); } }
    }
....


The Bind function is different as it must build a structure that can be run later. It instantiates a class that implements IMaybe, which will do all the work at runtime.

[source,C#]
....
namespace Monad.MaybeDeferredPull
{
    public static class ExtensionMethods
    {
        public static IMaybe<T> ToMaybe<T>(this T value)
        {
            return new MaybeSomething<T>(value);
        }

        public static IMaybe<U> Bind<T, U>(this IMaybe<T> source, Func<T, IMaybe<U>> function)
        {
            return new Maybe<T, U>(source, function);
        }
    }



    class Maybe<T, U> : IMaybe<U>
    {
        // implement the constructor, which receives the Action function
        private Func<T, IMaybe<U>> function;
        private IMaybe<T> source;
        private IMaybe<U> result;

        public Maybe(IMaybe<T> source, Func<T, IMaybe<U>> function) { this.source = source; this.function = function; }

        bool IMaybe<U>.HasValue 
        { get 
            {
                if (result == null)
                {
                    if (source.HasValue)
                    {
                        result = function(source.Value);
                    }
                    else
                    {
                        return false;
                    }
                }
                return result.HasValue;
            }
        }

        U IMaybe<U>.Value
        {
            get
            {
                if (result == null)
                {
                     result = function(source.Value);  // will throw exception if no value
                }
                return result.Value; // will throw exception if no value
            }
        }
    }
}
....

The code that runs later in the Maybe class is the HasValue and Value getters. They do all teh work. 

Bind creates objects of the class Maybe and chains them together. This diagram shows the resulting structure from our little bit of application code:


image::MaybeDeferredPullMonadDiagram.drawio.png[title=Object diagram of expression using deferred/pull version of IMaybe monad]

Because this is a pull implementation of the monad, the references go in the opposite direction of the dataflow - from destination to source or from right to left. When you want to run the combined function, you pull the value from the right end. 



===== Deferred monad IMaybe (push)

Now the push version of the deferred IMaybe monad. Here is the application layer code, which in this case returns a IMaybeObservable.

[source,C#]
....
// composition of functions that return IMaybe

IMaybeObservable<int> combinedFunctions = 
    42.ToMaybe().Bind(function1).Bind(function2).Bind(function3);
....

I've purposely left the lambda expressions out for now. Well get back to them in a minute.

As with the deferred push version of the MinusOne monad, we need two interfaces, IMaybeObservabe<T> which Bind takes and returns, and IMaybeObserver for doing the actual pushing of data at runtime.

Here are the two interfaces:

[source,C#]
....
    public interface IMaybeObservable<T>
    {
        void Subscribe(IMaybeObserver<T> observer);
    }
....


[source,C#]
....
    public interface IMaybeObserver<T>
    {
        void NoValue();
        void Value(T value);
    }
....

The Bind functon uses the IMaybeObservable to wire the IMaybeObserver interface in the oposite direction.

Now we consider the type that the functions that you compose should return. Normally with monads, this is the same interface that Bind takes and returns. So that would be IMaybeObservable. IMaybeObservable will certainly work, but the functions will be a little complicated. They will have the form: Func<T, IMaybeObservable<U>>. They would have to create an object implementing the IMaybeObservable interface to return. That interface then has a Subscribe method called on it, which gives the object an IMaybeObserver. Then the object can finally push out its result by pushing it via the IMaybeObserver.

It would be just so much simpler if the functions were passed the IMaybeObserver when we call them. If we did that, the functions would have the form Action<T, IMaybeObserver<U>>. Now when the functions run, they don't need to create an object to return. Instead they just directly push the result out via the IMaybeObserver<U>> interface they already have. This kind of makes sense because it's a push monad. 

Here is the application layer code with lambad expressions:

[source,C#]
....
IMaybeObservable<double> combinedFunctions = 42.ToMaybe()
.Bind((x,ob) => ob.Value(x+1))
.Bind((x,ob) => ob.Value(x*10+1))
.Bind((x,ob) => { if (x==0) ob.NoValue(); else ob.Value((double)1/x); } );
....

So remember when reading the monad implementation below, the functions that you compose in the application layer take a T and a IMaybeObserver<U> and don't return a value.


[source,C#]
....
namespace Monad.MaybeDeferredPush
{
    public static class ExtensionMethods
    {
        public static IMaybeObservable<T> ToMaybe<T>(this T value)
        {
            return new MaybeStart<T>(value);
        }

        public static IMaybeObservable<U> Bind<T, U>(this IMaybeObservabe<T> source, Action<T, IMaybeObserver<U>> action)
        {
            var maybe = new Maybe<T, U>(action);
            source.Subscribe(maybe);
            return maybe;           
        }
    }





    class Maybe<T, U> : IMaybeObserver<T>, IMaybeObservable<U>
    {
        private Action<T, IMaybeObserver<U>> action;

        public Maybe(Action<T, IMaybeObserver<U>> action) { this.action = action; }


        private List<IMaybeObserver<U>> subscribers = new List<IMaybeObserver<U>>();

        void IMaybeObservable<U>.Subscribe(IMaybeObserver<U> observer)
        {
            subscribers.Add(observer);
        }


        void IMaybeObserver<T>.NoValue()
        {
            foreach (var subscriber in subscribers)
            {
                subscriber.NoValue();
            }
        }

        void IMaybeObserver<T>.Value(T value)
        {
            action(value, new ActionObserver<T, U>(this));
        }


        private class ActionObserver<T, U> : IMaybeObserver<U>
        {
            private Maybe<T, U> outer;
            public ActionObserver(Maybe<T, U> outer) { this.outer = outer; }

            void IMaybeObserver<U>.NoValue()
            {
                foreach (var subscriber in outer.subscribers)
                {
                    subscriber.NoValue();
                }
            }

            void IMaybeObserver<U>.Value(U value)
            {
                foreach (var subscriber in outer.subscribers)
                {
                    subscriber.Value(value);
                }
            }
        }
    }




    class MaybeStart<T> : IMaybeObservable<T>
    {
        private T value;
        public ToMaybe(T value) { this.value = value; }

        private List<IMaybeObserver<T>> subscribers = new List<IMaybeObserver<T>>();
        void IMaybeObservabe<T>.Subscribe(IMaybeObserver<T> subscriber)
        {
            subscribers.Add(subscriber);
        }

        public void Run()
        {
            foreach (var subscriber in subscribers)
            {
                subscriber.Value(value);
            }
        }
    }
....


As you can see, the Bind function just creates an object of the Maybe class to do all the work at runtime.

You will notice a class called ActionObserver inside the Maybe class. At runtime, the Maybe class will need to call the action, and it needs an object that implements IMaybeObserver to pass to that action. That's what ActionObsserver is for. 

The only other new thing is to support multiple subscribers. It is normal for push monads to support fan out, in other words many observers can be listening to the same data that is pushed.

Here is an object diagram of the complete expression.

image::MaybeDeferredPushMonadDiagram.drawio.png[title=Object diagram of expression using deferred/push version of IMaybe monad]

You can see that the references between the objects, which use IMaybeIobserver, go in the same direction as the dataflow. IMaybeObservable is only used for wiring the structure up.
The structure starts executing when the Run method in the MaybeStart object on the left is called. The application needs to keep a reference to this object so it can start the program. 


So far we have seen deferred pull and deferred push implementations of the MinusOne and Maybe monads. Let's do a couple more examples of deferred monads to get used to the pattern:


===== Deferred monad IEnumerable

The IEnumerable monad is the deferred version of the list monad we did earlier. The IEnumerable monad is the most commonly used monad, and is what LINQ is based on.

The Bind function is called SelectMany in C#. SelectMany is not used as often as Select. Select takes a simpler function that returns U instead of IEnumerable<U>, so it doesn't expand the number of items, it keeps the number the same and just transforms the values. While Select is used more often, it is the SelectMany function that makes it a true Monad. Here in our example application will use three SelectManys in a row. Each will expand in number by 3, so we will end up with an IEnumerable with 27 items in the end.  

[source,C#]
....
// monad composition of functions that return IEnumerables

IEnumerable<int> CombinedFunction = 
    42.ToEnumerable().SelectMany(function1).SelectMany(function2).SelectMany(function3);
....

You may remember that we used lambda expressions in the immediate example above that returned literally lists something like this:

[source,C#]
....
var result = new[] { 0 }  
.Bind(x => new[] { x * 10 + 1, x * 10 + 2, x * 10 + 3 })
.Bind(x => new[] { x * 10 + 1, x * 10 + 2, x * 10 + 3 })
.Bind(x => new[] { x * 10 + 1, x * 10 + 2, x * 10 + 3 });
....

While this will run fine, it's not really in the style of a deferred monad to create memory hungry arrays. So let's write functions that will do the same job in a lazy way:

[source,C#]
....
private static IEnumerable<int> MutiplyBy10AndAdd1Then2Then3(int x)
{
    yield return x * 10 + 1;
    yield return x * 10 + 2;
    yield return x * 10 + 3;
}
....

The _yield return_ keyword cause the compiler to generate an IEnumerable object, which it returns. The IEnumerable object contains a state machine where each state executes code till it hits the next yield return statement. 

Let's just reuse that function three times in our composed function:


[source,C#]
....
static void Application()
{
    var program = new[] { 0 }  
    .Bind(MutiplyBy10AndAdd1Then2Then3)
    .Bind(MutiplyBy10AndAdd1Then2Then3)
    .Bind(MutiplyBy10AndAdd1Then2Then3);

    var result = program.ToList();  // now run the program
    Console.WriteLine($"Final result is {result.Select(x => x.ToString()).Join(" ")}");
}
....

The Bind function (SelectMany) for this type of monad, takes an IEnumerable<T> and returns an IEnumerable<U>. The Bind function doesn't use a for loop immediately as that would defeat the laziness. Instead the bind function uses an object that keeps state. Let's call this object the _output IEnumerable_. The output IEnumerable knows how to use the source IEnumerable<T> to get the first value, which it gives to the function. The function returns an IEnumerable<U> which we will call the _function IEnumerable_. The output IEnumerable then knows how to get the values from the function IEnumerable<U> and return them one at a time. When it has exhausted all of them, the output IEnumerable<U> then gets the second value from the source IEnumerable<T>, and gives that to the function. The function again returns an IEnumerable<U>. This process continues until the source and function IEnumerables are both exhausted. 

In C#, the Bind function is really easy to write because the compiler can build an IEnumerable for you using the _yield return_ syntax:

[source,C#]
....
namespace Monad.Enumerable
{
    public static class MonadExtensionMethods
    {
        public static IEnumerable<U> Bind<T, U>(this IEnumerable<T> source, Func<T, IEnumerable<U>> function)
        {
            foreach (var t in source)
            {
                var enumerator = function(t);
                foreach (var u in enumerator)
                {
                    yield return u;
                }
            }
        }
    }
}
....

Note that the code in the function does not run when this Bind function runs. The compiler sees the _yield return_ and builds an object conatining a state machine and implementing IEnumerable<U>, and returns that.

Since our purpose is to show how the Bind function works with imperative code, here is a version that doesn't cheat by using the yield return syntax:


[source,C#]
....
public static class MonadExtensionMethods
{
    public static IEnumerable<U> Bind<T, U>(this IEnumerable<T> source, Func<T, IEnumerable<U>> function)
    {
        return new EnumerableMonad<T, U>(source, function);
    }
}
....
    
All Bind does is instantiate the class and return it. The class get passed the source and the function. The class implements IEnumerable, which means it must be able to return an object implementing Enumerator. The eesiest way to do that is have the class implements Enumerator as well. Then the IEmumerable can just return this.


[source,C#]
....
namespace Monad.Enumerable
{
    class EnumerableMonad<T, U> : IEnumerator<U>, IEnumerable<U>
    {
        private readonly IEnumerable<T> source;
        private readonly Func<T, IEnumerable<U>> function;
        public EnumerableMonad(IEnumerable<T> source, Func<T, IEnumerable<U>> function)
            { this.source = source; this.function = function; }


        IEnumerator<U> IEnumerable<U>.GetEnumerator()
        {
            return (IEnumerator<U>)this;
        }

        IEnumerator IEnumerable.GetEnumerator()
        {
            return this;
        }


        private IEnumerator<T> sourceEnumerator = null;
        private IEnumerator<U> functionEnumerator = null;

        U IEnumerator<U>.Current => functionEnumerator.Current;

        object IEnumerator.Current => throw new NotImplementedException();

        void IDisposable.Dispose() { }

        bool IEnumerator.MoveNext()
        {
            while (true)
            {
                if (functionEnumerator != null)
                {
                    if (functionEnumerator.MoveNext())
                    {
                        return true;
                    }
                }
                if (sourceEnumerator == null) sourceEnumerator = source.GetEnumerator(); 
                if (sourceEnumerator.MoveNext())
                {
                    functionEnumerator =
                        function(sourceEnumerator.Current).GetEnumerator();
                }
                else
                {
                    return false;
                }
            }
        }

        void IEnumerator.Reset()
        {
            functionEnumerator = null;
            sourceEnumerator = null;  
        }
    }
}
....

The IEnumerator MoveNext method does all the work of the class at runtime. It gets the first element from the source, and feeds it to the function. Then it stores the Enumerator it gets from the function so it can use it in subsequent calls. Then it gets the first element from the function's Enumerator and returns it. A while loop is necessary because when the Enumerator that is returned by the function runs out, it needs to go back and get the next element from the source and then start on the new Enumerator.

The class is completely lazy, so it doesn't even get the source IEnumerator from the source IEnumerable until the first call of MoveNext.

The two fields, sourceEnumerator, and functionEnumerator are the state. The first can have a state of null, which is the state before we got the first value. 

The object diagram for the program again shows three objects wired in a chain from right to left:

image::IEnumerableDeferredPullMonadDiagram.drawio.png[title=IEnumerable Deferred Pull Monad Object Diagram]


At first Bind just wires the IEnumerables, and then the IEnumerable is used to wire the IEnumerator. The IEnumerable interface is somewhat redundant - we could have made Bind just wire up the IEnumerator directly, however we still implement the IEnumerable for compatibility with other source of destination objects we may want to wire in. 


===== Deferred monad IObservable

The push version of the IEnumerable monad is the IObservable monad.

Some writers equate IObservable with "asynchronous". A pushing interface can be either synchronous or asynchronous. Data flow originates from the source by pushing data by calling a method. That method can execute synchronously all the way to the destination end of the chain, or it can return at any point along the chain - it doesn't matter. If it returns before the destination, the data flow can resume from that point at a later time. 

Pull communications can't be asynchronous or broken up in time, at least not without blocking the thread (we don't want to go there) or using Task or future objects (which we cover later). The IEnumerable interface pulls the result so can only work synchronously. The destination end pulls data by calling a method. The function must execute synchronously all the way to the source otherwise it would return without a result.  

The ability of push to do either synchronous or asynchronous is a good reason to default to using push style dataflow programming paradigms, and one of the reasons that ALA defaults to using push. So it is worth looking at the IObservable monad, even though IEnumerable monads tend to be more common. Push should really be the default way to do things in the absence of any good reason to use pull. 

I think the reason the IEnumerable monad is more common may be because it seems more suited for database queries. After all, for this context it is the destination, not the source, that knows when it wants data. Or at least it's usually something nearer the destination such as a button. However, this doesn't mean that database queries should use pull. The system could well benefit from using push based communications even from a database. For example, this would allow all the work to be done on the UI thread  a few records at a time until the entire data transfer is complete. Asynchronous will also naturally accommodate data transfers over a network.

To use push for database queries, you need only invent a programming paradigm that implements a data transfer as two push channels, one in each direction. A request push channel goes toward the database, and a response push channel comes back. In ALA, because you can easily implement programming paradigms, this is really easy to do, and should be the way database queries are done. A database adapter at the end implements this "push/push back" programming paradigm and does the work of actually talking to the database with SQL. This makes the IObservable monad more interesting to us, as we shall see later.

The example code for the IObseravble monad is going to be synchronous, but it would be easy to change it to asynchronous.

Unlike the IEnumerable/IEnumerator pair of interfaces which go in the same direction, the IObservable/IObserver interfaces go in opposite directions. The IObservable interface goes from destination to source because that is what Bind requires. The IObserver interface goes from source to destination, because it is a push interface used when data actually flows. 

The IObservable interface is only used by Bind, and it wires up the IObserver interface. In addition to wiring the IObserver interface, the IObservable.Subscribe method also initiates the data transfer. This means that the destination initiates the data transfer even though this is essentially a push monad. This seems a bit weird and is only sometimes what we want. It is possible for the source to not initiate the transfer on subscribe, and wait until it receives a separate event. As discussed above, this destination initiated transfer appears to what we want for databases, or close to it. However, with database queries, we need to pass request data in the push channel toward the database, and the Subscribe method can't do that. The only information it can take is timing information, that is 'when' to send the data. So this initiatating the data transfer on subscribe is not really useful for queries after all.   

It is a great disadvantage to combine the 'wiring' and the 'start of transfer' in the same Subscribe method call. In ALA we keep these two things separate because we want the code for these two things to be in two separate places. The wiring code represents a user story and so goes in a user story abstraction. The starting of a data transfer is an event that originates, for example, from inside a button domain abstraction. The two pieces of code should be in two separate abstractions. However, because this is a monad implementation example and not yet ALA, the Subscribe method will do both the wiring and initiating the data transfer. 

Another thing we will do, like we did for the deferred/push version of the Maybe monad, is compose Actions instead of Funcs. When an Action is called at runtime, it will be passed an object implementing IObserver. The action will use the IObserver to output directly instead of having a function that has to create an IObservable in order to do its output. 

This greatly simplifies the code in the Actions, which is what we want because these Actions are application code. If we composed Funcs that return IObservables, we would need to make every Func more complicated. Instead the Bind function will take on extra work. It needs to create an IObservable object to pass to the action. 

If you look at the SelectMany implemented in the reactive extensions for C#, you will see that it takes a Func. But there are two overloads. In one, the Func returns an IObservable object as expected. For the other, it returns an IEnumerable. I don't see how this simplifies the Funcs. It's a shame that the second overload isn't an Action that takes an IObserver. That would have truly simplified things. Anyway that's what we will do in our example here.

Here is our test application. We use the same function three times.

[source,C#]
....
static void Application()
{
Observable.Create<int>(observer => { observer.OnNext(0); observer.OnCompleted();  return Disposable.Empty; })
    .Bind<int,int>(MutiplyBy10AndAdd1Then2Then3)
    .Bind<int,int>(MutiplyBy10AndAdd1Then2Then3)
    .Bind<int,int>(MutiplyBy10AndAdd1Then2Then3)
    .Subscribe((x) => Console.Write($"{x} "),
                (ex) => Console.Write($"Exception {ex}"),
                () => Console.Write("Complete")
                );
}


static void MutiplyBy10AndAdd1Then2Then3(int x, IObserver<int> observer)
{
    observer.OnNext(x * 10 + 1);
    observer.OnNext(x * 10 + 2);
    observer.OnNext(x * 10 + 3);
    observer.OnCompleted();
}
....

As usual in C# (in this case the reactive extension library) provides us with a cheat way to implement Bind. The cheat way obscures the way Bind actually creates objects and wires them together, which is our purpose. However, for reference here is the cheat version.



[source,C#]
....
public static class MonadExtensionMethods
{

    public static IObservable<U> Bind<T, U>(this IObservable<T> source, Action<T, IObserver<U>> action)
    {
        return Observable.Create<U>(outputObserver=> <1>
        {
            source.Subscribe(x => 
            { <2>
                action(x, new CreateObserver<U>(
                    value => outputObserver.OnNext(value),
                    ex => outputObserver.OnError(ex),
                    () => { }
                ));
            },
            ex => outputObserver.OnError(ex), <2>
            () => outputObserver.OnCompleted() <2>
            );
            return Disposable.Empty;
        });
    }



    private class CreateObserver<U> : IObserver<U>
    {
        private Action<U> onNext;
        private Action onCompleted;
        private Action<Exception> onError;

        public CreateObserver(Action<U> onNext, Action<Exception> onError, Action onCompleted) { this.onNext = onNext; this.onCompleted = onCompleted; this.onError = onError; }

        void IObserver<U>.OnCompleted() => onCompleted();

        void IObserver<U>.OnError(Exception ex) => onError(ex);

        void IObserver<U>.OnNext(U value) => onNext(value);
    }
....

This code needs a little explanation.

<1> The Create method in the reactive extension library will create an IObservable object. When this object's IObservable.Subscribe method is called, the observer that is passed in is given to a function that you provide. The entire lambda expression starting with outputObserver=> is that function. Let's call it the work function even though its anonymous. So outputObserver is where the output is to go and you can see that the entire work function is using it for its output.

<2> Remember the diagram we had for the general monad pattern in Figure 3. There is an input IObservable. That input IObservable has been passed to us as source. The first thing that the work function does is subscribe to the source. That will wire the source to the set of three lambda expressions labelled with a 2. 
+ 
The OnError and OnCompleted lambda expressions simply pass straight through to the outputObserver. For the OnNext lambda expression, we give the value to the action. The action also needs an observer object to which it will give its output. You might think we should just do this: 

[source,C#]
....
x => action(x, outputObserver);
....

That would indeed correctly pass the multiple outputs of the action to the outputObserver. However, the action may call OnCompleted at the end of each of its subsequences. If it does we need to intercept it and remove it because otherwise it will terminate the outputObservable sequence prematurely. This removal of the OnCompleted from the function's output is effectively what 'flattens' the sets of outputs.

To remove the OnCompleted call, we pass to the action a locally created observer like this:

[source,C#]
....
action(x, new CreateObserver<U>(
    value => outputObserver.OnNext(value),
    ex => outputObserver.OnError(ex),
    () => { }
));
....

The CreateObserver class is making objects that implement IObserver where you want to provide the three methods in the IObserver interface. I can configure a CreateObserver object with three lambda expressions for OnNext, OnError and OnCompleted. (I have ignored dealing with the IDisposable, so this code is only for understanding.

This observer object is given to the action, and passes everything through to the outputObserver except the OnCompleted, which it intercepts and does nothing.

Just in case the version using Observable.Create is too confusing, here is the equivalent version without using Observable.Create. This will show more clearly that the Bind function works by instantiating an object that will do all the work at runtime, and wiring that object to the previous one. 

[source,C#]
....
public static IObservable<U> Bind<T, U>(this IObservable<T> source, Action<T, IObserver<U>> action)
{
    return new Observable<T, U>(source, action);
}
....

The class's constructor stores the source and the action. The Observable subscribe method, which is used by the next one in the chain, stores the output observer. It also does the subscribing to the source, for which it needs an observer object. Again the easiest way to do that is to have the class implement IObservable as well, and then calling Subscribe on the source just needs to pass this:


[source,C#]
....
private class Observable<T, U> : IObserver<T>, IObservable<U>
{
    private readonly IObservable<T> source;
    private readonly Action<T, IObserver<U>> action;
    
    public Observable(IObservable<T> source, Action<T, IObserver<U>> action) { this.source = source; this.action = action; } <1>


    IObserver<U> output;

    IDisposable IObservable<U>.Subscribe(IObserver<U> observer)
    {
        output = observer;
        source.Subscribe(this);
        return Disposable.Empty;
    }

    void IObserver<T>.OnCompleted()
    {
        output.OnCompleted();
    }

    void IObserver<T>.OnError(Exception ex)
    {
        output.OnError(ex);
    }

    void IObserver<T>.OnNext(T value)
    {
        action(value, new CreateObserver<U>(
            value => output.OnNext(value),
            ex => output.OnError(ex),
            () => { }
            )  // intercep OnComplete, we are combining the IObservables from the function
        );
    }
}
....

The OnNext method does all the work at runtime. It calls the action, and passes it a locally created observer which discards OnCompleted, but passes OnNext and OnError to the output.

If the source pushes through OnCompleted or OnError, they are passed directly to the output.

And that is the IObservable monad's Bind function.




===== Deferred monad Task or future

With this monad, we will be able to compose functions that return a Task or future object.

We did the imperative code that called the functions one after the other early in this section. We attached a continuation to each of the three future objects returned by each function. Each continuation required another level of nesting so it was impossibly awkward code to build chains of any length. This is a common scenario, so we really need this monad.

We did not do an immediate version because the functions can't return the results immediately. So this is the first time we will do this monad.

The application code uses the Bind function in the same way as any other monad:


[source,C#]
....
// monad composition of functions that return a future

Task<int> CombinedFunctions = 
    42.ToTask().Bind(function1).Bind(function2).Bind(function3);
....

The difference is that the interface that Bind takes and returns is Task<T>. The value has to be converted to a Task<T> first.

Once again there is a way of using the compiler to cheat to implement the Bind function:


[source,C#]
....
public static async Task<U> Bind<T, U>(this Task<T> source, Func<T, Task<U>> function)
{
    return await function(await source);
}
....

The async/await feature is indeed powerful, but our purpose is to see how Bind builds a structure of objects that represent the program that can be executed later. So here is the version that does not use async/await.


[source,C#]
....
public static Task<U> Bind<T, U>(this Task<T> source, Func<T, Task<U>> function)
{
    var tcs = new TaskCompletionSource<U>();
    source.ContinueWith(
        (t) =>
        {
            function(t.Result).ContinueWith(
                (t) =>
                {
                    tcs.SetResult(t.Result);
                },
                TaskScheduler.FromCurrentSynchronizationContext()
            );
        },
        TaskScheduler.FromCurrentSynchronizationContext()
    );
    return tcs.Task;
}
....

The Bind function uses two closure objects.

The Bind function starts with a Task<T> and immediately creates a new Task<U> to return. The Task<U> is contained in the TaskCompletionSource. A TaskCompletionSource allows you to put the result into the Task later. It saves the TaskCompletionSource<U> as tcs for use later.

The bind function sets up a continuation (callback) function to be called when the input Task<T> produces a result. At a future time this continuation function will run. When it does, it receives the value from the Task<T> and passes it to the function. The function immediately returns a Task<U>. The Task<U> will produce a result at an even later time. A second continuation function is attached to the Task<U>. This second continuation will be called when the Task<U> produces a result. It receives the value from the Task<U> and puts it into the tcs.

The TaskScheduler.FromCurrentSynchronizationContext() parameter to ContinueWith causes the whole thing to run on a single thread. That thread is never blocked, so can be the main UI thread. 

So that is the Task monad. It is inherently a push monad and inherently capable of asynchronous.



===== deferred monad mod 360

Finally, let's do a deferred version of the mod360 monad that we used as an example of refactoring out arbitrary code. Let's make it a push version.

Here is a suitable interface:

[source,C#]
....
interface IMod360Observer
{
    void Push(Tuple<int,int> value);
}
....

Item0 in the Tuple is the angle, and ITem2 in the tuple is the rotations.

And we will need a second interface for the Bind function to use:

[source,C#]
....
interface IMod360Observable
{
    void Subscribe(IMod360Observer observer);
}
....


Here is the application example code using the monad:

Application layer code
[source,C#]
....
var program = 42.ToMod360Monad();
program.Bind(function1).Bind(function2).Bind(function3);
....


Here is the Bind function:

Monad layer code
[source,C#]
....
namespace Monad.MaybeDeferredPush
{
    public static class ExtensionMethods
    {
        public static IMod360Observable ToMod360Monad(this int value)
        {
            return new Mod360Start(value);
        }

        public static IMod360Observable Bind(this IMod360Observable source, Func<int,int> function)
        {
            var mod360 = new Mod360(function);
            source.Subscribe(mod360);
            return mod360;           
        }
    }





    class Mod360 : IMod360Observer, IMod360Observable
    {
        private Func<int,int> function;

        public Mod360(Func<int,int> function) { this.function = function; }


        private List<IMod360Observer> subscribers = new List<IMod360Observer>();

        void IMod360Observable.Subscribe(IMod360Observer observer)
        {
            subscribers.Add(observer);
        }


        void IMod360Observer.Push(Tuple<int,int> value)
        {
            int functonResult = function(value.Item1);
            Tuple<int,int> result = new Tuple<int,int> (
                    functionResult mod 360,   // normalize the angle
                    value.Item2 + functonResult/360) // count rotations
                );
            foreach (var subscriber in outer.subscribers)
            {
                subscriber.Push(result);
            }
                    
        }
    }




    class Mod360Start : IMod360Observable
    {
        private int value;
        public Mod360Start(int value) { this.value = value; }

        private List<IMod360Observer> subscribers = new List<IMod360Observer>();
        void IMod360Observabe<T>.Subscribe(IMod360Observer<T> subscriber)
        {
            subscribers.Add(subscriber);
        }

        public void Run()
        {
            foreach (var subscriber in subscribers)
            {
                subscriber.Push(new Tuple<int,int> {value,0});
            }
        }
    }
}
....



===== State monad

TBD

===== IO monad

TBD 


===== summary of deferred monads 

You can see that the application code examples that use the deferred versions of Bind still look much the same as for the immediate versions. 

We prefer to have the deferred versions of Bind because then we have the option of executing it straight away as if it was immediate, or to keep the composite expression for executing later. 

You can see that for each example of imperative code we had at the beginning of this section, the version using Bind was syntactically very different. It was simply composing functions to expressing the user story. 

It refactored common code out of the application code. That common code included the code that controlled the execution flow, leaving the application code as a pure dataflow. 

It made it reasonable to build longer chains of functions. This is especially true in the cases where we had indenting in the imperative version, which is invaluable to avoid triangle hell.

We can see that in every case Bind was just creating an object to do the work at runtime, and then wiring that object to the previous one. This brings us to ALA, because in ALA we are also write application code that simply creates objects and wires them together.



==== ALA compared to monads

In chapter three, I compared ALA and monads. 

The points were:

. ALA composes objects and monads compose functions.

. Deferred monads can be used as part of the code that describes an entire user story, fully separating user story code from implemention code.

. Deferred push interfaces can do synchronous or asynchronous.




===== Monads are analogous to ALA's dataflow programming paradigms

ALA programming paradigms, which are usually interfaces, are analogous to monad interfaces. ALA programming paradigm interfaces can do anything that monad interfaces can do, such as IMaybe, IEnumerable, or Task or futures. 

The Bind function is somewhat analogous to the WireTo function, however most of the time the of monads is analogous to the WireTo function of ALA. The Bind function is different for every different monad type. The WireTo function is usually the same for every programming paradigm, however sometimes it is different, e.g. when an intermediary object is required. 

The top layer of ALA which composes domain abstractions is pure functional code. In this respect ALA and monads achieve the same job by putting the dirty work inside pre-written classes or monads. 



==== Versatility of composing with objects

Composing with objects is more versatile than composing with monads. Monad functions effectively have two ports, one for input and one for output. Some monad functions such as Join can have two input ports.
b
In ALA, the objects can have an aritrary number of input and output ports, and they can use entirely different programming paradigms. For example,  For example it is common to have a single domain abstraction with a UI port (to be displayed in the UI) multiple event driven ports (for mouse clicks) and a dataflow port (for binding to a data source).

Composing pure functions is essentially only a dataflow programming paradigm (although there are different variants of dataflow as there are in ALA as well). ALA dataflow ports could have more than one type of data passing through. For example, it is common to have dataflow ports that send tabular data to be able to send the header information for the columns of the tabular data.


==== using monads and ALA together

In chapter three, we showed some code that used monads to configure a domain abstraction called EnumerableQuery.





TBD review from here

==== Immediate and deferred execution monads

Monads are either immediate (eager) or deferred (lazy). Of the example monads given above, IEnumerable, Lazy and Future are all of the deferred type, whereas Maybe and List are immediate types. 

Composing immediate monads means the bind function actually gets the value from the first monad and then passes the unwrapped value to the function. Composing deferred monads means returning a new monad that represents the binding together of the two. It is actually an object structure of connected monads and closure objects. The bind function never actually gets values or executes lambda functions itself.
 
ALA programming paradigms are always deferred execution. Both deferred monads and ALA programming paradigms are used to compose a structure which you then run after the wiring up is completed. They both have two phases, the wiring up phase and the run-time execution phase. However, in ALA, we tend to wire up the entire application first and then set it running. Deferred monads are often wired up and then executed all in the same statement, for example by ending a sequence of IEnumerable monads with a ToList().   


==== Composing with plain objects 

As we said, the major difference between ALA and monads is that ALA composes using plain objects rather than functions. The only slightly unusual thing about these objects is the use of the term _ports_ for their input/outputs. But the term _port_ is readily understood. They are just a field, or an implemented interface, provided (most importantly) the interface is an abstract interface, which means it doesn't belong to the object, but is abstract enough to be used by many objects.

By using plain objects the barrier to understanding seems lower than for monads, at least for developers already familiar with objects. Functional programming, and monads in particular, seem to have quite a high barrier to entry unless you are a mathematician. The world needs programmers who are able to get objects but don't get math. I'm not sure what would happen if all universities only taught functional programming so that everyone is introduced to pure functions first. Perhaps then it would be objects which have a barrier to entry.

The concept of monads seems difficult to grasp because it is usually described in abstract terms, either in terse mathematical looking language or with some type of diagrams using boxed values to represent monads. I don't believe the idea of a box or a container is a good metaphor for a monad. It may work for list and maybe, but it doesn't work well for IEnumerable, Lazy or Future. A much better way is to think of the monad as an interface which gives you a way get the value(s) not a container for the values. (Of course to be a monad, in addition to the interface, we also need the composing operator for composing functions that take a value and produce the interface, and a unit function that turns a simple value into an object implementing the interface.) 

Imperative/object language programmers often learn new concepts by learning how the underlying code works. Often concepts are explained in terms of syntactic sugar for some equivalent code that the programmer already understands. For example, a lambda expression can be explained as equivalent to a delegate pointing at a normal named function.

The reason why monads are umost often explained in abstract terms is that the equivalent code is even more complicated than the concept of monads itself. This is especially true of monads that have deferred execution. For example, in C#, composing a series of monad functions results in a network structure of monad objects, Delegate objects and closure objects. I did an exercise where I used reflection to write out the structure from a simple monadic expression like Wrap(1),Add(2).Divide(3) which is using the lazy monad. In addition to three monad objects, it has four Delegate objects and four Closure objects. Each monad object connects to a Delegate object, which connects to a closure object, which connects to both another monad object and another Delegate object, which connects to another closure object.

TBD show diagram of a monad expression structure

The network can be different for different monads. For example an expression using IEnumerable ends up with a different network of a monad (that does the iterating), and Delegate objects and closures objects. You couldn't use this approach to explain monads, although it helped me a lot going through the exercise.

 By the way, a Delegate object contains a pointer to another object and a MethodInfo referencing a method in that object. Thus a Delegate object is just a reference to a method of an object. A closure object contains a single method, which is the lambda expression it was created from, together with some fields for the captured variables, which are variables used by the lambda expression from its containing scope. The compiler generates classes for these objects automatically.
 
The equivalent network for an ALA structure is just the three instances of the domain abstractions, each with a field referencing the next.

In ALA you write new programming paradigms as a normal activity. In the functional world, you can certainly write new monad types, but it doesn't seem that easy.
 
==== State



TBD 

==== WireTo vs bind

. ALA uses the WireTo function whereas monads use the _bind_, _flatmap_, SelectMany, ==> function to do composition.

Monad version:

[source,C#]
....
Source.Bind(x==0 ? : new nothing() else new something(1/x)); 
....


ALA version:


[source,C#]
....
Source.WireTo(new Expression(x==0 ? : new nothing() else new something(1/x)));
....


More often in monad programming, we use a more specialized composing function whose name represents the type of abstraction being wired, such as Filter or Select. These composing functions either create the necessary monad function and call bind, or they do the equivalent.

Similarly in ALA, we usually have domain abstractions that are a little more specialized:


Monad version:

[source,C#]
....
Source.Reciprocal(); 
....


ALA version:


[source,C#]
....
Source.WireTo(new Reciprocal();
....


The Monad version is more succinct syntactically. For the ALA version, we could write a function called Reciprocal(), which instantiates a Reciprocal object and then passes it to WireTo. That would make the syntax identical to the monad version. However we don't usually choose to do that. The reasons we don't are:

. It would create a lot of very small functions that do very little. I prefer to just use the longer WireTo(new Abstraction()) syntax, which keeps it clearer what is going on.

. In ALA, the first object being wired to may have multiple output ports of the same programming paradigm. WireTo has the ability to have overloads which name the port to be wired. Overloads such as this that specialize the behaviours of WireTo would need to be repeated for all the domain abstraction specific composing functions.

. When graphical tools are used for the wiring, the WireTo code will be generated automatically, and it doesn't matter what it looks like.


==== using monads within ALA

Although composing with objects can do everything that monads can do, that doesn't means we want to reinvent an object version of every monad abstraction. If monad operations such as Filter, Sort, GroupBy, Join etc are already available, we want to be able to use them. Besides, monad composition is normally done with text because the composition is normally a linear sequence of operations. ALA composition is done with a diagram because when you express a whole user story, not just the dataflow operations, you tend to get a graph structure, not just a linear structure. 

So it makes sense to create a domain abstraction in which you can put a monad sequence of operations as a parameter in text form. 

TBD write such an abstraction for IObservable monads. Call it RX. Let it take a Linq RX expression as its configuration. Let it have one input and two output ports all of type DataflowPushSynchronous. Actually do an async version as well. Demonstrate by connecting it to an input source such as an array of data, and connect the two outputs to the console output. Have eit do various things such as Sort, Filter, Select, Split.
For the Async version, introduce a delay operator to slow them down. 






==== with monads



However there are other differenes:

. Non-mathematicians find it hard to learn and use functional concepts, and even the syntax of functional languages. It is difficult for them to understand monads, never mind write them. On the other hand, these same programmers can readily understand the simpler concepts of imperative programming, and happily string together 100000 lines of it to create a system (even though that system is then unmaintainable).
+
By using plain objects, ALA is an easier concept to grasp, and writing domain abstractions is easier even than conventional code. The hard part is conceiving the abstractions and programming paradigms, which a senior ALA team member needs to do, then the abstractions can be readily delegated to junior programmers.

. I have only seen monads using dataflow programming paradigms. ALA domain abstractions can use multiple programming paradigms, so that composition can mean anything you want. A given abstraction can have ports of different programming paradigms,   
+
I'm not sure how far monads could go in this direction. Monads rely on the bind function returning the dataflow interface e.g. IEnumerable, ready for the next bind function in the chain to use. That implies there can be only one output port. The WireTo method returns the first object, so the next WireTo can wire any of the remaining ports of the same object. The WireIn operator returns the second object, so now WireTo or WireIn can wire any of its other ports.

I find that keeping the _WireTo_ and _new_ when instantiating abstractions demystifies how the whole thing works. Monads use methods defined on e.g. IEnumerable, that do both the _new_ and the _bind_. This is better once you are used to it, because you can write e.g. 

Queries tend to be a linear chain structure, or sometimes a structure with two sources combining into one stream. If monads are available to do what you want, make a domain abstraction called query, and when you instantiate a query you pass to it a query written using your available monads.


instead of monads. Although ALA uses objects, it is not object oriented as its primary design focus. It uses objects for the following four reasons.

. Objects store references to other objects to which they are wired via their ports. These references can be considered immutable. This is similar to the way lazily evaluated monads work. Behind the scenes effective objects (probably closures) are created which are wired together.

. Domain abstractions, being reusable entities, often need configuring. For example, a filter abstraction needs configuring with a cutoff frequency and a stop band rejection. The configuration data is immutable. If an abstraction consisted only of functions, then that configuration data would need to be passed in every time a function is used. That would be awkward. It would also mix the dataflow parameters of the functions with the configuration parameters, breaking the Interface Segregation principle. By using objects, ALA can configure an abstraction once, and then the contained function can be used may times. This separation of configuration and function use is important for abstractions - the configuration is done at instantiation time of the abstraction, whereas the function can be used many times once it is running. Monads can do the same thing.

. Some abstractions need state to work. Most of the time, objects do not hold state data in ALA. Instead a sequence of dataflow objects is set up once, and when needed the data moves directly from the source to the destination. However, sometimes an abstraction by its very nature needs to hold some state. For example, a running average abstracton needs to hold past values. In pure functional programming, the array of past values becomes a global in a 'top layer' and is passed into the function every time. In ALA, we don't want to break the abstraction by splitting the averaging code from its data. For cases like these, we become 'object oriented' and store the data in the object. Monads can also do this.
+
In a multithreaded environments, such a running average abstraction would need a separate instance for each thread. 








===  Monads (old, needs review)

We have talked about monads a few times because they are an important example of composition of instances of abstractions. Also, like ALA, they can use the concept of separating (in time) composition from execution. You can bind functions to monads making new monads, and it builds a structure that you can then execute. ALA is like a generalisation of monads. In the same way, you can wire instances of domain abstractions together, and it builds a structure that you can later execute.

When you execute a monad structure (generally by calling a function on the last monad you binded), it (usually) terminates. It is only executes again if it is wired up again. An exception is when using hot observables, such as an IO monad. The monad structure stays in existence, and it executes whenever there is input or output. ALA is more similar to this second case. When you start execution of a wired ALA structure, it usually starts running continuously.

Monads are restricted to generic types that have a single type of value. Each monad binding is restricted to stay in this generic type, and only the type of the internal data can change. The functions that are binded must take one parameter and have one return value. Dataflows in one direction. 

ALA wiring is more arbitrary in its meaning. It can be anything depending on the relationships that need to be expressed in the requirements. A single wired connection can carry data as needed in both directions, or the composition may be about something completely different from dataflow.  

Often when monads are used, the execution is done immediately following the binding. So the deferred nature of the execution is not always obvious.  I found that the separation between composition and execution of monads to be an important aspect to understand when comparing with ALA composition. In ALA all composition takes place at a clear wiring phase time. Then comes run-time. 

For queries with monads you run code to compose (bind) IObservable or Task monads etc each time for a query. In ALA you would tend to compose (wire up) data streams once at the start and then initate them to re-run when needed with a wired event.

Another difference is syntax. Monads are composed using a dot operator, a method call, and configured with lambda function passed to the method:

[source,C#]
....
source.Filter(x=>x>=0).Select(x=>sqrt(x))
....

This code filters out values from the source that are negative and then calculates the squareroots. In ALA, because composition is generalised, the syntax would look like this:

[source,C#]
....
source.WireIn(new Filter(x=>x>=0)).WireIn(new Select(x=>sqrt(x))
....

But usually this code is generated from a diagram.

In functional programming, the code that composes monads is pure functional code without side effects. The functions that are binded are pure functions. When the composed structure 'executes', that may not be pure functional code. It may change states, perform I/O or write to a database. But all that code is inside the well tested monads. Al the code you wrote to compose those monads was purely functional.  

ALA makes use of this same property of reusable abstractions. Domain abstraction may not internally be purely functional. Being object oriented they are almost certainly not. But they can be tested to ensure they are correct. The code that uses them to build specific applications can be pure functional.  


==== Monad syntax

Sometimes in ALA, we will wire together domain abstractions that operate on dataflows. Domain abstractions that perform the same function as Select, Where, and OrderBy are possible. This is in the same problem space that monads also solve. So it is worth comparing the two systems. I am not an expert on monads, but here is my current understanding of how ALA wiring and monad binding compare with one another.

When we want to bind a function to a monad such as Nullable<T>, it's contained value can be immediately unwrapped, operated on, and then wrapped in a new monad Nullable<R>. But when we want to bind a function to a monad such as IEnumerable<T>, we may not want to pull all the values out immediately. Instead we just want a new IEnumerable<R> that when enumerated wil get values from the first IEnumerable<T> and perform the function on them. Similarly, when we want to bind a function to a monad such as Task<T>, we can't immediately get its content because the value wont be there until some time in the future. Instead we want a new Task<R> that waits for the first task and then applies the function. In the second two examples, the binding to a monad is like wiring in ALA. Successive binding creates a connected chain of objects that collectively know how to do the workflow. Same in ALA, the wiring code creates a connected chain of objects which know how to process data through them later. 

We can try to compare the monad pattern's _bind_ with ALA's _WireIn_. 

Let's assume for the ALA case, that the instanceB being wired converts objects from one type to another, the same as a function binded to monad does. So in both cases, we have a source of TAs and we want wire in a function that will convert them to TBs. 

Both bind and WireIn have an object as their first argument. That object is the source for TAs. Both bind and WireIn can be written using the dot operator style:

.Monad wiring code
[source,C#]
....
objectA.bind(...)
....


.ALA wiring code
[source,C#]
....
instanceA.WireIn(...) 
....


_bind_ and _WireIn_ are different in their second argument. _bind_ requires a function and WireIn requires an object. The function takes a TA and returns an MTB (a TB wrapped in a monad container). The object has an input port of type TA and an output port of type TB.

.Monad wiring code
[source,C#]
....
monadA.bind((a)=>(func<TA,Monad<TB>>)
....


.ALA wiring code
[source,C#]
....
instanceA.WireIn(instanceB) 
....


In the monad case, the bind function returns a new monad object. 
In the ALA case, the WireIn function returns instanceB.
Therefore, in both cases you can now chain additional operators using fluent style:

.Monad wiring code
[source,C#]
....
monadA.bind(func<TA,Monad<TB>>).bind(...)
....


.ALA wiring code
[source,C#]
....
instanceA.WireIn(instanceB).WireIn(...)
....

In the monad version, we often want to specify the function to return a TB instead of a Monad containing a TB. That is what Select is for in C#. Select uses bind under the covers but does the wrapping of the TB into a monad for you:

.Monad wiring code
[source,C#]
....
monadA.Select(func<TA,TB>)
....

In the ALA case, we will usually use a prexisting domain abstraction to perform the operation. For example, we might use the domain abstraction OffsetAndScale. This allows code to be generally be inside domain abstractions layer, and only configuration constants (that come directly from requirements) to be in the application layer. But to get closer to the same problem that monads solve, let's assume we have no domain abstraction that does what we need, and we really do want to specify the mapping function in the application layer right in amongst the wiring. In other words we want a domain abstraction that is configured with a lambda function. In this case we can invent a domain abstraction called Lambda which takes a lambda function when it is constructed: 

.ALA wiring code
[source,C#]
....
instanceA.WireIn(new Lambda<TA,TB>(funct<TA,TB>))
....


Just as _Select_ is a more specialized version of bind that changes the type, _Where_ is also a more specialized version that removes records from the stream. It requires a predicate function that returns a bool:

.Monad wiring code
[source,C#]
....
monadA.Where(funct<TA,bool>)
....

.ALA wiring code
[source,C#]
....
instanceA.WireIn(new Where<TA>(funct<TA,bool>))
....


You can see that the ALA syntax for solving this particular problems is now more verbose. It requires the additional use of WireIn and the _new_ keyword. The tradeoff for the extra words is versatility. We could consider using the less verbose Monad syntax for all ALA wiring. What would we lose if we did that:

For example:

.ALA wiring code
[source,C#]
....
    adc.WireIn(new LowPassFilter(10)).WireIn(new OffsetAndScale(0,0.5));
....


.consider monad style ALA wiring code
[source,C#]
....
    adc.LowPassFilter(10).OffsetAndScale(0,0.5);
....

To accomplish this syntax, we would have to provide methods with the same names as the domain abstractions. These methods would perform the new operation and then the wiring operation.

We would briefly consider defining these methods directly on the domain abstractions such as ADC, but that would pollute ADC with knowledge of LowPassFilter. Since there are many ways of wiring things, every abstraction would need methods for every other abstraction to which it could be wired. That would be ridiculous. 

Instead we might make every domain abstraction implement an _IWireable_ interface. I think this inerface would be empty. Then all the wiring methods would be extension methods on _IWireable_. They would all return an _IWireable_ ready for fluently calling the next wiring method. Now the code for ALA would look like:

[source,C#]
....
    (adc as IWireable).LowPassFilter(10).OffsetAndScale(0,0.5);
....

which is pretty much the same as the Monad code.


The methods would be fairly simple:

[source,C#]
....
static class LowPassFilterExtensonMethod
{
    static IWireable LowPassFilter(this IWireable instanceA, int strength)
    {
        return instanceA.WireIn(new LowPassFilter(strength));
    }
}    
....


Note that IWireable is kind of analogous to IEnumerable in the monad examples we have been looking at. We give it the more abstract name _IWireable_ because domain abstractions can have more than one output port, and we could be wiring any one of them, whereas monads generally only have one output such as IEnumerable.

In ALA we use the explicit WireIn and new operators for the following reasons.

* In ALA, domain abstractions are written much more frequently than new monads are written. They are extremely simple to write. The only difference from plain classes is that you have to know that input ports must use implemented interfaces from the programming paradigms layer, and output ports must be plain private fields of the types of these same interfaces in the programming paradigms layer. We don't really want the extra burden of adding a corresponding extension method in a separate static class, and adding the IWireable interface.

* In ALA we can choose between WireIn and WireTo depending on whether we want to chain instances of abstractions or do fanout wiring.

* Monads tend to be used only for _amplified types_ and for composing functions that use them that have one parameter and one return value. You are composing functions. In ALA you are composing instances of abstractions, which is more general. Also composing monad functions is primarily using a dataflow programming paradigm. In ALA different programming paradigms can be used for what it means when composing instances of abstractions, so again it is more general. Also in ALA, you can mix different programming paradigms in the same solution, so that requirements can be fully expressed.

* Monad functions look like operations on data, but hide the fact they really just instantiate objects and wire them together for the operation to occur later. Although confusing at first, it is readable once you are used to it. But imagine if it were used for all programming paradigms. Take for example the UI layout programming paradigm. The code below puts a TextBox and Grid inside a window.

.ALA wiring code
[source,C#]
....
    window.WireTo(new TextBox())
          .WireTo(new Grid());
....

The equivalent monad style code would be confusing:


.Monad style wiring code
[source,C#]
....
    window.TextBox();
    window.Grid();
....



* ALA wiring handles arbitrarily complex objects with multiple input and output ports. Using explicit WireTo and WireIn operators directly reflects this mental model. Monads are at first quite difficult to understand because of what happens under the covers. For most people it is quite a learning curve. By using explicit _WireTo_ and _new_, ALA code is fairly obvious.

* Domain abstractions can have multiple ports. WireIn allows us to specify which ports we want to wire when it could be ambiguous.

* Inherent in the requirements of a typical application is really a network of relationships, which we can often represent with a diagram. Explicit WireIn and WireTo operators allow us to more easily see the one-to-one correspondence between the diagram and the code. Also, it is very easy for a diagramming tool to automatically generate wiring code containing .WireTo and new.

* Monads based on IEnumerable or IObservable can handle a finite sequence. The IEnumerator interface tells you when it is finished. ALA dataflow interfaces may or may not be written to handle a finite sequences. If they do handle finite sequences, such as for a database query, they are not unwired and rewired every time they are used, They stay permanently wired up, and are reset for each query.

 
==== Understanding monads


Monads are notoriously hard to learn, but they are nice simple insight once you get there. Monads actually seem to have this property that you cannot understand any explanation of them until you first understand them. Thus it is a bootstrapping problem. Here is my experience of going through that bootstrapping process in case it is useful. I am not going to try to explain monads myself, because, even it was possible, others would do that far better than I would. 

. First understand that Monads are like physics. Physicists explain that you never really understand physics, you just get used it. Unless you are a mathematician or otherwise gifted, the same is true for monads. 

. The way to get used to new concepts is to read multiple web-sites on the topic. Read each one until you get lost then swap to another one. Keep going like this. For average concepts like design patterns I use this technique and it requires maybe five websites. For monads it took me maybe ten. You will need to return to some of them iteratively to get further each time.

. If you don't know Haskell, prefer the web sites that explain them in the language you already know.

. The common essential ideas in those websites will start to embed themselves in your brain.

. Eventually, and fairly suddenly, the simple insight that is monads will happen.

I thought few of the web-sites that I used adequately emphasised the monad property of separation (in time) of composition and execution. They did use examples of it such as IEnumerable and Task. They represent what they can do in the future, without actually doing it now. That's why the binding functions are called bind in the functional world, because it doesn't (necessarily) do anything except build a structure that can later be executed to actually do the work. 




=== Encapsulation, polymorphism and inheritance

ALA replaces encapsulation with abstraction.

ALA removes associations and inheritance and instead uses composition (provided the composition uses a more abstract abstraction).

ALA replaces polymorphism with zero coupling.

The first two we know as fundamental principles in ALA, and have already been discussed in chapter three.

The third statement requires some elaboration.

In the meme pool of software engineering we have at least five memes for the one concept. These are polymorphism, information hiding, protected variations, dependency inversion principle and open closed principle. 

I shall argue in their individual discussion later that none of them is a principle.
All five are just a simple pattern. The motivation is that if you have code that couples knowledge of different 'things', you extract the knowledge into their own modules. Now when the 'thing' changes, you can change it or swap it out without affecting the client module. Switch statements were a smell in traditional code that different things were mixed.

You may already have separated out one implementation of a thing. So now your client code talks to a concrete thing. The conical example is a particular database. But now you need to use a different thing. Instead of putting in a switch statement everywhere to talk to different databases, you use the polymorphism / information hiding / protected variations / dependency inversion / open closed pattern. 

The pattern itself consists of an interface. That's it. All those memes all trying to tell you to use an interface. Oh, and another one - if you have heard the phrase "program to interfaces".

On top of that, single responsibility also pretty much forces the use of an interface. Referring to a peer concrete object is always a second resposibility.

ALA does not use this pattern.

To understand why, lets call the client module B and the modules that implement the interface, C1, C2 etc. B doesn't know which of the C modules it is talking to at run-time. If we want it to be C2 for a particular application, we have higher level code that injects C2 into B.   

It's important that we realize that in this pattern the interface is owned by B. It describes what B _requires_. It is cohesive with B. It is part of abstraction B. This still the case even if the interface is split out into a module or even a different compilation unit of its own. 

Therefore C1, C2 etc have a dependency on B. They implement B's requirements. They collaborate with it. The dependency in the design is just inverted from what it might have been. C1 & C2 are coupled with B. 

So this is illegal in ALA (assuming B and C1, C2 etc are all at a similar level of abstraction, which they likely are. That's why for ALA I have stated that the equivalent is zero coupling. ALA replaces the dependency with nothing at all between A and C1, C2 etc.

We have talked about how ALA still works in Chapters three and four. It does still use an interface but it is not owned by B (or C1 or C2). It is at a much more abstract level, the level of a programming paradigm. For example if abstractions B, C1 and C2 know about the event-driven programming paradigm, then instances of them may be wired together.

ALA further requires that the higher level code that does the injecting is also an abstraction. It is just one that is specific to a user story. Let's call it A. A needs to cohesively do all the wirings of all the instances of domain abstractions to implement a whole user story in a cohesive way.

These five memes don't have anything to say about that. They are redundant with respect to ALA. By just using ALA the job is done in a better way.

The SRP, DIP amd OCP are discussed further in the sections below.


=== SOLID Principles

The SOLID principles collated by Robert Martin are confusing. Their one or two sentence descriptions don't describe them very well, so you have to go a read a lot to understand them. Unfortunately they are collected up into the catchy acrostic "SOLID" with a meaning that is undeserved. This has made the collection more well known than it deserves, as we shall explain.  


====  Single Responsibility Principle

The SRP strangly worded differently from it's name. It states that a module (function, class or package) should have only one reason to change. I find this s strange formulation of the name.


By using abstractions, the SRP is complied with in terms of reasons to change. However, some abstractions arguably have more than responsibility. I often use the question "What do you know about?" to an abstraction. It is always one thing it knows about, but it may have multiple responsibilites for that thing.

Examples:

* An ADC driver (analog to digital converter hardware) knows all about a particular ADC chip. It has the responsibilies of initializing it and getting the readings from it. It changes only if the HW chip changes.

* A protocol abstraction knows about a protocol. It has the responsibility to send data using the protocol and to receive it. It changes only if the protocol changes.

* A file format abstraction, such as CSVFileReaderWriter knows about a file format. It has the responsibility to both read it and write it. It changes if the file format changes.

My advice is that the SRP is made redundant by thinking in terms of abstractions, which accomplishes the intention of the SRP better. 


====  Open Closed Principle

Talk about confusing. Firstly Betrand Meyer coins the phrase, which is impossible to understand without further reading. On further reading you find that Robert Martin has a completely different principle by exactly the same name. Then he has two verions of that, one for modules in the same compilation unit and one for when the client is in a different compilation unit and is already published. By the way, being already published was also the context of Meyers OCP.

None of them are principles - they would need to be used in the right conext at best. They have associated patterns anyway (or anti-patterns relative to ALA).

===== Martins version

The sources of knowledge about the meanings of these memes are:

Craig Larman
Kevlin Henny





==== Liskov Substitution Principle

TBD

====  Interface segregation principle

TBD

==== Dependency Inversion Principle

The DIP is stated:

A.   High-level modules should not import anything from low-level modules. Both should depend on abstractions (e.g., interfaces).

B.   Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions.

This sounds the same as the ALA fundamental rule that all dependencies must be on abstractions that are more abstract. 

The Dependency Inversion Principle, and its associated pattern goes some way toward ALA in one respect and far too far in another respect.

Firstly ALA uses the word abstraction for the unit of code. The DIP really only uses the word abstraction as a synonym for interface – e.g. abstract class. The essence of the difference is that when ALA allows a dependency on an abstraction, it means more abstract than what DIP does. In both cases an interface is introduced. But in DIP, that interface is owned by the first module, and expresses what that module requires, so it’s highly coupled with the module, not really more abstract than it. ALA’s interfaces don’t belong to domain abstractions but go all by themselves in a lower layer. They are so much more abstract that we call them programming paradigms.

To be more precise, the DIP (as its name suggests) reverses a dependency used for communication between two classes, but ALA completely removes it. But the ALA wiring pattern also adds other dependencies. It adds a dependency on each module from a higher layer for dependency injection and it adds dependencies from each module to a programming paradigm interface in a lower layer for ports.

Let’s start with conventional code where B talks to C. It uses a dependency:

B ----> C

DIP does this:

B < --- C

ALA does this:

B ---- > I

C ---- > I

Those who know the DIP might immediately say “no the DIP has a version where the interface is put into its own separate package like that as well”. The DIP allows for the interface to be placed in a different compilation package than B. Lets call it IB. Theoretically this allows C (the implementer of IB) to be reused without B. However, this is a superficial change from the point of view of abstraction level. Simply moving IB doesn't make it more abstract. That interface is still owned by B - it represents what B requires. So as it still just a part of the B abstraction.

With DIP, you get to choose a specific implementation, C, to satisfy what B requires. In ALA you get a port with a programming paradigm that will take any domain abstraction instance with a compatible port of the same programming paradigm. 

Both DIP and ALA require dependency injection. So let’s draw the injection dependencies as well:

Conventional code version:

B ----> C

DIP version:

A ---> B

A ---> C

C <--- B

ALA version

A ----> B

A ----> C

B ---- > I

C ---- > I

DIP effectively moves the interface from C to B. B gains an interface that does a similar job to C. C then implements it and B uses it.  

Because the new interface is owned by B, it may be different from the one in C because now it’s about what B requires rather than what C provides.

Because of this, it might often be an adapter that implements the interface, and then the adapter uses the original interface of C.

TBD

Think of B as being some business logic and C being the database. B no longer depends directly on a specific database. But the databases do now depend on B. To avoid changing the databases, you would use adapters. The pattern is designed to increase the reuse potential of B, the business logic, because different databases can be plugged into it. But it likely decreases the reuse potential of the things around the business logic unless adapters are used. The DIPs application is primarily around making business logic reusable, and leads to hexagonal architecture, which has the business logic in the middle, and all the peripherals are plugged into its interfaces.

 

 

Returning to the sentence in the DIP that states: “High-level modules should not import anything from low-level modules.”.

 

The 2nd  ALA dependency rule is in a way less constraining than the DIP here. If a low-level module is much more abstract, ALA allows to keep the dependency. This is what allows the dependencies between the application user stories and the domain abstractions. It comes down to what is meant by high-level and low-level in Martin’s writings. I think by ‘low-level’ he refers to what would have been depended on in conventional code. Things like the database, middleware for communications, and frameworks.(e.g. for supporting asynchronous events.)

 

In ALA, yes you would wire the specific database adapter and the specific middleware adapter (and the specific UI), but you wouldn’t wire in the framework. It doesn’t matter that the abstraction depended on is low level. I want to commit to only one implementation of the framework. It would be silly to have to use ports on every single domain abstraction so I can wire in a framework of my choice, and have to wire it to every single domain abstraction, when I want to commit to using one. This becomes more obvious as you get to even lower levels such as math libraries. I don’t need to allow for swapping out the math library implementation. So ALA allows dependencies on more abstract abstraction even if they are low-level modules. In fairness, Martin probably doesn’t mean to include all low-level modules in the DIP, just certain ones that should be decoupled.


===  Dependency injection pattern

By now we know that ALA uses dependency injection. It uses it for wiring up all instances or all domain abstractions.

We have favoured using reflection to do the injection in our examples, but that is just a syntactic shortcut that allows domain abstractions to have many ports without also having many setters. It also allowed us to keep the ports private from direct access by the application layer. It allows ports to be implemented very simply, without the need for setters at all. It allows some other interesting things to be done. For example, after an instances port has been wired, there may events in the interface of the port that need internally wiring to event handler methods. The wireTo method can look for and call a method in the instance to do this immediately after wiring.

ALA always uses explicit wiring. This is one of the most important aspects of ALA. It's usually in the form of a diagram, because the wiring is usually an arbitrary graph. ALA never does dependency with automatic wiring. Having a dependency injection container means that the wiring itself is implicit in the interface types. If one module requires an interface, and the container has a module that implements it, that means these two modules get wired together. This type of implicit wiring is indirect and obfuscated and illegal in ALA. 

In ALA, abstraction pairs don't have their own interfaces for their instances to communicate. So we don't have the situation where class A has a dependency on class B, and so an object of class B (or one of its subclasses) is injected into class A. Similarly, we wouldn't have the situation where class A requires an interface that is implemented by class B.

In ALA the interfaces must be programming paradigm interfaces, which are a whole abstraction layer more abstract. So we need to be thinking that if class A accepts or implements a certain programming paradigm interface, there could be any number of other abstraction instances that could be wired to it. Furthermore, we could build arbitrarily large compositions. Some abstractions will have some ports that don't need to be wired to anything. So it doesn't really make sense to call what we are injecting 'dependencies'. We just think of it as wiring things up. You wouldn't describe what an electronics engineer does as dependency injecting components into each other.

In ALA, the explicit wiring should not be XML or JSON. I do not consider these readable programming languages. They are data languages. 

Usually user stories contain a graph structure of relationships. So the wiring should be a diagram to best show that structure. 

However, if the graph is mostly a tree structure (with relatively few cross connections), then it may still make sense to avoid the weight of a diagramming tool, and represent the wiring in text form. But in this case I still much prefer the readability of code written in a programming language than XML or JSON. An argument can be made for the declarative nature of say XAML and that UI designers could learn this declarative language more easily than a programming language. But I would maintain that a the subset of the programming language needed to the equivalent of XML is declarative style. That's what most of the wiring examples in this website are: declarative composition.

Besides, its not just about UI. For a given user story there will likely be UI, business logic, data transformations, and data storage. These should all be expressed togther cohesively. They should all be composed inside one abstraction. To handle the sometimes non-trivial configuration of the abstraction instances, normal code is sometimes needed, for example for lambda expressions or delegates. If we have a UI designer on the team, great, just teach him the subset of domain abstractions that are used for the UI, how to configure them, and how to compose them. Languages like XAML are not particularly easy just because they are declarative.





===  Physical boundaries

I was listening to a talk by Eric Evans where he said that Microservices works because it provides boundaries that are harder to cross. We have been trying to build logical boundaries for 60 years, he said, and failed. So now we use tools like Docker that force us to use say REST style interfaces in oder to have physical boundaries. I have also heard it suggested that using multiple MCUs in an embedded system is a good thing because it provides physical boundaries for our software components. And I think, really? Is that the only way we can be create a logical boundary? I can tell you that multiple MCUs for this reason is not a good idea if only because all those MCUs will need updating, and the mechanisms and infrastructure needed to do that make it not worth it. Unless there is a good reason, such as to make different parts of your code independently deployable, the extra infrastructure required for physical boundaries that are just logical boundaries is not necessary. Furthermore, physical boundaries, like modules do not necessarily make good abstractions. The only boundary that works at design-time is a good abstraction. So ALA achieves it's design-time boundaries by using abstractions.

===  Test Driven Development

It is said that TDD's main advantage is not so much the testing, but the improvement in the design. In other words, making modules independently testable makes better abstractions. This is probably true, but in my experience, TDD doesn't create good abstractions nearly as well as pursuing that goal directly. The architecture resulting from TDD is better but still not great.


===  Observer pattern

TBD




===  Layer patterns

==== MVC

TBD

==== Application, Services, Drivers, Hardware

TBD

===  Factory method pattern

The Factory Method pattern in both the GOF book and in online examples has multiple variations. The only thing they seem to have in common is that the client doesn't use "new ConcreteProduct()". It just wants an object that implements an interface, IProduct. For any reason it doesn't want to be the one who will decides at design-time what that concrete product will be. 

Here are some of the variations. 

* Several ConcreteCreators exists to encapsulate knowledge of how to use the ConcreteProduct constructor which has many parameters, in a consistent way to make a valid ConcreteProduct. The common example is different named pizzas or sandwiches. 

* The Client finds out at run-time what ConcreteProduct is needed (usually a string name). We want to move the switch statement out of the client and into a Creator class.)

* The client knows when the objects are needed, but needs to be more stable. Which product is needed changes more often (although still known at design-time). So it goes into a class that changes. 

In all cases we end up with two objects wired together through the IProduct interface. These two objects we will refer to as the Client and the ConcreteProduct (from the pattern terminology). To get them wired using the Factory Method pattern requires the use of a FactoryMethod. The FactoryMethod typically goes in an abstract class called ICreator, which may do the creating itself, or maybe overridden by one or more ConcreteCreators.

In the context of abstraction layers, ALA gives more insight into the FactoryMethods pattern. Remeber we expect lower layers to more stable. The IProduct and ICreator interfaces are in the ProgrammingParadigms layer (lowest layer). The Client and all the different ConcreteProducts are in the DomainAbstractions layer (middle layer). The ConcreteCreator is in the Application layer and wires one of the ConcreteProducts to the client. So now when we want to change the ConcreteProduct, only the ConcreteCreator in the application layer has to change.

But in ALA we typically accomplish that in a far simpler way. We commonly let the application code instantiate the right concrete class (that implements the interface, IProduct), and wire it to the Client object using the WireTo() method. This is nothing more than static wiring, but can only work when the required ConcreteProduct is known at design-time.


==== case 1

Now to the case in ALA where we have a client that needs a concrete product creating later than design-time, that is at run-time. Such a client is the Multiple Abstraction. It's job is to make many instances of a Domain Abstraction. But it is an abstraction so can be used to make instances of any object. They don't even have to implement a specific interface such as IProduct, because Multiple doesn't interact with these instances itself.

==== case 2

Let's say you have a Table domain abstraction that stores a table of data. In your application, you want to instantiate many Tables. Now lets suppose that we want these Table instances to persist their data. A database must be attached via an IPersistance interface. We don't want the Table class to know about concrete Databases. We want the application layer at the top to do that. But we don't want the application layer to have to wire the database to every instance that requires an IPersistance. We want the Application to be able to just use a Table as if it is a self-contained abstraction. We want the Table instances to take care of themselves for Persistence. So we make a Peristence abstraction in the Programming paradigms layer. The concept of Persistence is at the right abstraction level to go in this layer. The Table class can use this persistence abstraction through a FactoryMethod. A variable in the Persistence abstraction stores the IPeristence object. The application instantiates which database it wants to use and passes it to the Peristence abstraction.


=== Decorator pattern

TBD

===  Bridge pattern 

TBD


===  Architecture styles

I am not an expert at these so called 'Architectural styles'. Any feedback about the accuracy of the following comparisons would be appreciated.


==== Components and connectors

David Garlan and Mary Shaw in their paper titled "An Introduction to Software Architecture" 1994 use components and connectors as a framework for viewing architectural styles. Depending on the style, the connectors can be a procedure call, event broadcast, database query, or pipe (which we call dataflow).

*Similarities*

ALA follows this idea closely. 


*Differences*

In ALA we call the styles programming paradigms, and it is emphasised that multiple programming paradigms can be used in the one user story. The reason not to call them 'styles' in ALA is that the word style tends to imply using a single style throughout the program.

In ALA 'components' becomes 'abstractions' and 'connectors' becomes 'ports and wirings'. This change in terminology is to emphasis that the wiring is distinct from the abstractions themselves. The term components and connectors can (albeit not necessarily)) refer to an effectively monolithic system that is just separated into pieces and the pieces connected back together in a fixed rigid arrangement. This is especially true if the design methodology is decomposition of the system into elements and their relations. Such a system is loosely coupled at best. In ALA you can't do that. Systems must be composed of instances of abstractions wired together by a higher layer abstraction that directs the wiring. Abstractions are necessarily zero-coupled with one another. They use ports that have the types of a small number of programming paradigms so that instances of them can be composed in (generally) an infinite variety of ways. The style where components being filters and connectors being pipes works this way. 

I suspect that most components and connector systems use interfaces that are specific to the components. 

Examples using the UML component diagram, even though it uses the term ports, show interfaces that rigidly couple their components to one another, for example, interfaces with names such as CustomerLookup. This would mean that only components that are implementations of that specific interface could be substituted. Usually there appears to be only one, making the components effectively just modules. In UML, components appear to be just containers. They are the first level of decomposition of a system, and themselves just contain connected classes. This type of architecture is incompatible with ALA.   



==== Component Based Software Engineering

// TBD, some of this may be repeated

ALA uses many of the same methods found in component based engineering or the Components and Connector architectural style.


===== Similarities

* Components are Abstractions.

* Reusable software artefacts.

* Connection ports for I/O.

* Composability

* Both instantiate components, specialize them by configuration, and compose them together to make a specific system.

* ALA's 3rd layer has interfaces used to wire abstractions in the 2nd layer, so at a lower level (more abstract) level. They represent something more like programming paradigms. The equivalent pattern in components engineering is "Abstract Interactions".  

* The architecture itself is composed of a generic part and a specific part. The general part is the ALA reference architecture itself and the components or the connectors architectural style. The specific part is the wiring diagram of the full system.

===== Differences

* Component based engineering technologies such as CORBA primarily solve for platform and language interoperability in distributed system whereas ALA brings some of the resulting concepts and properties to everyday small-scale, non distributed development as well, where the only separation is logical.

* In ALA there is perhaps more particular emphasis on making components clearly more abstract than the systems they are used in, and making the interfaces clearly more abstract than the components. The components are pushed down a layer and the interfaces down to a layer below that. Then all dependencies must be strictly downwards in these layers. In component based engineering, this structure is not necessarily enforced. If the components are just a decomposition of the system, then the system, components and interfaces may all be at the same level of abstraction, making the system as a whole complex.

* ALA depends on the 'abstractness property' of components to get logical separation, and so calls them 'Abstractions' and not components to help them retain that property. Even if there will only be one use and one instance, it is still called an abstraction. This keeps them zero coupled and not collaborating with other abstractions they will be wired to.

* ALA layers are knowledge dependency layers.  Components may still be arranged in layers according to run-time dependencies, such as communication stacks. In ALA run-time dependencies are always implemented as explicit wiring inside another higher layer component.

* ALA's top layer must be a straight representation of the requirements, whereas components may tend to be decomposed pieces of the system.

* ALA's 2nd layer of components are designed for expressiveness of user stories or requirements, and provide DSL-like properties. ALA puts emphasis on the 2nd layer of components having the scope of a domain as the means of explicitly controlling the expressiveness of the pallet of components.

* ALA is not fractal. In ALA the components of components are abstractions that become more abstract and thus ubiquitous and reusable. ALA therefore uses abstraction layers rather than hierarchies.

* ALA forces decisions about which abstraction layers the software artefacts go into, and then controls knowledge (semantic) dependencies accordingly.

* ALA tries to make the abstraction layers discrete and separated by a good margin. 

* ALA puts greater emphasis on wiring being able to represent any programming paradigm that suits the expression of requirements, and the use of many different paradigms in the same wiring diagram.

* ALA emphasises the cohesion of functional parts of a system such as UI, logic and Data, by bringing them all together in one small diagram using domain level components

* Instead of 'required' interfaces, in ALA they are called 'accepts' interfaces. This is because the abstractions are more abstract and composable, so, as with Lego blocks, there isn't necessarily a connection to another instance.





==== Presentation, Business, Services, Persistence, Database

TBD

==== Presentation, Application, Domain, Infrastructure

The middle two layers appear to be the same as ALA's. The Presentation (UI) only has run-time dependencies on the Application, and the Domain layer only has run-time dependencies on the Infrastructure (Persistence etc), so these layers are not present in ALA. 

Instead Presentation is done in the same way as the rest of the application, by composing and configuring abstractions in the domain. The meaning of composition for UI elements (typically layout and navigation-flow) is different from the meaning of composition in the use-cases (typically workflow or dataflow).

In ALA, the foundation layer is also done in the same way as the rest of the application, at least a little. Domain abstractions that represent say a persistent table are in the Domain layer. The composition and configuration of them again goes in the Application layer. This time the meaning of composition is, for example, columns for the tables and schema relations.  

If the implementation of any domain abstraction is not small (as is the case with the persistent Table abstraction mentioned above, which will need to be connected to a real database), it will be using other abstract interfaces (in the Programming Paradigms layer) connected to its runtime support abstractions in a technical domain, the same as in Hexagonal Architecture.

==== Object Oriented Programming

From my reading, it seems that the most characteristic feature of OOP is that when data and operations are cohesive, they are brought together in an object. Others may see it as enabling reuse, inheritance, and still others may see it as polymorphism. New graduates seem to be introduced to polymorphism in inheritance and not be introduced to interfaces at all, which is a shame because the concept of interfaces is much more important. 

I have never been an expert at Object Oriented Design as I found the choice of classes difficult and the resulting designs only mediocre. But I think the most fundamental and important characterising feature of OOP is under-rated. That is the separation of the concepts of classes and objects. This separation is not so clearly marked when we use the terms modules or components. The separation is fundamentally important because it's what allows us to remove all dependencies except knowledge dependencies. In the way described earlier in this article, you can represent the knowledge of most dependencies as a relationship between instances completely inside another abstraction. What OOP should have done is represent relationships between objects completely inside another class. The problem is that OOP doesn't take advantage of this opportunity. Instead, it puts these relationships between objects inside those objects' classes, as associations or inheritance, thereby turning them into design-time dependencies, and destroying the abstract qualities of the classes. Abstractions, unlike classes, retain their zero coupling with one another.

ALA addresses the problem by calling classes abstractions and objects instances. Abstractions differ from classes by giving us a way to have logical zero coupling, as if they were on different physical platforms. Instances differ from objects by having ports because their classes give them no fixed relationships with other objects.

Of course, when you are writing ALA code, abstractions are implemented using classes, but you are not allowed associations or inheritance. Instances are implemented as objects but with ports for their connections. A port is a pair of interfaces that allow methods in both directions. The interfaces are defined in a lower layer.
 
In ALA, the UML class diagram completely loses relevance. Because classes have no relationships with each other, bar knowledge dependencies, a UML diagram in ALA would just be a lot of boxes in free space, like a pallet of things you can use. You could show them in their layers and you could even draw the downward composition relationships that represent the knowledge dependencies, but there would be no point to this except in explaining the concepts of ALA. When you are designing an actual system, the real diagram is the one inside of an abstraction, especially the uppermost one, the application. It shows boxes for instances of the abstractions it uses, with the name of the abstraction in the box, the configuration information for those instances, and of course the lines showing how they are wired together. The names inside the boxes would not even need to be underlined as in UML, because the boxes in such diagrams would always be instances. 

Such a diagram is close to a UML object diagram. However, a UML object diagram is meant to be a snapshot of a dynamic system at one point in time. In ALA, any dynamic behaviour is captured in a static way by inventing a new abstraction to describe that dynamic behaviour. Thus the design-time view is always static. So the object diagram is static. The application class specifies a number of objects that must be instantiated, configured, and wired together to execute at run-time. Since the structure is always static, ideally this would be done by the compiler for best efficiency, but there is no such language yet. So, in the meantime, it is done at initialization time. The object diagram can be fairly elegantly turned into code using the fluent coding style shown in the XR5000 example.

===  DSLs

We briefly discussed ALA as a DSL in the structure chapter <<DSL1, here>> 

ALA includes the main idea of DSLs in that the fundamental method "represent[s] requirements by composition of domain abstractions". It shares the DSL property that you can implement a lot more requirements or user stories in a lot less code. 

But ALA only tries to be a light-weight way of telling ordinary developers how to organise code written in your underlying language. Although the domain abstractions do form a language and the paradigm interfaces give it a grammar, ALA doesn't pursue the idea of a language to the point of textural syntactic elegance. Instead, you end up with explicit wiring methods to combine domain entities, or plain old functional composition, or some other form of composition in the wider sense of the word. Often, the text form is only a result of hand translation of an executable diagram. ALA certainly doesn't overlap with DSLs to the extent of an external DSL, nor does it try to sandbox you from the underlying language. It therefore does not require any parsing and doesn't need a language workbench, things that may scare away 'plain old C' developers.

Like DSLs, ALA can be highly declarative depending on the paradigm interfaces being used to connect domain abstractions. It is better to have the properties of composition and composability in the your domain language even if they may not be in a perfectly elegant syntactic form. ALA may end up composing abstractions with calls to wireTo methods instead of spaces or dots. But often a diagram using lines is even better than spaces and dots.  

In DSLs, it is important that different languages can be combined for different aspects of a problem. For example, a DSL that defines State machines (the state diagram) and a DSL for data organisation (Entity Relationship Diagram) may be needed in the same application. You don't want to be stuck in one paradigm. ALA recognises this importance by having paradigm interfaces that are more abstract than the domain abstractions. 

DSLs probably work by generating a lot of code from templates whereas ALA works by reusing code as instances of abstractions. Both of these methods are fine from the point of view of keeping application specific knowledge in its place, and domain knowledge in its place. Howver, the distinction between ALAs domain layer and programming paradigms layer is probably not so as clearly made in the implementation of the templates.   

It is an advantage of DSLs that they can sandbox when needed. An example from the wiring pattern earlier is that the ports of instances do not need to be wired. Therefore, all abstractions need to check if there is something wired to a port before making a call on it. Enforcing this is a problem I have not yet addressed.

A possible solution, albeit inferior to a real DSL that would tell you at design-time, might be that when there are tools that generate wiring code from diagrams, they automatically put stubs on all unwired ports. These stubs either throw an exception at run-time, or just behave inertly. 

ALA is different from external DSLs. ALA is just about helping programmers organise their code in a better way. It doesn't try to make a syntactically elegant language, as a DSL does. Certainly an external DSL will end up representing requirements in a more elegant syntax. But that is not the most important thing in ALA. The most important thing is the separation of code that has knowledge of the requirements, which will cause the invention of abstractions that have zero coupling (because the coupling was really in each requirement - that is why a requirement is cohesive). ALA also avoids taking the average imperative language programmer out of their comfort zone. It does not require a language workbench and does not sandbox you from the underlying language.

ALA probably does fit into the broadest definition of an internal DSL. However, again, it does not target syntactic convenience in the expression of requirements so much as just separating the code that knows about those requirements from the code that implements them. An internal DSL usually aims to have a mini-language that is a subset of the host language, or it tries to extend the host language through clever meta-programming to look as if it has new features. ALA is about abstraction layering. It is about this design-time view of knowledge dependencies: what abstractions in lower layers are needed to understand a given piece of code.







===  Multi-tier Architecture

TBD


===  Clean Architecture

Clean architecture is initially viewed as concentric circles which are in effect layers. Entities are innermost, with business logic next, and the external system consisting of things like database, UI and communications on the outer. These layers are allowed to have dependencies going inwards. 

In conventional code, dependencies tend to follow communications, and communications, when implemented in the form of direct function or method calls, flow from the initiator of the communications.  

This gives rise, for example, to dependencies from the UI to the business logic, and then from the business logic to the database. In clean architecture, these are referred to as primary and secondary I/O with respect to the business logic. The idea in clean architecture is to invert the secondary dependencies so that all communications dependencies are now toward the business logic.

In this way the business logic at the core is reusable, and perhaps more importantly understandable without knowing details of a concrete database, middleware, or UI. It also facilitates easier testing of the business logic.

The business logic uses interfaces to communicate with the outside world. The primary communications have interfaces that the business logic _implements_ (unchanged from conventional code). The secondary communications have interfaces which the business logic _requires_. The concrete implementations of database, etc are passed in or injected in. This wiring is specific to a unique application, so in ALA terms, it goes in the top layer.

From the point of view of the business logic only, this is compliant with ALA, except for the dependencies on entities, which is discussed below. The elements of the business logic, which in clean architecture are called use cases, can be considered abstractions that know about the business use cases and nothing else.


==== Adapters

In the clean architecture, dependencies, such as those between business logic and database, are reversed (following the dependency inversion principle) from what it would have been in conventional code. These reversed dependencies do not comply with ALA. I think most implementations recognise these as bad dependencies, and solve it by removing the dependencies altogether using adapters. This is now a lot closer to ALA compliance. 

Something must pass-in or inject the adapters into each of the business logic use cases. If this logic is thought of as being in a higher layer, then this is also ALA compliant.

In terms of ALA abstraction layers, the use cases, the database, the UI, and other IO are all about the same level of abstraction. They all know about different types of details. While the use cases know about the domain and it's requirements, the database knows about how to efficiently store data. They are all abstractions that are zero coupled with one another. The adapters go in a layer above, and are specific to a use case / external IO pairing. The main() (or a function it delegates) goes in a layer above that and wires everything up using (usually) constructor dependency injection on the use cases.

==== Entities

Clean architecture allows dependencies of use cases on entities. This is incompatible with ALA.  

Entities typically hold all sorts of domain details, for example various informations about customers. When the requirements change, these will change. We expect requirements to change - that's why we have agile.

Entities are an easy place to just add all fields to do with an identity. They will tend to hold some fields that, although they associate with an identify, really belong to separate use cases. These fields should be cohesive with their use cases. If entities hold information that is not significantly abstract with respect to use cases, such as the customer's address, which is primarily used by one or two use cases only, then it is not ALA compliant. The customer identity abstraction's responsibility should not be to know all data that can be associated with a customer, but to know about the idea of identity. It should not be used as the carrier of information between two use cases, which would expose all entity data to all use cases. Instead, use cases should all know about the abstraction, _customer identity_. A particular use case should only know about it's own data, and only store it against a customer identity.


In other words, a user story should be able to have private data that is associated with an identity and still ultimately stored with all other data for that identity in the database. The only idea that is abstract enough to go in a layer below the use cases is the customer identity, which is likely to be reused by most new use cases. Subclassing, so that every use case has its own subclass may solve the problem in one way, but I expect would cause other problems.

Even if some customer detail needs to be shared with another use case, communicating this via a shared entity is bad. For example, consider a use case in a system that knows about the address that customers enter into the system. It could have an output port called 'address' that can be used to wire it to other use cases. This port will probably have a DTO type that belongs to it. The DTO cannot be shared with other features in the same layer without violating ALA constraints. A feature such as frieghtcost may need an address to calculate freight. Remember it is written separately from the address feature so is not coupled with it.  It cannot know about the address feature. It can't know the DTO of the address. Nor does it need the entire address. So it may be written, for example, to have input ports for country and zip code. Yet another feature is shipping. It needs an address for a shipping label. It may have an input port that takes a string for of address, because it isn't interested in the content of the address, only in faithfully printing it. So these three ports are incompatible. The wiring layer, which knows that it needs to wire these three together also knows how to adapt them, which can be done quite simply by passing in a lambda expression into the WireTo method (analogous to a Select clause in LINQ).

More generally in ALA, such applications are best viewed primarily in terms of dataflows rather than abstracted entities. Dataflows to/or from the database, for example. It flows to particular use cases, and only the data that is needed by the use case. At any point in the flow, the flow has a type. It is still nice to have a compiler generated, anonymous, fully type checked class at each point in the flow. But nowhere do we want to create an explicit class for sharing a whole entity, or even a part of an entity.

The identity of a customer itself is probably an abstract concept that can be used by all features. We therefore want a shared abstraction for the identity (just knowing about a unique internal or external number or key). 

It should be possible to add a feature that needs a new private field (private to the feature). The data can still be associated with an identity and be stored in the database. Adding this field should cause a database migration, but not changes to other use cases. 

So the way entities should be handled is quite different in ALA.

TBD do a simple 'task list' application on Github in both ALA and clean architecture to show how entities are handled in ALA. Then add a feature such as e-mail notification on due date to show how a new feature can have it's own private data stored against the task identity (the e-mail sent status) and communicate via a port with an existing feature (the due date feature).



==== Primary separation

There is a second major difference between clean architecture and ALA. In clean architecture, the UI and other externals IO such as the database are considered to be separated first. That is how it is shown on an architecture diagram, almost as if they are separate packages. You hear of being able to switch between a GUI or CLI based UI. 

This view of primarily separating UI from business logic will likely lead to coupling. It is unlikely that the UI is so generic that it knows nothing about the business logic. It will need to specific to the data the business logic needs or produces. Similarly, the design of the UI will usually influence the way the business logic works. For example, the UI may be designed so that you enter all data first (like a form) and then submit, or it may be designed so that you select generally what you want to do, and then wizards guide the user through. The choice is likely to affect the way the business logic works.

In ALA, the primary separation is by features first. The UI and the business logic for a particular feature is considered to be cohesive with respect to that feature abstraction. The use case will wire up both the elements of the business logic and the elements of the UI (and those for the necessary database queries, etc). The UI elements used can still be swapped out for different ones, but that is an operation on the feature. 

In the case that the UI design is not changing, but its implementation is, that involves swapping out the implementations of the UI domain abstractions. The abstraction themselves do not change, so the use cases wont change. But the new UI abstractions can shift to a different technology, shift from desktop to cloud, or the like. 


==== DTOs

DTOs have two different uses.

- part of an interface to group together related data that is sent through the interface at one time. 
- to collect data together to be transported together to cut down on the overhead of messaging.

===== interface DTOs

In ALA, DTOs are not generally abstractions in themselves. Therefore, they may not be put in a lower layer and shared by two abstractions to communicate. That would couple the knowledge inside the two abstractions. If many abstractions want to know about the same DTO, this is likely to be the case as new abstractions are added, then maybe it is sufficiently abstract to be in a lower layer and shared. 

Otherwise in ALA, you need to use adapters. This can be as simple as a lambda expression passed to the WireTo operator, in the same way that you would pass a lambda expression to a .Select clause in LINQ.

Although this is ALA compliant, in ALA we generally prefer not to use adapters. Instead we use interfaces that are a significantly more abstract that are not owned by the business logic core. These are of course at the abstraction level of programming paradigms. These types of interfaces are heavily reused, allow composability in the wiring, and help tremendously to keep all abstractions from being implicitly coupled.

If a DTO can be avoided by, for example, having two dataflow ports that use primitive types, this will increase the abstraction level, reusability and composability of your abstractions. 

===== transport DTOs

In ALA you wouldn't use DTO for transport purposes. Instead, invent an abstraction say called multiplexer_demultiplexer for packing/unpacking (or serializing/deserializing) multiple input or output ports. Then instances of any two abstractions A and B, that would normally be compatible for wiring together, and which use asynchronous communications, can be physically deployed to opposite sides of the transport system. The wireTo operator, knowing they are in different physical locations, defers to a version that wire each of them to the respective multiplexer_demultiplexer instances.

==== Stability of wiring/adapter/feature layers

A system built from a wiring layer at the top, then an adapters layer below that, and then a layer below that for independent features, use cases, databases, UIs etc is ALA compliant. This is because the abstraction layers are more abstract as you go down. The top layer abstraction is a specific application. The second layer adapters are specific to pairs of things in the third. The third is the layer of fully reusable things. A database, even though we call it concrete, is a lot more reusable than a particular application, or a particular adapter.

An ALA application using these three types of layers is a little different from the layers we normally talk about, which uses domain abstractions that are wired directly together using compatible ports instead of via adapters in the layer above. To enable the ports to be compatible, there must be a layer below that provides abstract interfaces, which is what we call the programming paradigms. This latter arrangement has compositionality. For example, two domain abstractions currently wired together can have another domain abstraction, which is a decorator such as a filter, wired between them.

The two styles of layering can be used together.

==== Swapping out technology

In clean architecture, part of the reason for avoiding dependencies from business logic to things like a particular database or framework is to allow swapping out the technology. The database in the third layer can be exchanged for a completely different type - the coninical example is changing it from a relational database to a simple file. The business logic does not change. Only new adapters are needed, one for each use case. The top layer wiring of course also needs to change to use the different adapters.

An ALA application that uses the preferred layering scheme of application layer, domain abstractions layer, programming paradigms layer can also have its technologies swapped out. Let's again use the canonical example of swapping a relational database for a simple file. The domain abstraction that implements persistence using a database will have a port that implements a suitable programming paradigm. Usually this port has a type like ITableDataflow. You only need to substitute this domain abstraction with one that uses the same programming paradigm, but implements it as a simple file. Effectively these domain abstractions are wrappers, not adapters. 

The wiring again needs to change in all the places that were instantiating the database implementation. This is probably the only practical way to do it, as the database implementation probably needs different application specific configuration than what a simple file implementation would.

Now let's consider swapping out the UI. Let's say we are changing the UI from a desktop windowed application to a browser, or from a PC window to a CLI (Command Line Interface).

In the original PC application, the wiring instantiates UI GUI domain abstractions. These domain abstractions are wrappers for, say, WPF UI elements. The wrappers have ports which the wiring uses to connect them to the corresponding parts of the business logic. These ports are, or course, abstract interfaces from the programming paradigms layer.

To swap out the UI involves changing the wiring to instantiate from a different set of these UI domain abstractions. They will have the same ports that are still wired to their relevant place in the business logic as before.

In the case of the browser, these new domain abstraction work by changing elements of the HTML that will be returned by an initiating HTTP request. Just as the windowed domain abstractions were wired to their containing window, browser domain abstractions will be wired to their containing page. The containing page will request their content when it is time to send the response to the HTTP request.

The case of the CLI is more interesting. Whenever there is a case of either a GUI or a CLI user interface in conventional architecture, the business logic is tied to the CLI commands, and the GUI then uses the CLI. But in ALA we have the option to do this without coupling the design of the business logic to the design of the CLI commands.  

This is how it could work. Imagine we have previously built the application as a desktop windowed application, just as we did before. Now we change the wiring to use a set of CLI domain abstractions instead. Actually we need only two abstractions, one called command and one called response. Instances of the command abstraction are configured with the command that they handle. The command has an output event port which fires when the command is entered. If there are parameters, the abstraction can have other output ports for them, which are output before the event port fires. Alternatively you could chain up a series of parameter abstractions, each with a single output port. The response abstraction has an input port, and just prints any input data it receive. Optionally it could have a configuration name so it can identity itself when it prints.

Just as there are containing domain abstractions that describe layout for the GUI types of UI domain abstractions, CLI domain abstractions would also connect to a common domain abstraction that receives commands in a general form and passes them to the handler that is configured for that command. It would also collate the responses, add newlines to the output, etc.  

There is one other possibility. In the above cases of swapping out the UI, we changed the names of UI domain abstraction instantiated by the wiring. That was potentially all we needed to change.

It is possible that the configuration of the domain abstractions did not need to change. For example, CLI command abstractions need to be configured with the actual command string they will respond to, whereas their GUI equivalents, which are buttons, need to be configured with a button name. These could potentially be the same. If other configuration information of UI domain abstractions, such as style, is implemented in a generic way such as having a style port wired using WireMany, then it is possible that the wiring only needs to specify the UI domain abstraction names. 

In this case we could name all equivalent UI domain abstraction with the same name. Then by which set of classes we include in the project, it will be built for different technologies. I'm not really proposing it be done this way, just exploring the idea.




===  Onion Architecture

TBD



===  Hexagonal Architecture (Ports and Adapters)

ALA includes the basic idea of hexagonal architecture, but with modification using the Bridge Pattern to keep cohesive knowledge belonging to the application from being split. 

In a previous section we intimated that the sideways chains of interfaces going out in horizontal directions were the same as hexagonal architecture. While ALA shares this aspect of hexagonal architecture, there is still an important difference.

ALA retains domain abstractions of the UI, Database, communication and so on. For instance, in our XR5000 example, we had a domain abstraction for a persistent Table. We had domain abstractions for UI elements such as Page, Softkey etc. We don't just have a port to the persistence adapter, we have an abstraction of persistence. We don't just have a port for the UI to bind to, we have abstractions of the UI elements. The implementation of these abstractions will then use ports to connect to these external system components. Why is it important that we have domain abstractions of these external components?

. The Database and the UI will have a lot of application specific knowledge given them as configuration. Remember the creativity cycle. After instantiation of an abstraction comes configuration. The database will need a schema, and the knowledge for that schema is in the application. The Softkey UI elements will need labels, and that knowledge is in the application. By making domain abstractions for persistence and UI, the application can configure them like any other domain abstraction as it instantiates and wires up the application. To the application, these particular domain abstractions look like wrappers of the actual database and UI implementations, but they are more like proxies in that they just pass on the work. 
+
The Persistence abstraction then passes this configuration information, via the port interface to the actual database. The Softkey abstraction then passes its label, via the port interface, to the softkeys. Otherwise the Application would have to know about actual databases and actual softkeys.
+
If you need a design where the UI can change, you just make the UI domain abstractions more abstract. A softkey may be a command abstraction. It is still configured with a label. But it may be connected to a softkey, a menu item, a CLI command, a web page button, or a Web API command.

. From the point of view of a DSL, it makes sense to have concepts of UI and persistence and communications in the DSL language. The application is cohesive knowledge of requirements. The UI and the need for persistence are part of the requirements. In fact, for product owners communicating requirements, the UI tends to be their view of requirements. They talk about them in terms of the UI. Many of the product owners I have worked with actually design the UI as part of the requirements (with the backing of their managers, who are easily convinced that software engineers can't design UIs. PO can't either, but that is another story.). The point here is that the UI layout, navigation, and connection to business logic is all highly cohesive. We explicitly do not want to separate that knowledge. 
+
As a restatement of an earlier tenet of ALA, it is much better to compose the application with abstractions of Business logic, UI and persistence than to decompose the application into UI, persistence and business logic.

. We want the application to have the property of composability. We have previously discussed how that means using programming paradigm interfaces for wiring up domain abstractions. By using domain abstractions to represent external components, the abstractions can implement the paradigm interfaces and then be composable with other domain abstractions. For example, the Table domain abstraction which represents persistence may need to be connected directly to a grid, or to other domain abstractions that map or reduce it. Indeed, the Table abstraction itself can be instantiated multiple times for different tables and be composed to form a schema using a schema programming paradigm interface. I have even had a table instance's configuration interface wired to a another Table instance. (So its columns can be configured by the user of the application.)     

. The fourth reason why it is important for the application to not directly have ports for external components of the system is that we don't want the logical view of the architecture to become just one part of the physical view. If there is a communications port that goes to a different physical machine where there is more application logic, the application's logical view should not know about that. It may be presented as an annotation on the application (lines) connecting certain instances, but it shouldn't split the application up. At the application level, the collaboration between parts instantiated on different machines is still cohesive knowledge and belongs inside one place - the application.  

=== Domain Driven Design

Domain Driven Design's "Bounded Contexts" and ALA's Domain Abstractions layer have the same goal, that of encapsulation of the domain specific knowledge.

Domain driven design appears to concentrate on common languages to allow  pairs of elements to communicate, which ALA explicitly avoids. ALA tries to abstract the languages so that they are more abstract and fundamental than the domain, and more like programming paradigms.

// TBD Discuss with a DDD expert the comparison between ALA and DDD.




===  Microservices

TBD




===  Architecture evaluation methods

Methods such as ATAM tell us how to evaluate an architecture for quality attributes such as maintainability, for instance by giving it modification scenarios to test how difficult the modifications would be to implement. There are several scenarios based methods to do this such as ATAM. Using this we could, theoretically, iteratively search over the entire architecture design space to find a satisfactory solution. It's a bit analogous to numerically solving for the maxima of a complex algebraic formula. In contrast, ALA is analogous to an 'algebraic solution'. If the desired quality attributes, and all the software engineering topics listed above are the equations, ALA is the algebraic solution. It simplifies them down into a parameterised template architecture, ready for you to go ahead and express your requirements.


anchor:Monads[]


===  Reactive Extensions

TBD 




===  WPF & XAML

TBD

===  Functional programming

TBD

===  Functional programming with monads

TBD

===  Functional Reactive Programming

TBD

===  Example project - Game scoreboard

For the example project for this chapter, we return to the ten-pin bowling and tennis scoring engines that we used in Chapter two, and add a scoreboard feature (well a simple ASCII scoreboard in a console application rather than real hardware).

As the requirement, say we want a console application that displays ASCII scoreboards that look like these examples:

....
Ten-pin

 -----+-----+-----+-----+-----+-----+-----+-----+-----+--------
|   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |    10  |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
| 1| 4| 4| 5| 6| /| 5| /|  | X| -| 1| 7| /| 6| /|  | X| 2| /| 6|
+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+--+
|   5 |  14 |  29 |  49 |  60 |  61 |  77 |  97 | 117 |   133  |
 -----+-----+-----+-----+-----+-----+-----+-----+-----+--------
....

....
Tennis

 -----++----+----+----+----+----++--------
|   1 ||  4 |  6 |  5 |    |    ||    30  |
|   2 ||  6 |  4 |  7 |    |    ||  love  |
 -----++----+----+----+----+----++--------
....



As usual in ALA, our methodology begins with expressing those requirements directly, and inventing abstractions to do so. So, we invent a 'Scorecard' abstraction. It will take a configuration which is an ASCII template. Here are the ascii templates that would be used for ten-pin and tennis:

....
 -------+-------+-------+-------+-------+-------+-------+-------+-------+-----------
|   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |     10    |
+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
|F00|F01|F10|F11|F20|F21|F30|F31|F40|F41|F50|F51|F60|F61|F70|F71|F80|F81|F90|F91|F92|
+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+   +---+---+
|  T0-  |  T1-  |  T2-  |  T3-  |  T4-  |  T5-  |  T6-  |  T7-  |  T8-  |    T9-    |
 -------+-------+-------+-------+-------+-------+-------+-------+-------+-------------
....

....
 -----++----+----+----+----+----++--------
| M0  ||S00 |S10 |S20 |S30 |S40 || G0---  |
| M1  ||S01 |S11 |S21 |S31 |S41 || G1---  |
 -----++----+----+----+----+----++--------
....

The scorecard ASCII template has letter place-holders for the scores. (A single letter is used so it doesn't take up much space on the template design.) Different letters are used for different types of scores. Digits are used to specify where multiple scores of the same type are arranged on the scoreboard. They are like indexes. Either 1-dimensional or 2-dimensional indexes can be used in the scoreboard template. For example, the frame scores in ten-pin bowling have scores for each ball for each frame, F00, F01 etc, as shown in the example above.

The scorecard abstraction needs functions it can use to get the actual scores. The functions are configured into little 'binding' objects that we then wire to the scoreboard. The binding objects are configured with the letter that they return the score for. 

==== Ten-pin

Having invented the Scorecard and Binding abstractions, we can now do the ten-pin application diagram:
 

[plantuml,file="diagram-bowling-3.png"]
----
@startdot
digraph foo {
rankdir=LR

#note rankdir does not work inside subgraphs
subgraph cluster_C {
fontsize=20
label="Ten-Pin Bowling                                                            "
style=rounded

node [shape=Mrecord]
console [label="ConsoleGameRunner|\"Enter number of pins\""]

scoreboard [fontsize=14,label=<
<table border='0' cellborder='1' cellspacing='0'>
<tr><td colspan="21" sides="B"><font point-size="14">Scorecard</font></td></tr>
<tr><td colspan="2">1</td><td colspan="2">2</td><td colspan="2">3</td><td colspan="2">4</td><td colspan="2">5</td><td colspan="2">6</td><td colspan="2">7</td><td colspan="2">8</td><td colspan="2">9</td><td colspan="3">10</td></tr>
<tr><td sides="LTR">F00</td><td>F01</td><td sides="LTR">F10</td><td>F11</td><td sides="LTR">F20</td><td>F21</td><td sides="LTR">F30</td><td>F31</td><td sides="LTR">F40</td><td>F41</td><td sides="LTR">F50</td><td>F51</td><td sides="LTR">F60</td><td>F61</td><td sides="LTR">F70</td><td>F71</td><td sides="LTR">F80</td><td>F81</td><td sides="LTR">F90</td><td>F91</td><td>F92</td></tr>
<tr><td colspan="2" sides="LBR">T0</td><td colspan="2" sides="LBR">T1</td><td colspan="2" sides="LBR">T2</td><td colspan="2" sides="LBR">T3</td><td colspan="2" sides="LBR">T4</td><td colspan="2" sides="LBR">T5</td><td colspan="2" sides="LBR">T6</td><td colspan="2" sides="LBR">T7</td><td colspan="2" sides="LBR">T8</td><td colspan="3" sides="LBR">T9</td></tr>
</table>
>]

framebind [label="Binding|F"]
totalbind [label="Binding|T"]
game [label="Frame|\"game\"|nFrames==10"]

node [shape=record]
function1 [label="GetSubFrames()\n.Select(sf =\> sf.GetScore()[0])\n.Accumulate()"]
function2 [label="GetSubFrames()\n.Select(f =\> f.GetSubFrames()\n.Select(b =\> b.GetScore()[0])"]
translate [label="Translate\nX,/,- etc"]

console -> game  [label = "IConsistsOf"]
console -> scoreboard [constraint=false, label = "IPullDataFlow"]
scoreboard -> framebind -> translate -> function2 -> game
scoreboard -> totalbind -> function1 -> game

{rank=same console scoreboard}
{rank=same framebind totalbind}
{rank=same function1 function2}

}
}
@enddot
----

An abstraction we didn't mention yet is the ConsoleGameRunner. Its job is to prompt for a score from each play, display the ASCII scoreboard, and repeat until the game completes. 

The 'game' instance of the Frame abstraction on the right of the diagrams is the scoring engine we developed in Chapter Two. Together with this engine, we now have a complete application. 

The rounded boxes in the diagram are instances of domain abstractions as usual for ALA diagrams. The sharp corner boxes are instances of Application layer abstractions. They are the mentioned functions for the Bindings. That code is application specific so goes in the application layer. They just do a simple query on the scoring engine.

Now tranlate the diagram into code. Here is the entire application layer code for ten-pin:
....
consolerunner = new ConsoleGameRunner("Enter number pins:", (pins, engine) => engine.Ball(0, pins))
.WireTo(game)
.WireTo(new Scorecard(
"-------------------------------------------------------------------------------------\n" +
"|F00|F01|F10|F11|F20|F21|F30|F31|F40|F41|F50|F51|F60|F61|F70|F71|F80|F81|F90|F91|F92|\n" +
"|    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+    ---+----\n" +
"|  T0-  |  T1-  |  T2-  |  T3-  |  T4-  |  T5-  |  T6-  |  T7-  |  T8-  |    T9-    |\n" +
"-------------------------------------------------------------------------------------\n")
.WireTo(new ScoreBinding<List<List<string>>>("F", 
    () => TranslateFrameScores(
        game.GetSubFrames().Select(f => f.GetSubFrames().Select(b => b.GetScore()[0]).ToList()).ToList())))
.WireTo(new ScoreBinding<List<int>>("T", 
    () => game.GetSubFrames().Select(sf => sf.GetScore()[0]).Accumulate().ToList()))
);
....

....
....
If you compare this code with the diagram, you will see a pretty direct correspondence. 
Remember 'game' is the reference to the scoring engine project in the previous chapter.

That's pretty much all the code in the application. Oh there is the 'translate' function, but it is pretty straight forward once you know the way a ten-pin scorecard works. For completeness here it is.

....

/// <summary>
/// Translate a ten-pin frame score such as 0,10 to X, / and - e.g. "-","X".
/// </summary>
/// <example>
/// 7,2 -> "7","2"
/// 7,0 -> "7","-"
/// -,3 -> "-","7"
/// 7,3 -> "7","/" 
/// 10,0 -> "",X
/// 0,10 -> "-","/"
/// additional ninth frame translations:
/// 10,0 -> "X","-"
/// 7,3,2 -> "7","/","2"
/// 10,7,3 -> "X","7","/"
/// 0,10,10 -> "-","/","X"
/// 10,10,10 -> "X","X","X"
/// </example>
/// <param name="frames">
/// The parameter, frames, is a list of frames, each with a list of integers between 0 and 10 for the numbers of pins.
/// </param>
/// <returns>
/// return value will be exactly the same structure as the parameter but with strings instead of ints
/// </returns>
/// <remarks>
/// This function is an abstraction  (does not refer to local variables or have side effects)
/// </remarks>
private List<List<string>> TranslateFrameScores(List<List<int>> frames)
{ 
    // This function looks a bit daunting but actually it just methodically makes the above example tranlations of the frame pin scores 
    List<List<string>> rv = new List<List<string>>(); 
    int frameNumber = 0;
    foreach (List<int> frame in frames)
    {
        var frameScoring = new List<string>();
        if (frame.Count > 0)
        {
            // The first 9 frames position the X in the second box on a real scorecard - handle this case separately
            if (frameNumber<9 && frame[0] == 10)
            {
                frameScoring.Add("");
                frameScoring.Add("X");
            }
            else
            {
                int ballNumber = 0;
                foreach (int pins in frame)
                {
                    if (pins == 0)
                    {
                        frameScoring.Add("-");
                    }
                    else
                    if (ballNumber>0 && frame[ballNumber]+frame[ballNumber-1] == 10)
                    {
                        frameScoring.Add(@"/");
                    }
                    else
                    if (pins == 10)
                    {
                        frameScoring.Add("X");
                    }
                    else
                    {
                        frameScoring.Add(pins.ToString());
                    }
                    ballNumber++;
                }

            }
        }
        rv.Add(frameScoring);
        frameNumber++;
    }
    return rv;
}
....


==== Tennis


So now that we have these domain abstractions for doing console game scoring applications, let's do tennis:


////
[plantuml,file="diagram-bowling-4.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
#subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
bonus [label="Bonus||score\<10 \|\| plays==3"]
frame [label="Frame|\"frame\"|frameNum\<9 && (balls==2 \|\| pins==10)\n \|\|\ (balls==2 && pins\<10 \|\| balls==3)"]
ball [label="SinglePlay"]
game -> bonus -> frame -> ball
}
}
@enddot
----
////


[plantuml,file="diagram-tennis-3.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Tennis"
style=rounded

node [shape=Mrecord]
console [label="ConsoleGameRunner|\"Enter winner of play\""]

scoreboard [label="Scoreboard| -----++----+----+----+----+----++--------\n\| M0  \|\|S00 \|S10 \|S20 \|S30 \|S40 \|\| G0---  \|\n\| M1  \|\|S01 \|S11 \|S21 \|S31 \|S41 \|\| G1---  \|\n -----++----+----+----+----+----++--------\n"]

gamebind [label="Binding|G"]
setbind [label="Binding|S"]
matchbind [label="Binding|M"]
match [label="Frame|\"match\"|score.Max()==3"]

node [shape=record]
function1 [label="GetScore()"]
function2 [label="GetSubFrames()\n.Select(sf =\> sf.GetSubFrames().First())\n.Select(s =\> s.GetScore()).ToList()"]
function3 [label="GetGameOrTieBreakScore\n(see function)"]

console -> scoreboard [constraint=false, label = "IPullDataFlow"]
console -> match [label = "IConsistsOf"]
scoreboard -> setbind -> function2
scoreboard -> matchbind -> function1
scoreboard -> gamebind -> function3
function1 -> match
function2 -> match
function3 -> match

{rank=same console scoreboard}

}
}
@enddot
----

////
[plantuml,file="tennis4.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]

node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
switch [label="Switch||(setNumber\<4 &&\n score[0]==6 && score[1]==6"]
wtp3 [label="WTP"]
tiebreak [label="Frame|\"tiebreak\"|score.Max()==7"]
play2 [label="SinglePlay"]
match -> wtp1 -> switch -> set -> wtp2 -> game -> play
switch:s -> wtp3:w
wtp3 -> tiebreak -> play2
{rank=same set wtp3}

// }
}
@enddot
----
////

I left the code out of the GetGameOrTieBreakScore box as it is a little big for the diagram here. It is similar to the other queries but it must first determine if a tie break is in progress and get that if so. Also it translates game scores from like 1,0 to "15","love".

And here is the code for the Tennis diagram:
....
consolerunner = new ConsoleGameRunner("Enter winner 0 or 1", (winner, engine) => engine.Ball(winner, 1))
.WireTo(match)
.WireTo(new Scorecard(
        "--------------------------------------------\n" +
        "| M0  |S00|S10|S20|S30|S40|S50|S60|  G0--- |\n" +
        "| M1  |S01|S11|S21|S31|S41|S51|S61|  G1--- |\n" +
        "--------------------------------------------\n")
    .WireTo(new ScoreBinding<int[]>("M", () => match.GetScore()))
    .WireTo(new ScoreBinding<List<int[]>>("S", () => 
        match.GetSubFrames()
            .Select(sf => sf.GetSubFrames().First())
            .Select(s => s.GetScore())
            .ToList())
    .WireTo(new ScoreBinding<string[]>("G", () => GetGameOrTiebreakScore(match)))
);

....

If you compare this code with the diagram, you can see a pretty direct correspondence. match comes from the scoring engine project in Chapter two.

==== Concluding notes

Although the diagrams must be turned into text code to actually execute, it is important in ALA to do these architecture design diagrams first. They not only give you the application, they give you the architectural design by giving you the domain abstractions and programming paradigms as well. If you try to design an ALA structure in your head while you write it directly in code, you will get terribly confused and make a mess. Using UML class diagrams will make it even worse. Code at different abstraction levels will end up everywhere, and run-time dependencies will abound. Our programming languages, and the UML Class diagram, are just not designed to support abstraction layered thinking - it is too easy to add bad dependencies (function calls or 'new' keywords) into code in the wrong places.

Note that at run-time, not all dataflows have to go directly between wired up instances of domain abstractions. The data can come up into the application layer code, and then back down. This was the case when we did the functional composition example in Chapter One. In this application we are doing that with the code in the square boxes that get the score from the engine. The important thing is that all the code in the application is specific to the application requirements.  




////


////


////
Now let's have a look at some of the code in the two of the new domain abstractions. Here is the essence of the Scoreboard domain abstraction (remember we are down a layer now, so it has no knowledge of bowling):

....
public string GetScorecard()
{
    var matches = Regex.Matches(ASCIITemplate, "(([A-Z][0-9][0-9])|([A-Z][0-9])|([A-Z]))-*"); // The regular expression matches e.g. A, B1, C12, D-, E00--
    var rv = ASCIITemplate;
    foreach (Match match in matches)
    {
        char id = match.Value[0];
        foreach (IScoreBinding sg in scoreGetters)
        {
            if (id == sg.Label[0])
            {
                if (match.Length>=2 && char.IsDigit(match.Value[1]))
                {
                    if (match.Length >= 3 && char.IsDigit(match.Value[2])) // e.g. A11
                    {
                        rv = rv.Replace(match.Value, sg.GetScore(Convert.ToInt32(match.Value[1]) - Convert.ToInt32('0'), Convert.ToInt32(match.Value[2]) - Convert.ToInt32('0')).PadLeft(match.Length));
                    }
                    else // e.g. A1
                    {
                        rv = rv.Replace(match.Value, sg.GetScore(Convert.ToInt32(match.Value[1]) - Convert.ToInt32('0')).PadLeft(match.Length));
                    }
                }
                else // e.g just A, no index
                {
                    rv = rv.Replace(match.Value, sg.GetScore().PadLeft(match.Length));
                }
            }
        }
    }
    return rv;
}
....

The ScoreBinding domain abstraction has three overloads of GetScore - one for two indexes, one for one index, and one for zero indexes. Here is the code for the one that has one index. The other two are similar. Because we are given one index, we expect the function that we have been wired to will return a one dimensional something. It could be a List or array, of type int or string. T tells us what type it is. Our job is to index into whatever it is, and return it as a string:

....
public string GetScore(int x)
{
    object temp = function();
    if (typeof(T) == typeof(List<int>))
    {
        List<int> list = (List<int>)temp;
        if (x < list.Count) return list[x].ToString();
    }
    if (typeof(T) == typeof(int[]))
    {
        int[] array = (int[])temp;
        if (x < array.Length) return array[x].ToString();
    }
    if (typeof(T) == typeof(List<string>))
    {
        List<string> list = (List<string>)temp;
        if (x < list.Count) return list[x];
    }
    if (typeof(T) == typeof(string[]))
    {
        string[] array = (string[])temp;
        if (x < array.Length) return array[x];
    }
    return "";
}
....


////

That completes our discussion of the console applications for ten-pin and tennis. The full project code can be viewed or downloaded here:

https://github.com/johnspray74/GameScoring[GameScoring code]

