:imagesdir: images

== Chapter four - Programming paradigms and their execution models. 

=== Introduction to programming paradigms

ALA fundamentally begins with the premise of using abstractions to achieve zero coupling at design-time. Zero coupling is preserved if relations between abstractions are always in the direction of greater abstraction. Thus abstraction layers emerge, with each layer significantly more abstract than the one above. It is interesting to observe how these layers seem to emerge typical usage patterns, which in turn give rise to their names: Application layer, User stories layer, Domain abstractions layer, Programming Paradigms layer, and so on.

image::JacquardLoom.jpg[JacquardLoom.jpg, 400, title="Jacquard loom as a programming paradigm for combining weave pattern rows", float="right"]


The layer below the domain abstractions is really interesting in this respect. When we compose or wire two or more instances of domain abstractions together, we need that to have a meaning. Here are some common examples:

* Imperative ((sequential activities in computer time)
* event driven
* dataflow
* UI layout
* activity flow (sequential activities in real time)
* state machine transition
* state machine substate
* data schema

These are all quite abstract concepts. They have great potential for reuse. We call them programming paradigms because each is a different way of thinking about programming. Each gives different meaning to composition. We call the layer they go in _programming paradigms_.

It is an essential part of ALA that we can use multiple programming paradigms in the same user story. In ALA, user stories (or features) are cohesive abstractions. To completely describe a user story, common programming paradigms needed may be UI layout,  dataflow, activity and data schema. This use of multiple programming paradigm is referred to as polyglot programming paradigms.

The programming paradigms layer may also contain other abstractions useful for building domain abstractions, but are not used as relationships between instances of abstractions. Examples are the concepts of 'Persistence' or 'Styles'. In this chapter we will be concentrating on programming paradigms used to compose instances of domain abstractions, and showing the ways they actually execute, which is called their execution model.

Here is Peter Van Roy's taxonomy of programming paradigms which gives us an idea of what the term "programming paradigm" means in general. Peter Von Roy is an advocate for multiple programming paradigms in a computer language, which is why the language Oz appears all over the diagram. In ALA we invent and implement our own programming paradigms, without relying on them being built into the underlying programming language. 

image::TaxonomyProgrammingParadigms.png[TaxonomyProgrammingParadigms.png, title="Taxonomy Programming Paradigms - cited from Van Roy", link=images/TaxonomyProgrammingParadigms.png]

[TIP]
====
In ALA "programming paradigms" are used to compose instances of domain abstractions. The programming paradigm provides "the meaning of composition".
====

Programming paradigms are by no means limited to the ones discussed in this chapter. Custom ones can be invented as needed (when they allow better expression of the requirements). We do this with the example at the end of this chapter for scoring games such as Bowling or Tennis. We use a 'ConsistsOf' programming paradigm which allows us to express that a match consists of sets, a set consists of games, and so on.



=== Introduction to execution models

Programming paradigms and execution models are closely related but not precisely the same thing. Programming paradigms are the meaning of composition. Execution models are how we make that meaning actually work. Essentially, the execution model is how the underlying imperative programming paradigm of the CPU is going implement the new programming paradigm.

Some programming paradigms can have simple execution models. They can be implemented with an interface with one method. The simplest example of this is the synchronous event programming paradigm: 

.IEvent.cs
[source,C#]
....
namespace ProgrammingParadigms
{
    interface IEvent
    {
        void Send();
    }
}
....

Such an interface looks like it adds nothing, but it transforms  programming to a paradigm of composing instances of abstractions in an infinite variety of possible ways.

Ports use instances of these interfaces. Essentially the interface allows any output port using this paradigm to be wired to any input port using this paradigm. 

Other programming paradigms may require an engine or framework. Possible ways these execution models could work is discussed under each programming paradigm in this chapter.



=== Coding execution models

Here we just show simple code examples for two execution models for the event driven programming paradigm to get a grounding at the code level. 


==== Synchronous events


The interface for synchronous events was given just above. 

The difference between this and your everyday common imperative function call or method call is only the indirection. The sender doesn't know who it's calling. (In conventional code, indirection creates problems, but in ALA these problems do not exist, so all communications use this type of indirection. This is discussed further later.)  

First let's create two dummy domain abstractions with ports using the synchronous event driven programming paradigm. The two domain abstractions will use and implement this interface respectively:



.A.cs
[source,C#]
....
using System;
using ProgrammingParadigms;

namespace DomainAbstractions
{
    class A
    {
        private IEvent output; <1>

        public void start()
        {
            Console.WriteLine("1");
            output?.Send();
            Console.WriteLine("3");
        }
    }
}
....



<1> The output port is a private field of type interface


.B.cs
[source,C#]
....
using System;
using ProgrammingParadigms;

namespace DomainAbstractions
{
    class B : IEvent <2>
    {
        // input port
        void IEvent.Send()
        {
            Console.WriteLine("2");
        }
    }
}
....

<2> The input port is an implemented interface


Now we can write an application that wires an instance of A to an instance of B.

.Application.cs
[source,C#]
....
using System;
using DomainAbstractions;
using ProgrammingParadigms;
using Foundation;

namespace Application
{
    class Application
    {
        static void Main(string[] args)
        {
            var program = new A().WireTo(new B()); <1>
            program.start();
        }
    }
}

....

<1> The meat of the application wires an instance of class A to an instance of class B.

The output of the program is "123".

The Main function instantiates one instance of each of our domain abstractions, and wires them together. (If you have not seen the WireTo abstraction before, it is an extension method that uses reflection to search in class A for a private variable with a type that is an interface. It then sets it pointing to the instance of B if B implements the interface. WireTo is not central to the current discussion, the IEvent interface is.  WireTo is discussed in more detail in the example projects of chapters two and three.) 

Notice just how abstract IEvent is. It's highly reusable. It's not specific to any domain abstraction or the application. It just knows how to transmit/receive an event. Because it is so abstract, it is stable. The more domain abstractions that depend on it the better, as that will allow them to be wired together in arbitrary ways, which gives us composability.

The IEvent interface can be compared with the observer pattern (publish/subscribe) which also claims to achieve decoupling. However the observer pattern only reverses the dependency of a normal method call. Instead of the sender knowing about the receiver, the receiver knows about the sender (when it registers for the event). If the sender and receivers are peers in the same layer, the observer pattern does not solve the problem. The IEvent interface decouples in both directions. The job of 'subscribing' is moved to the application layer, because only the application should have the knowledge of what should be wired with what.




==== Asynchronous events (the event loop)

In the above example, we used the word _event_, but implemented it in a specific way (a synchronous method call). The terms _event_ and _event driven_ may have overloaded meanings. To some it may mean asynchronous or it may mean observer pattern (an event is a public thing you can subscribe to), or it may mean both.

In ALA the term means neither of these. As a programming paradigm it simply means that we think of programming as reacting to what happens instead of prescribing what will happen next - a reactive rather than prescriptive programming style. They can be either synchronous or asynchronous. They are never public - the layer above always wires them up from point to point explicitly. Events can be wired to fan-in or fan-out. 

We discuss the meaning of synchronous and asynchronous in more depth later, but here we just want to see how asynchronous can be implemented at the code level. Synchronous and asynchronous are two different execution models for the same programming paradigm.

To implement the asynchronous execution model, conventional code may use an event loop that works something like this: the originator of the event calls a Send method on an EventLoop object. It passes a reference to a function or method of another object that it wants to send the event to. The Send method in EventLoop creates an object that represents the event and puts it into a queue. The Send method then returns. The main loop resides in this EventLoop object. It loops taking events from the queue one at a time and calls the referenced function or method. This is sometimes called the reactor pattern, but its actually a simplified version of reactor so we will call it simply an _event loop_.

For ALA, the only difference is that the sender can not specify the receiver function and object.

Here is the application layer code:


.Application.cs
[source,C#]
....
using System;
using DomainAbstractions;
using ProgrammingParadigms;
using Foundation;

namespace Application
{
    class Application
    {
        static void Main(string[] args)
        {
            // instantiate an asynchronous execution model
            var eventLoop = new EventLoop();
            
            // Wire using the asynchronous execution model
            var program = new A().WireTo(new B(), eventLoop); <1>
            program.Start();
            
            eventLoop.Start();
        }
    }
}
....

<1> The meat of the application wires an instance of class A to an instance of class B.

The difference with our previous synchronous application is that we first spin up an asynchronous execution engine object called eventLoop. The WireTo is used in the same way except that we pass in the execution model. 

Here are the A and B dummy domain abstractions again. They are identical to the ones we used for the synchronous version above.  

.A.cs
[source,C#]
....
using System;
using ProgrammingParadigms;

namespace DomainAbstractions
{
    class A
    {
        private IEvent output;

        public void Start()
        {
            Console.WriteLine("1");
            output?.Send();
            Console.WriteLine("3");
        }
    }
}
....


.B.cs
[source,C#]
....
using System;
using ProgrammingParadigms;

namespace DomainAbstractions
{
    class B : IEvent
    {
        // input port
        void IEvent.Send()
        {
            Console.WriteLine("2");
        }
    }
}
....

When this program runs, it will print "132" instead of the "123" that the synchronous version did. At the domain abstraction level, we need to not care whether it is "123", or "132". If we do care, then we need to use a different programming paradigm.

Now let's have a look at the programming paradigm abstraction to see how it works.




.AsynchronousEvent.cs
[source,C#]
....
using System.Collections.Generic;
using Foundation;

namespace ProgrammingParadigms
{
    public interface IEvent <1>
    {
        void Send();
    }


    static class EventLoopExtensionMethod <2>
    {
        public static T WireTo<T>(this T A, object B, EventLoop engine, string APortName = null)
        {
            engine.WireTo(A, B, APortName);
            return A;
        }
    }




    class EventLoop
    {

        private List<IEvent> queue = new List<IEvent>(); <3>

        public void WireTo(object A, object B, string APortName) <4>
        {
            A.WireTo(new Intermediary(this, (IEvent)B), APortName);
        }

        public void Start()
        {
        
            while (!Console.KeyAvailable) 
            {
                if (queue.Count > 0)
                {
                    IEvent receiver = queue[0];
                    queue.RemoveAt(0);
                    receiver?.Send();
                }
            }
        }


        private class Intermediary : IEvent
        {
            private IEvent receiver;
            private EventLoop outerClass; // needed to access our outer class instance

            public Intermediary(EventLoop outerClass, IEvent receiver)
            {
                this.receiver = receiver;
                this.outerClass = outerClass;
            }

            void IEvent.Send() <5>
            {
                outerClass.queue.Add(receiver);
            }
        }
    }
}
....

<1> The abstraction begins with the interface itself, which is unchanged from the synchronous version.

<2> Overload of the WireTo extension method. We want an extension method so that we can wire things up using the same fluent syntax as the standard WireTo. This WireTo overload differs from the usual WireTo by the extra parameter for passing in the AysnchronousEventLoop instance. The method simply defers to the WireTo method in the EventLoop class.  

This mechanism of overloading the WireTo method can be used by any programming paradigm.

<3> The EventLoop class keeps a list of events waiting for execution.

<4> Unlike for the synchronous case, the sender's port is not wired directly to the receiver's port. An intermediary object is wired in-between. The class for the intermediary object is inside the EventLoop class as we don't want it to be a public part of the EventLoop abstraction.

The WireTo method instantiates an intermediary object, stores the receiver object cast as the interface into it (which is effectively wiring the intermediary to the receiver), and then calls the standard WireTo in the Foundation layer to wire the sender to the intermediary object. 

<5> When the sender calls Send on its output port, the intermediary object intercepts the synchronous call. The intermediary object queues the call in the EventLoop class and immediately returns. It actually queues the reference to the interface of the receiver. 

<6> The EventLoop class has a loop that takes the references to receiver objects out of the queue one at a time, and calls the IEvent's Send method in the receiver.

In this example we have put the main loop inside the execution model for simplicity. We would not normally do this because we may have several different programming paradigms each with their own main loops. So we could have the main loop in the Foundation layer, and the different execution models would register a Poll method on it. Alternatively we could make the loop function an async function that awaits on an awaitable queue implementation.   

As usual in ALA, we do not try to decouple anything inside the AsynchronousEvent.c abstraction. Everything in it cohesively works together.

The propose of the examples is to show that we can create our own programming paradigms and that their implementation can be simple.


=== Execution model properties

Now that we have the idea of what we mean by programming paradigms and execution models at the code level, we next need to discuss some general properties of execution models, such as direct vs indirect, synchronous vs asynchronous, push vs pull, etc. we will refer to these properties when discussing specific execution models later.

In conventional imperative code, the execution model is inherently synchronous in the use of the function or method call. In ALA we have free  choices for execution models. Also in conventional code, one of the forces is managing dependencies. This can influence the execution model. For example we might pull for a dependency reason even though we would rather push for a performance reason. In ALA, wiring does not involve dependencies, so we are free to focus on other design choices with respect to execution models. 

In this section we will try to clarify what these design choices are for programming paradigms that mean communication. We will note the forces on these design choices.


==== Sideways vs down vs up communications 

In conventional code, communications generally follow dependencies directly. If we try to think in terms of layers, with dependencies always going down the layers, these communications always go either up or down. So we may not be used to thinking of sideways communications. Or if we do allow sideways dependencies within a layer, we may not be used to thinking about sideways communications and up/down communications as different things. 

In ALA, we need to think of them differently. Let's refer to sideways communications as _wired-communications_, and up/down communications as _abstraction-use-communications_. 

A common example of abstraction-use communication is when you configure an instance of an abstraction by passing parameters to the constructor, or by calling setters. Another example is calling a squareroot function in your math library. A common example of upward communication using abstraction-use-communication is executing a lambda expression that has previously been passed in to an instance of an abstraction during its configuration. Upward calls are always indirect in some way, such as the mentioned lambda expression, passing in an anonymous or named function, observer pattern (publish subscribe), callback, or strategy pattern. We don't use virtual functions in ALA for up calling because we don't need or want to use inheritance. 

A common example of _wired-communications_ is when an instance of an abstracton sends something out on a port. It arrives at the input port of another instance of an abstraction to which it was wired by the layer above. 

In all the following discussions of programming paradigms, we will be talking about _wired-communications_ unless noted otherwise. Note that we use the word _communications_ to cover for both events and dataflow types of programming paradigms. Another common term is _message_.


==== Indirect function calling

Sideways communications in ALA is _always_ indirect. The sender never names the receiver or the function or method in that receiver. Conversely, receivers never register themselves to a sender, or to a public event. Global event names are never used. Communications always follows the wiring put in place by the layer above.

In conventional code, there is a downside associated with indirection, which is tha it becomes harder to follow the flow of execution through the modules for a given user story. That downside does not exist for ALA. In fact it is the opposite - it is easier to trace the flow of calls through the system. This is because user stories are expressed in one place cohesively. You see all the explicit wiring of a user story abstraction in one place instead of tracing it through multiple modules. Only if an abstraction it uses does something unexpected do you need to drop down inside the abstraction, and enter a different self-contained self-cohesive set of code. 

When reading code inside an abstraction, it is in the very nature of abstractions that they know nothing of the outside world. They do not need or want to know where events come from or go to externally. Indirection is used so that flow can lift out of the internals of an abstraction to the more specific wiring code in a layer above.

Even synchronous function calls are always indirect. At run-time, the inside of one abstraction synchronously calls a method inside another abstraction under the direction of the wiring in the layer above. But at design-tie, it has no knowledge of what that other abstraction or method is. Whether the run-time execution model is synchronous or asynchronous, push or pull, with fan-out or fan-in, the wiring model between instances of abstractions with ports is always indirect.

Even if the communications is asynchronous, the caller does not send the event to a particular destination, nor does it give the event a global name so that receivers can register to it. Both patterns would involve a bad dependency. Instead it still only goes as far as its own output port.

Conventional code will often use an interface or the observer pattern (publish-subscribe) (or C# events) to invert a dependency. If the two modules were peers in the same layer, inverting the dependency by adding an indirection only makes the program even more difficult to follow. ALA does not need to use the dependency inversion principle or the observer pattern for peer to peer communications because there is no dependency. In other words ALA completely sidesteps the dependency inversion principle and the observer pattern for all communications between peers.

ALA generally uses dependency injection directed by explicit wiring.

Having said that ALA does not use the observer pattern (or any other form of the receiver subscribing to senders in the same layer), the observer pattern is sometimes used within a programming paradigm interface. Consider a programming paradigm where communications is needed in both directions. In the same direction as the wiring, it is usually implemented as a simple method call. The way interfaces work in our programming languages, the A end uses the interface and the B end implements it. The asymmetry is a shame. If we want a method call in the other direction, we use the observer pattern inside the interface. The publisher, the B end, implements the observer pattern. The subscriber, the A end, subscribes to it. The difference from the standard observer pattern is that the subscriber does not know the publisher. It is only subscribing to it indirectly via the interface.

If a dependency were going up from one abstraction layer to a higher one, then of course we invert the dependency. But a dependency from a more abstract abstraction to a more specific one doesn't make sense in the first place and so this situation never occurs. The dependency inversion principle is already built into the ALA constraints, so you never need to invert dependencies later.



==== Push vs pull

If we are using standard synchronous function calls or method calls as the execution model, we have a choice between push and pull. In other words, does the sender of an event or data initiate the call, or does the receiver?

Push
[source,C#]
....
Send(data);
....


Pull
[source,C#]
....
data = Receive();
....


In conventional code, the decision as to whether to use push or pull is often dictated by the need to control the direction of dependencies. To change a pull to a push without reversing the dependency would require indirection or the observer pattern. Similarly, to change a push to a pull without reversing the dependency would require an indirection. So usually we use the one that allows us to use a simple function call with the dependency in the desired direction. 

With ALA, most run-time communications take place within a layer, and there are no dependendencies between the abstractions involved. Instances of abstractions are wired using interfaces that represent programming paradigms:

.IDataflow.cs
[source,C#]
....
namespace ProgrammingParadigms
{
    interface IDataflow<T>
    {
        void Push(T data);
    }
}
....

.IDataflow.cs
[source,C#]
....
namespace ProgrammingParadigms
{   
    interface IDataflowPull<T>
    {
        T Pull();
    }
}
....



Because there are no dependency constraints, we are free to choose between push and pull. Usually it would be for performance reasons. If the source data changes infrequently we would use push. If source data changes frequently, and the receiver only needs the value infrequently, we could choose to use pull. 

 An example of pull is getting data from a database. Pulling is the only choice that makes sense because any particular data is needed so infrequently. And pushing is the only sensible choice for putting data into a database. For this reason, in conventional code, the dependency is almost always towards the database. This is not the desired direction. Clean architecture reverses this dependency. But we don't want the reverse dependency either. So clean architecture will use a set of adapters that have dependencies both on the business logic interfaces and the database. ALA uses no dependencies on the business logic. In effect it will use a single adapter with dependencies on both a programming paradigm interface and the database.       

It would be nice if you could choose between push and pull at wiring time. In other words, we design domain abstraction ports to handle both push or pull, and you choose push or pull when wiring instances in the application. For example a signal filter could support both push and pull. If not we might need two version of the filter.  Unfortunately it increases the amount of code inside the abstractions. So we usually write abstraction ports to use either push or pull. 

To allow optimal composability of abstractions, I use push ports by default so that most ports can be wired directly. Push also works quite naturally for events. It means that the initiator of an event pushes it as soon as it happens. The opposite is possible: receivers poll the source when they are interested to know if an event has occurred.

For dataflows, push means that the data 'flows' whenever it changes. This works better performance-wise if the data does not change too frequently. It works well when all data must be processed. It is ok when all the data does not need to be processed, and only the latest data is important. Push is usually more efficient than periodically polling for data. 

A final factor in the preference to use push by default is that push ports can be wired for either synchronous or asynchronous execution models without changing the domain abstractions (discussed above in the section on synchronous vs asynchronous). To allow this for pull ports requires the pull end to be written for an asynchronous execution model, which can be awkward. This aspect is discussed more fully in the section on the request/response programming paradigm later.

For all the above reasons we use push ports by default, and pull ports when we have to. It is analogous to using RX (reactive extensions).

Remember we are talking about 1-way communcations. In a later section we discuss programming paradigms that use 2-way communications. 


===== Wiring incompatible push & pull ports

It is possible to wire together instances of domain abstractions that have incompatible ports with respect to push and pull, provided the communications becomes asynchronous. A send port that uses push can be wired to a receive port that uses pull. And a send port that uses pull can be wired to a receive port that uses push. This can even be done automatically, so that the user story doing the wiring does not need to worry about it.

For the case of a push send port being wired to a pull receive port, the wiring system detects this situation and wires in an intermediary object which is an instance of a simple buffer abstraction. If the paradigm is simple events, the abstraction stores a flag for whether or not the event has been sent. When the receiver pulls the event, it clears the flag.

For the case of a pull send port being wired to a push receive port, the wiring system detects this situation and wires in an intermediary object which is an instance of a simple polling abstraction. This instance is configured with a default polling rate. It polls the sender periodically to see if the event has occurred, and then calls the receiver if it has. For dataflow, it calls the sender periodically, and then calls the receiver at least once and thereafter whenever the  data changes. 

A situation where a sender may want to have a pull port is a driver that gets data from the outside world. The driver doesn't want the responsibility of controlling when the external read takes place. So it will use a pull port so it reads at a time determined by the user story. The user story will either configure the polling rate of the intermediary or configure an active object somewhere that will pull the data when needed. 

Another situation to use pull is where the sender is completely passive or lazy. For example, it doesn't want to execute a computationally expensive routine until the output is needed. 

Another situation where a pull port makes sense is an abstraction with many inputs. We want the abstraction to react when a specific port receives data or an event. If we don't want to buffer the data coming in on other inputs internally in the abstraction, we can just make them pull ports. If they need to be wired to push ports, then intermediary buffer objects would be wired in. 

When a sender with a push port is wired to a receiver with a pull port using a buffer intermediary object, a situation can arise where the sender produces data faster than the receiver consumes it. In some cases this wont matter. In other cases the user story has the knowledge of how to resolve the situation. It can wire in an averager or filter abstraction. If the receiver must process all the data, and the sender produces data only in bursts, the user story can wire in a FIFO abstraction to smooth out the rate of data. The Fifo can have a reverse _flow control_ channel that tells the source when to stop and start so the fio doesn't overflow. If none of these solutions work, the user story can wire in a load splitter to multiple receivers.  

If pull ports are quite common, we may then want 'pull' versions of some domain abstractions. For example, we may need a filter abstraction to have a pull variant. 

In summary, I use push ports for domain abstractions by default. In situations where this doesn't suit I can still use pull ports. When  incompatible ports need to be wired, then a variety of intermediary objects can be wired in to solve the issues without having to change the sender or receiver abstractions. 



==== Fan-in, fan-out wiring

In chapter three, we used the terms _fan-in_ and _fan-out_ in relation to dependencies down layers. Here the terms _fan-in_ and _fan-out_ are used for something completely different. Here we are talking about wiring.  

Fan-out means that an output port of one instance of an abstraction is wired to many instances. Fan-in means many instances are wired to a single input port. It depends on what makes sense for each particular programming paradigm.


===== Fan-out implementation

Some programming paradigms support fan-out out of the box. An example is the UI programming paradigm. Many UI domain abstractions have a list port for  child UI elements. The WireTo can wire directly from this port to multiple instances of other UI elements.

Most output ports of domain abstractions for other programming paradigms do not use a list for their output ports, so they do not directly support fanout. This is because they are usually wired one point. If they used a list, then the domain abstraction internal code would need to use a _for_ loop to output to every instance in the list. We can still do fanout using an intermediary object. This intermediary object simply contains the needed for loop. An example of such an intermediary for the Dataflow programming paradigm is: 

.IDataFlow.cs
[source,C#]
....
/// <summary>
/// DataFlowFanout has multiple uses:
/// 1) Allows fanout from an output port
/// 2) If the runtime order of fanout dataflow execution is important, DataFlowFanout instances can be chained using the Last port, making the order explicit.
/// 3) Allows an abstraction to have multiple input ports of the same type. (A C# class can implement a given type of interface only once.)
/// --------------------------------------------------------------------------------
/// Ports:
/// 1. IDataFlow<T> implemented interface: incoming data port
/// 2. List<IDataFlow<T>> fanoutList: output port that can be wired to many places
/// 3. IDataFlow<T> last: output port that will output after the fanoutList and IDataFlow_B data changed event.
/// 4. IDataFlow_B<T> implemented interface: ouput port but is wired opposite way from normal.
/// </summary>

public class DataFlowFanout<T> : IDataFlow<T>, IDataFlowPull<T>, IDataFlow_R<T>  <1> <2> <3>
// input, pull output, push output
{
    // properties
    public string InstanceName = "";

    // ports
    private List<IDataFlow<T>> fanoutList = new List<IDataFlow<T>>(); <4>
    // ouptut port that supports multiple wiring 

    private IDataFlow<T> Last; <5>
    // output port that outputs after all other outputs to allow controlling order of execution through chaining instances of these connectors. 


    // IDataFlow<T> implementation (input) ---------------------------------
    void IDataFlow<T>.Push(T data) <6> <7>
    {
        this.data = data; // buffer the data in case its needed by the pull output
        foreach (var f in fanoutList) f.Push(data);
        push_R?.Invoke(data);
        Last?.Push(data); <5>
    }

    // IDataFlowPull<T> implementation ---------------------------------
    private T data = default;  // used to buffer data for later pull on the output port
    T IDataFlowPull<T>.Pull() { return data; } <7>

    // IDataFlow_R<T> implementation ---------------------------------
    // make explicit so it's not visible without using the interface
    private event PushDelegate<T> push_R; 
    event PushDelegate<T> IDataFlow_R<T>.Push { add { push_R += value; } remove { push_R -= value; } } <7>
}
....

<1> IDataFlow<T> is the input port

<2> IDataFlowPull is an output port (purpose discussed later)

<3> IDataFlow_R is an output port (purpose discussed later)

<4> Output port that's a list to support fan-out. WireTo will wire it any number of times.

<5> Output port called Last (purpose discussed later)

<6> Implementation of the input port. When data arrives at the input, it outputs the data directly to all the different output ports, including to every destination in the fanout output port list. 

<7> All implemented interfaces are implemented explicitly in C# (not implicitly). There are two reasons for this in ALA: 1) We only want the interface's method/event to be visible through a reference to the interface, not the public interface of the class. The public interface of the class is for the layer above to create and configure objects of the class. It generally has no need to access the ports of the class at run-time, and if it did we would want to cast to the interface to make that clear. 2) If there were two interfaces using the same method name or same event delegate, we will want to implement them separately. 





===== Fan-out ordering 

The need for fan-out in the wiring is common for many programming paradigms. The order of the synchronous calls to the different fan-out destinations may or may not be significant. Only the layer above doing the wiring knows if the order is significant. Sometimes it is sufficient for the order to be defined as the order they are wired in, or ’down’ in a diagram. The UI fanout works this way to control top to bottom or left to right UI layouts. This is a satisfactory way to define order in a UI.

For events or Dataflows, this is not considered explicit enough. Where order matters, we should use ”Activity Flow” (exactly analogous to UML activity diagrams) to control ordering. The order can be controlled by using a chain of DataFlowFanout instances. DataFlowFanout has a port called _Last_ which facilitate this chaining. Last is invoked after all other output ports. 

<5> The _Last_ port can be seen in the DataFlowFanout listing given above.


==== Work around for multiple inputs of the same type

C# and other languages don't allow an interface to be implemented more than once. Sine we use interfaces as ports in ALA, this can be a serious limitation.

For example, consider implementing an AND gate with 4 inputs all IDataFlow<bool>.

.AndGate.cs
[source,C#]
....
    public class AndGate : IDataFlow<bool>, IDataFlow<bool>, IDataFlow<bool>, IDataFlow<bool> 
    {
    }
....

Implementing IDataFlow<bool> more than once like that gives a compiler error.

It's a valid thing to do however. I can only assume that outside of ALA, no one seems to have needed it. In fact the whole concept of _ports_ should be part of all object oriented languages. Only then would OOP realize it's potential for reuse. (ALA is really just OOP done right.)

If the C# language allowed the same interface to be implemented multiple times, the only syntactical difference would be that the implementations would be given names:

.AndGate.cs
[source,C#]
....
    // We want to do this, but can't in C#
    public class AndGate : IDataFlow<bool> Input1, IDataFlow<bool> Input2, IDataFlow<bool> Input3, IDataFlow<bool> Input4
    {
        void Input1.Push(bool data)
        {
        }
    }
....


You would be able to set a reference to the object's interface using this name instead of casting to the interface type.

[source,C#]
....
// We want to do this, but can't in C#
var ag = new AndGate();
IDataFlow<bool> referenceToInput1 = ag.Input1;
....

Java almost allows this to be done using method references. But it only works when there is one method in the interface.

We already used a work-around for this limitation of C# in the _Add_ domain abstraction in chapter 2. In that work-around we created a Double2 type which was a simple struct containing a double. That allowed us to implement both IDataFlow<double> and IDataFlow<Double2>. But it's not a general solution.

A more general work-around for this limitation of C# is to use interface fields instead of interface implementations and reverse the wiring.


.AndGate.cs
[source,C#]
....
    IDataFlow_R<bool> Input1;
    IDataFlow_R<bool> Input2;
    IDataFlow_R<bool> Input3;
    IDataFlow_R<bool> Input4;
....

We append an "_R" to the name of the interface to indicate it is a 'reversed wired' interface. Here is the interface:

.IDataFlow.cs
[source,C#]
....
    public delegate void PushDelegate<T>(T data);

    public interface IDataFlow_R<T>
    {
        event PushDelegate<T> Push(T data);
    }
....

The receiver registers an event handler method to the event in the interface:


.AndGate.cs
[source,C#]
....
    private void Input1Initialize() <1>
    {
        Input1.Push += PushHandler1;
    }

    private void PushHandler1<T>(T data) <2>
    {
        ...
    }
....

<1> After the WireTo operator has wired a port, it looks for a method named <Portname>Intialize and calls it. This method is useful if the port's interface has a C# event. It can be used to register a method to the event.

<2> Method called for incoming data on port Input1.


To complete the workaround we need an intermediary object. Both the sender and receiver are wired _to_ this object. It implements both IDataFlow<T> and IDataFlow_R<T>. The class for this object resides inside the IDataFlow programming paradigm abstraction:

.IDataFlow.cs
[source,C#]
....
public class DataFlowIntermediary<T> : IDataFlow<T>, IDataFlow_R<T> // input, output <1>
{
    void IDataFlow<T>.Push(T data) <2>
    {
        push?.Invoke(data);
    }

    // IDataFlow_R<T> implementation ---------------------------------
    private event PushDelegate<T> push; <3>
    event PushDelegate<T> IDataFlow_R<T>.Push { add { push += value; } remove { push -= value; } } <4>
}
....

<1> Unlike normal output ports, this output port is an implemented interface.

<2> When data arrives on the input port it outputs it directly to the output port. 

<3> The output port interface has a C# event, which needs to be implemented.

<4> The interface implemented explicitly so that the event is only accessible via a reference to the interface.  

The above code is also added to the DataFlowFanout class listed above. See note 3 in that listing. This allows the DataFlowFanout intermediary object to be used for the purpose of this workaround among its other uses. 

A problem with this workaround is that you need to wire in the reverse direction to the flow of data. So if data is to flow from A to B, we would need to write:

[source,C#]
....
var intermediary = new DataFlowFanout();
new A().WireTo(intermediary);
new B().WireTo(intermediary);
....

This is unintuitive at the wiring level. 


We would prefer to write like we do normally: 

[source,C#]
....
new A().WireTo(new(B));
....

We can write an override of WireTo in the programming paradigm abstraction and register it with the Foundation WireTo. 

The override WireTo would look for a field interface in A that matches a field interface in B by name with a _R suffix.

TBD write the override WireTo.


==== Wiring arbitrary execution models

To accomplish wiring, the application, feature or user story abstraction's code makes calls to the WireTo method, passing in the two object/ports to be wired. The WireTo method, by default, wires the two objects by assigning the second object to a private field in the first object, provided the interface matches. This default behaviour sets up a direct connection between two communicating objects.

For arbitrary execution models, we don't always want direct connection between connected objects. We may want an intermediate object to be automatically wired in, or other special behaviours. For example if the two objects being wired are in different locations, we will want to automatically wire in the necessary middleware intermediary objects. Intermediary objects are commonly needed in ALA. We have previously used them for several different purposes, such as asynchronous communications, pull communications, etc.

In the asynchronous programming code earlier in this chapter, we used an override of the WireTo method that had an extra parameter. But what if there is not extra parameter. Then the WireTo method that resides in the ALA foundation layer is the one that will be called. It can't know anything about programming paradigms or execution models in higher layers. But it can know in an abstract way about allowing itself to be overridden. 

The WireTo method in the foundation layer can support a list of registered override functions. It calls every override function in the list. If all return false, then it does its default behaviour.

The foundation WireTo can first do the reflection work. It can create lists of potential field and implemented interface ports in both the A and B objects. Then it can pass these lists to the override functions.

TBD Modify WireTo to support run-time overridding. Use it to implement a null decorator intermediary on the synchronous programming paradigm. Then use it to implement wiring of a push port to a pull port and a synchronousmiddleware for wiring objects in different locations.





==== Diamond pattern glitches

Consider a wiring topology of an application in which wiring diverges from a single instance of an abstraction, and then converges to a single instance of an abstraction. The two paths will be executed at sightly different times. So one input of the end instance will get data from the common source before the other. During the time between the two, the inputs may be in an invalid state. This is what we mean by a glitch.   

Glitches also happen in conventional code where they are a cross cutting concern. They also even happen in electronic circuits. 

In ALA, they are a concern within a single abstraction, either the application abstraction, or a feature or user story abstraction. This is where the diamond topology of the wiring is apparent and the problem can be easily understood.

Abstractions may have a minor inputs which it expects to get data first and major input that triggers operation. In such a scenario, the application can control the order of execution in the wiring so that the major input gets its data last.  

One solution is to provide an trigger event port on abstractions that have multiple inputs. The application must trigger the port once all inputs are valid.

It is a future topic of research to automatically detect glitches on abstractions with multiple inputs, and potentially to automatically resolve diamond wiring glitches.


==== Circular wiring

In ALA, it is no problem to have circular data paths. Note that by _circular_, we are referring to wiring inside an abstraction, not dependencies between layers. Circular wiring naturally occurs in feedback systems, just as it does in electronics. It is nice to be able to represent such feedback systems directly in the wiring.  

In conventional code, circular data paths may need a pull or an indirection to avoid circular dependencies. ALA does not have this problem. Circular wiring is as natural as it is in electronics.

A programming paradigm's execution model needs to consider circular wiring. For example, circular wiring using all synchronous programming paradigm will result in an infinite loop at run-time, just as it does in conventional code. It easy to solve however. It can be as simple as an abstraction instance placed in the circuit that does an asynchronous call, or an abstraction instance that does a delay. This effectively causes a return to the main loop where the circuit can be called again. The main loop can process higher priority tasks first. It is no problem for such a circuit to repeat forever.

Alternatively, we can implement programming paradigms utilizing existing rigorous execution models, such as the discrete time execution models used in function blocks or clocked-synchronous execution models. The continuous time execution model underlying Functional Reactive Programming will automatically ﬂags such loops. 

While circular data loops can occur in conventional code as well (recursion), they are more likely in ALA because ALA is likely to have dataflow abstractions which can easily be wired as a circuit. However, in ALA it is usually explicit and clear in the wiring diagram or code. The Calculator project in chapter two contained Dataflow loop circuits.





==== Synchronous vs asynchronous

Although we already did simple coding examples for synchronous and asynchronous execution models at the start of this chapter, the design choice between synchronous and asynchronous needs deeper considerations.

Synchronous communication is like asking someone a question. You stop your life and wait, albeit for a brief time. You don't resume your life until you get the answer or a nod. Asynchronous communication is like sending an e-mail.

Synchronous means that the calling code resumes execution after the callee has finished processing the communication.

There are reasons why you may want to use synchronous communications. The communication may cause a side effect, which we want to be sure is completed before continuing execution.

If the receiver will take a long time to execute, which can be for many reasons such as a long running algorithm, receiver not ready, external IO, a deliberate delay, etc, then a synchronous call will do what is referred to as blocking. Blocking means the thread will stop and wait. If the blocked thread needs to do something else in the meantime, this blocking will be a problem in one way or another.

In ALA we prefer single threaded solutions. Multi-threaded programming should only be used for performance reasons e.g. meeting a challenging latency or throughput requirement. A single threaded system will use run to completion, so in that respect is commonly referred to as cooperative. Being cooperative sounds like it doesn't comply with ALA's zero coupling. To some extent this is true, but the requirement to keep _all_ routines short (non-blocking) can be thought of as an abstract requirement from a lower layer rather than relative coupling between domain abstractions. All higher abstractions need to know about this. Usually if nothing in an application blocks, the latencies needed for an application to respond to a human in reasonable time (which is the most common soft deadline requirement) will be acceptable. Using a single thread when things take time, or things need to happen in real time requires asynchronous communications.

Asynchronous means that the sender instance's call returns before the callee has finished processing the communication. It will usually be before the callee even receives the communication.

Asynchronous calls can be implemented in several different ways. What they all have in common is that the caller makes a synchronous call that starts the communication or starts the callee's execution in some way. The caller will then resume executing the next line of code pretty much immediately.

In ALA, as with the synchronous case, the caller does not know where it is sending the communication and the callee does not know where it came from. Where synchronous and asynchronous communications differ is only in when the call returns. 

Note that here we are discussing the fundamental case of one way communication. We will consider two way communication programming paradigms later. 

With one way communication, we have the option to decide at wiring time whether to use synchronous or asynchronous, provided the sender doesn't care whether it resumes processing before or after the receiver gets or processes the message.

Some common ways of implementing asynchronous calls are:

. The sender can make a synchronous call on the receiver, which just initiates an on-going activity and returns. It can be starting I/O, starting a timer, changing a state, etc.


. The sender can make a synchronous call that just sets a flag, which is later polled by the main loop which then calls the receiver code. 
+
In ALA this is easily implemented using an intermediary object that is wired between the caller and callee. See "Wiring arbitrary execution models" below. The intermediate object's class resides inside the programming paradigm abstraction. It contains the flag. Within the programming paradigm abstraction, all the intermediary objects are put on a list. The main loop simply polls every object on the list. When the poll method in the object sees that the flag is set, it clears it and calls the callee.

. The sender can make a synchronous call which is turned into an object which goes into a queue. The main loop takes these objects from the queue and calls the receiver code. In terms of run-time execution this is the same as the simple version of the reactor pattern or simply 'event loop'. Example code for this method was given above.

. The sender can make a synchronous call which puts an object into the receiver's queue on a different thread, process or processor. 

. If the language has async/await, the sender can call a method marked with the async keyword (without using await itself). The call returns immediately the first time the receiver awaits.

Other mechanisms are possible. Note that all of these mechanisms describe how the sender's synchronous call returns before the receiver completes.

Remember that in all these implementation examples, we are talking about fundamental one-way communication - an event or pushing some data. Two-way communications gets more complicated, and is discussed below. 

All the asynchronous programming paradigm execution models discussed above use pushing. Analogous pulling asynchronous communications are also possible. For an asynchronous pull, the receiver makes a synchronous call which returns a previously calculated result without waiting for the sender to calculate it. It returns the last result available from the sender, or a value from a FIFO, etc. The sender will calculate new values in its own time. 





Asynchronous communications has inherent concurrency. This simply means that tasks of different features or user stories or channels or whatever can be executing in an interleaved fashion. That's why we are using it. The concurrency is at a courser grained level compared with pre-emptive multitasking. There can still be a need to lock any resources that can be in an invalid state for a time, or to think in terms of _transactions_.  


////
==== Synchronous vs asynchronous in the real world

TBD - not sure if this section helps - reread later and delete

The meaning of synchronous and asynchronous can be confusing. In the real world we don't normally think about it. It all happens naturally because we are used to it. Mostly we are asynchronous. But sometimes we are synchronous.

If we are paying for something in a store, we naturally wait for the other person to give us our change. Synchronous can operate on slightly longer time scales as when we go to the coffee machine and wait for the coffee. You could argue that this is really asynchronous, because more than likely we don't sit idle. We wipe the bench, we have a conversation with someone nearby. But consider waiting for a doctor's appointment. We basically do nothing until we are synchronised with the doctor's availability. 

On longer time scales, everything is naturally asynchronous. We start the washing machine or we send an e-mail. We don't be idle while waiting until the receiver of our e-mail is ready to receive it. The recipient reads to our e-mail in their own time. In the meantime we can do other things.

Sometimes we want to do something synchronously until completion but can't because it takes too long. We would like to finish painting the wall, but have to break for coffee or the barking dog. So we can do the job synchronously only in batches.

When its asynchronous, and if the response is not that important, it does not matter if we don't get a response, because we are not idle while waiting for it. Like a application for a job, the sender can simply send and forget. If the is a reponse, that is considered a separate asynchronous communication.

If it is important to get the response, like a payment of an invoice, the sender still does not have to be idle while waiting for it. She will generally time out and take an alternative action. Timeouts frequently come into play with asynchronous request/response messaging, especially between machines.

Asynchronous events or messages are the fundamental form. Synchronisation is an added property that involves being idle while waiting. You can be either waiting for the receiver to be ready for you, or waiting for the receiver to complete.

An asynchronous sender can behave synchronously, but not the other way around. 

If you are inherently asynchronous, then if the coffee machine is available you get it immediately. If it works instantly, then you get a coffee immediately. No waiting involved. If the coffee machine is not available, you can still be idle while waiting for it, doing nothing else. While it is making your coffee you can also be idle. 

If you are inherently synchronous though, then you can't do asynchronous. You must do nothing else while waiting for your coffee. While you wait, if someone tries to start a conversation with you, you need to say "sorry I don't do asynchronous". They would think you very strange. When you take your car in for repair and they tell you it will be ready next week, you would need to say sorry I don't do waiting. They would think you very strange.

In ALA we can take advantage of the fact that asynchronous can do either synchronous or asynchronous. If we build our sender abstractions to work asynchronously, then they can be wired for either asynchronous or synchronous. 

////


==== Wiring incompatible synchronous/asynchronous ports

Generally ALA can use both asynchronous and synchronous execution models in its programming paradigms. It does not have rules for when to use one or the other. The design choices remain more or less the same as in non-ALA applications according to real-time factors discussed above. 

However, ALA is all about abstractions and zero coupling at design-time. It would be good if the abstraction didn't need to know whether the external communications beyond their ports is going to be asynchronous or synchronous. We would like to decide that when we wire instances of them up. It is therefore desirable that domain abstraction ports that generate events and ones that listen to events can be wired for either synchronous or asynchronous execution. That way, for example, they can be wired synchronously by default for best efficiency, but asynchronously if they are in different locations, or if the recipient will take a long time.

===== One directional case

A sender port that is strictly one way can be coded to be synchronous and still be used asynchronously. The receiver can be either synchronous if the operation is quick, or asynchronous if the operation takes time. Either way the call returns quickly so that the sender is never blocked. 

If it is strictly one way, we are not interested in the function call return value or its return timing. By _strictly_ it means that the sender is zero-coupled with the reactions to the communication. It doesn't care if it executes before our own next line of code or after. 

In the example code at the beginning of this chapter, the domain abstractions did not change when we did the asynchronous version. But the order of output of system did. One was "123", and the other was "132". The application has knowledge of this order, but not the domain abstractions themselves.

If a certain domain abstraction needs to make an assumption that the next line of code executes after the call must execute after the effects of the call, then that abstraction knows something about the outside world. It isno longer an abstraction. It is probably orchestrating a side effect of some kind. It would need to be written differently and not use one-way communWhatever that orchestration is, it needs to be factored out into a higher layer where it will become cohesive code.


===== Two-directional case

The two-directional, synchronous, case is familiar to us because it can be implemented with the common and elegant function call mechanism of the CPU. 

Although a 2-way communication port can be implemented as a function call in the execution sense, in ALA it is always indirect. The function is always in an interface. The requester always has a reference to the reresponder, cast as the interface. The reference is always determined and set by the wiring in a higher layer. The interface itself always comes from a lower layer and is always more abstract, representing the request/response programming paradigm.

The subroutine call instruction can be thought of in this way: it passes both the request message and the CPU resource to the responder, and receives both the response message and the CPU resource back to the requester when done. 

This allows the lines of code that are to be executed following the request/response completion to be written immediately following the call (direct style). We are so used to this that we take it for granted. But its actually a clever and elegant mechanism provided by the subroutine call instruction. Because of the convenience of this mechanism, the synchronous function call dominates as the default way to implement request/response in conventional code.

But the synchronous function call causes problems as soon as the function takes real time. For example, the responder may need to wait for input/output. Or, it may be in a different location or processor. Or it may have to delay. It will block the thread. Unlike the more fundamental one-way cases discussed earlier, if we want to use the CPU to do other work in the meantime during a real-time 2-way communication, life gets tricky in one way or another. 

Unlike the one-direction case, a port cannot support both synchronous and asynchronous. Here are two example interfaces for synchronous and asynchronous respectively. For the asynchronous one, we have used callbacks because they are easy to understand, but there are other better mechanisms as will be discussed shortly.

.IRequestResponse.cs
[source,C#]
....
namespace ProgrammingParadigms
{
    interface IRequestResponse<T,R>
    {
        R Request(T data);
    }
}
....


.IRequestResponseAsync.cs
[source,C#]
....
namespace ProgrammingParadigms
{
    public delegate void CallbackDelegate<R>(R data);

    interface IRequestResponseAsync<T,R>
    {
        R Request(T data, CallbackDelegate callback); <1>
    }
}
....

<1> For the asyncronous version of the interface, the request passes an additional parameter, the function to be called on completion.

Given that for 2-way communications, the interfaces for synchronous and asynchronous are different, you cannot directly wire a synchronous port to an asynchronous one or vice versa.

Ideally we would like to be able to wire instances of domain abstractions together without regard to whether the ports are synchronous or asynchronous. And we would like to be able to wire synchronous ports with  asynchronous wiring inbetween when we want to (for when they are on different processors.)

The only way to get this type of compatibility is for all senders to be asynchronous by nature. Asynchronous senders can work with either synchronous or asynchronous destinations. They can also work with asynchronous wiring (or synchronous wiring, provided the destination is synchronous). 

Unfortunately, making senders asynchronous by nature means not using the function call mechanism.

A domain abstraction with an asynchronous output port needs to have a callback function:


.Sender.cs
[source,C#]
....
namespace DomainAbstractions
{ 
    public class Sender
    {
        private IRequestResponseAsync<string,string> output;

        public void DoSomething()
        {
            output.Request("message", Callback);
        }

        public void Callback(string returnMessage)
        {
            Console.WriteLine(returnMessage);
            // next operation
        }
    }
}
....



Of course, such a sender port can be wired directly to an instance of any domain abstraction implementing IRequestResponseAsync.

But the sender can also be wired to any domain abstraction implementing IRequestResponse (via a small intermediary object). The sender doesn't care whether the callback is called back asynchronously or synchronously in the outgoing output.Request() call. Similarly if we had used a Task or Promise or async/await, it doesn't care if the Task or Promise already in the _complete_ state when it is returned.

Here is the intermediary object that needs to used when wiring an asynchronous port to a synchronous port:


.IRequestResponse.cs
[source,C#]
....
public class RequestResponseAsyncToSyncIntermediary<T,R> : IRequestResponseAsync<T>, // input
{
    private IRequestResponse<T,R> output;

    void IRequestResponseAsync<T,R>.Request(T data, CallbackDelegate<R> callback)
    {
        R returnValue = output.Request(data);
        callback(returnValue);
    }
}
....



We can't wire a sender with a synchronous port to an asynchronous destination. If we did, the call would return immediately without a result.


In summary, to have domain abstractions with two-way ports zero-coupled with respect to synchronous/asynchronous communications, the senders need to be asynchronous by nature.

Receivers with asynchronous ports can behave synchronously, but not the other way around. 

If instances of any two abstractions are connected within the same processor they can both behave synchronously from a performance point of view. If instances are on different processors, asynchronous middleware can be easily wired in. 


===== Making sender 2-way ports asynchronous

Unfortunately, if you make all your domain abstractions that have 2-way requester ports asynchronous so that they are compatible with either asynchronous or synchronous responders, they must be written in the 'coding style' of asynchronous. While never impossible, this can be seriously awkward. 

Mechanisms for asynchronous (2-way) calls include 

* using two separate one-directional calls, one in each direction (This is harder in conventional code, because you need to avoid circular dependencies. It is easy in ALA but requires two wirings. Intuitively a bi-directional port should need only one wiring.)
* callbacks
* coroutines or protothreads using Duff's device
* a promise or task object that will later have the result
* continuations
* async/await
* a state machine (a _complete_ event is sent back to the machine)

We will cover most of these below, but first we need to know about direct programming style.

===== Direct programming style

The problem with some of the mechanisms for asynchronous coding is that they don't allow direct programming style. Direct style is when you can do successive operations with successive statements in a with simple syntax. For example, consider the following direct style synchronous code (which will block the thread):

[source,C#]
....
    RobotForward(7);
    Delay(1000);
    RobotTurnRight(90);
....


Using callbacks, it gets unwieldy: 


[source,C#]
....
    void Step1() {RobotForward(7, Step2);}
    void Step2() {Delay(1000, Step3);}
    void Step3() {RobotTurnRight(90, null);}
....

And with anonymous callbacks, even more unwieldy because of increasing indenting at each step:

[source,C#]
....
    RobotForward(7,
        ()=>Delay(1000,
            ()=>RobotTurnRight(90)
        )
    );
....


That's why some of the mechanisms listed above go to great lengths to allow direct programming style.

But even if you settle for callbacks or a state machine, at least it only affects code that is written inside a single domain abstraction where it is contained.

===== Prescriptive and reactive styles

Callbacks or state machines have the advantage of not committing to _prescriptive style_. Prescriptive style means that we know what we expect to happen next. That's why we want to use direct style so we can put what we expect to do next in the following statement. 

But if something different may happen, then we want _reactive style_. We want to react to whatever events may happen in the meantime. In general we want to retain the flexibility to be reactive because during maintenance we learn about less likely scenarios.

Reactive style means we can easily add handling of unforeseen events to the code. There will almost always be a need to handle timeouts in abstractions because we don't know to whom the ports will be wired. If they are wired asynchronously across an unreliable network, or to an external device, a timeout will likely be needed. Or, if something arrives on a different port while we are waiting for an asynchronous function, we will want to handle that. And we may want to abort the asynchronous communication. Callbacks and state machine handle these kinds of situations easily and naturally because the CPU is not stuck at one point in the code.

What we really want is the direct style of a multithreaded solution, and the reactive style of callbacks or state machines. They are not necessarily mutually exclusive.



===== Asynchronous execution models

What asynchronous execution models all have in common is they use a synchronous call for the forward direction that always returns immediately, and possibly without a result. It must return all the way back to the main loop so that the thread can do other work. The response comes back later in some other way. 

There are several ways to handle the response:

====== async/await

If you have *async*/*await* available in your language, it is by far the best way to write asynchronous style code:

[source,C#]
....
    await RobotForward(7);
    await Delay(1000);
    await RobotTurnRight(90);
....

If the Task object returned by any of the function calls is not complete, the CPU returns (from the containing function) at that point so it can do other things in the meantime. When the task is complete, the CPU magically returns to the point of the await to resume execution. 

*await* gives you the benefits of direct style, needing only the addition of the keyword *await* on every asynchronous call (and the addition of the *async* keyword on the containing function).

*await* also gives you the benefits of reactive style. While the code waits for the response to an asynchronous function call, other code in the abstraction can still react to other incoming or internal events. If the waiting asynchronous function call needs to be cancelled, this can be done using a cancellation token. The await will release and you can use exceptions to change the course of the prescriptive part of the code.

async/await keywords must be put on every function in the call stack back to main. Apart from that, the direct style code looks syntactically the same as a synchronous function calls. But under the covers it is not - the compiler transforms the code into a state machine.
 
When an asynchronous call (using the await keyword) executes synchronously at the responder end, the task object that is returned by the call has a completed status and a return value already, and so awaiting on it simply causes execution to continue immediately with the next statement as if it was a synchronous call.
 
When an asynchronous call executes asynchronously at the responder end, the task object that is returned does not have a return value and a completed status. The requester async function returns immediately at the point of the await without executing the statements following the await. When the task object status changes to complete, the statements following the await then magically resume with the functions's context all restored.

The code following the await is actually compiled as callback function, but the syntax is such that it looks like direct style. It's the best of both worlds, however its confusing when you are new to it, because functions marked with async do not behave like normal functions.

Async/await is the best addition to programming languages since objects.


====== State machine

Consider if the requester is better written as a state machine. If the requester is mostly reacting to events anyway, it might be best viewed as a state machine. The requester sends an event out the port and puts itself in a state for handling a response event. This solution is more flexible because it can also handle any other events that might happen in the meantime, or even instead of the response, such as a timeout. The response comes back on the port as an event for the state machine.  

If the requester is not so much reacting to events but prescribing the order that things happen, then a state machine will be awkward, especially if the requesting function is nested in loops of other functions. In this case we want the direct style (that looks syntactically like a synchronous function call). Direct coding style allows the code that follows the request call to go immediately after it rather than in a different function. 


====== Coroutines or protothreads.

In C code there are mechanisms such as coroutines and protothreads that use macros that make the code style direct. Under the covers the macros make switch statements that work as a state machine.


====== Callbacks

The requester can pass a callback function reference to the responder. When the responder has processed the communication it calls the callback function.

This can be a workable, albeit not entirely elegant, solution. The function containing the call to the asynchronous port is split up into two smaller functions, which is not great if direct style code would express the solution better. Also local variables or parameters that would have been in the original function now end up as globals to be shared by the multiple functions. You can't put callback functions in a loop or another statement or inside another function, so such structures have to be split up also, and effectively made to work as a state machine.  

The request call will be at the very end of the function that contains it. This is so that it returns to the main loop when the request call immediately returns (tail call). The callback function immediately follows this function so that the flow is still relatively clear.

Finally, the callback function could be passed by the request call as an anonymous function. However this involves much nesting of brackets and indenting for successive callback functions. This is called triangle hell. If there is more than one such request/response in a row, these nestings will quickly become unreadable. I find named functions following each other is clearer.

====== Tasks, Futures, Promises

Without going into the detailed differences between futures and promises (the terms get mixed up anyway), this approach is more modern than callback functions.

The requester makes a synchronous call on the receiver which immediately returns with an object known as a future. The future object will have the result in it in the future. You can save a reference to the object, do something else in the meantime, and check it periodically.

The future can contain a continuation function, which is essentially just our previous callback function idea. 

The future may contain a continuation function reference which gets called when the result is ready.


====== Pairs of ports

Finally, request/response could be implemented asynchronously by having pairs of ports on each of the requester and responder and having two wirings, one to carry the request and one to carry the response. Both can be synchronous pushes in themselves, but the overall wiring is request/response. 

Doing function calls in both directions is usually avoided in conventional programming because it would involve circular dependencies. But in ALA its just wiring, so it is quite feasible.

Sometimes, it turns out that what would be request/response function calls in conventional code are really best written without request/response at all.

Let's have a look at an example:


[source,C]
....

void main()
{
    while (true)
    {
        data = Scale();
        Display(data);
        delay(1000);
    }
}


float scale()
{
    data = Filter;
    return = data*0.55 + 23.2;
}


float Filter()
{
    static float state = 0;
    data = Adc(channel=2);
    state = data*0.12 + state*0.88;
    return state;
}


float Adc(int channel)
{
    ...
}


void Display(float data)
{
    ...
}

....

The function main requests data from the adc at intervals via two functions which processes the data during the return trip. Main then pushes it to a display.  

The functions main, scale, filter and adc are chained using request/response implemented as function calls.

(The scale and filter functions being chained may look strange to some because they are so obviously abstractions. But add a few more application specific details to them and I have seen plenty of conventional code that chains function or method calls through multiple  modules or classes like this.)  

The main function is not abstract. Not like the ideas of adc conversion, filtering, scaling or displaying. It's code that's deciding when to read the ADC and then passing the processed result to the display. In other words, it's specific to the application. Also, in the chain of function calls, the chaining itself is specific to the application.

So let's get closer to ALA by pulling out the application specific bits into an abstraction in the application layer.

[source,C]
....
void main()
{
    while (true)
    {
        data = Adc();
        data = Filter(data);
        data = Scale(data);
        Display(data);
        delay(1000);
    }
}

....

It's almost ALA compliant, but the application is handling data a lot at run-time. Handling data is not an application specific detail. It's a very common implementation detail, so its done at the wrong abstraction level. The passing of data from abstraction to abstraction at run-time is the idea of dataflow, and it's quite abstract so it should go into a layer below the domain abstractions. 

Also the loop is a common implementation detail that doesn't belong in the application abstraction. We wnt the application to just be a composition of the 'ideas' of adc, filter, scale, display and clock. Something more like this: 

[source,C]
....

void main()
{
    new Clock(1000)
    .WireTo(new(Adc(channel=2))
    .WireTo(new Filter(0.88))
    .WireTo(new Scale(0.55, 23.2))
    .WireTo(new Display());
}
....

That's our target code. Let's see how to get there from the while loop code.


First let's switch to diagram form. Lets use the request/response programming paradigm used by the original code so that it closely mimics the function calling execution model of the main loop version.

[plantuml,file="diagram-clock-adc-filter-scale-display1.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="3"
graph [rankdir=LR]
node [shape=Mrecord]
Main [label="<f0> Pump|<f1> period = 1000"]
Adc [label="<f0> Adc|<f1> channel = 2"]
Filter [label="<f0> Filter |<f1> strength = 0.88"]
Scale [label="<f0> Scale |<f1> Offset = 23.2 |<f2> Slope = 0.55"]
Main -> Scale -> Filter -> Adc
Main -> Display
{rank=same Display Scale}}
@enddot
----

We've put the main loop into a new domain abstraction called Main. It pulls data from its request/response port and pushes it out on its output port at regular intervals. The execution model is working the same way as the conventional code.

The Main domain abstraction is not a great abstraction because it assumes all possible applications are just going to pump data.

Lets fix that:


[plantuml,file="diagram-clock-adc-filter-scale-display2.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="3"
graph [rankdir=LR]
node [shape=Mrecord]
Clock [label="<f0> Clock|<f1> period = 1000"]
Adc [label="<f0> Adc|<f1> channel = 2"]
Filter [label="<f0> Filter |<f1> strength = 0.88"]
Scale [label="<f0> Scale |<f1> Offset = 23.2 |<f2> Slope = 0.55"]
Clock -> Pump -> Scale -> Filter -> Adc
Pump -> Display
{rank=same Display Scale}}
@enddot
----

We have introduced a new domain abstraction called a 'Pump' that pulls data from a request/response port and then pushes it out of an output port. The pump has an input event port to tell it when to do it. Pump will also be a temporary abstraction, but lets run with it for now.

Note that the arrow between the clock and the pump is using the event programming paradigm. The arrows between the Pump, the Scale, the Filter and the Adc are the request/response programming paradigm. The arrow between Pump and Display is Dataflow (which pushes data).

Because the Adc takes real time, the pump, scaler, filter and ADC must all now have asynchronous request/response ports. So they must all be written in asynchronous style. But, if we look at the diagram, we can wonder if we really need to use request/response. Is it a left over artefact of the conventional code? 

We can see that we can lose some of the request/response ports simply by moving the Pump.


[plantuml,file="diagram-clock-adc-filter-scale-display3.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="3"
graph [rankdir=LR]
node [shape=Mrecord]
Clock [label="<f0> Clock|<f1> period = 1000"]
Adc [label="<f0> Adc|<f1> channel = 2"]
Filter [label="<f0> Filter |<f1> strength = 0.88"]
Scale [label="<f0> Scale |<f1> Offset = 23.2 |<f2> Slope = 0.55"]
Clock -> Pump -> Adc
Pump -> Filter -> Scale -> Display
{rank=same Adc Filter}
}
@enddot
----

Now the Filter and Scale abstraction uses simple push ports. 

Now let's take this one step further. The Adc abstraction is more versatile if the event that starts it does not have to come from the same place where the output goes. In other words, the Adc would be a better abstraction if it had a pair of ports, an event input called start, and a push dataflow port called output.

[plantuml,file="diagram-clock-adc-filter-scale-display4.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="3"
graph [rankdir=LR]
node [shape=Mrecord]
Clock [label="<f0> Clock|<f1> period = 1000"]
Adc [label="<f0> Adc|<f1> channel = 2"]
Filter [label="<f0> Filter |<f1> strength = 0.88"]
Scale [label="<f0> Scale |<f1> Offset = 23.2 |<f2> Slope = 0.55"]
Clock -> Adc -> Filter -> Scale -> Display
}
@enddot
----



Now we don't need the Pump. It was there just to make the request/response execution model work and wasn't providing anything useful. The application just wires the clocked event source directly to the ADC.

Now we have exactly what we wanted when we wrote that earlier code that just composed ideas. The composition now seems natural and elegant. The idea of splitting a request/response port into two separate ports has actually lead to better abstractions and a better solution overall.  


It also makes sense to split a request/response port when the requesting end is already a state machine. Waiting for the response becomes just another state, and the response becomes just another event wired back to the state machine machine.

For receiving the response, the requester has an input port and a function that implements the interface of that port. If that function makes a further request, the stack will have two returns pending, one for the original request and one for the 2nd request. Some systems use 'tail optimization' for this situation to stop the stack accumulating calls. Because request calls occur at the end of a function, tail optimisation converts the instruction from a call to a jump. 

The request/response pattern is common so we prefer to implement it as a single port on each of the requester and responder with a single wiring.



In summary, all these techniques allow us to write asynchronous requesters, which allows us to avoid using multithreading.

However there is still danger associated with these asynchronous mechinams compared with synchronous function calls. The CPU is freed up to do other work while a request that takes real time is being processed. This is still concurrency, its just not fine grained concurrency that multithreading has.  During concurrency, a shared state somewhere can be changed when you don't expect it. For example, if the requester is performing a transaction such as the canonical debit one account and credit another, the requester that was written using normal synchronous calls is safe without locking the two accounts. This is because synchronous calls effectively lock everything by hogging the CPU resource until they complete. The asynchronous version has to be worried about what else might happen between two successive request/response calls. We call this type of non-splitable operation a transaction. Transactions still need explcit locking of resources that need to be kept in an internally consistent state. This needs to happen at the user story level becasue it is the user story that understands transactions. We can deal with this type of locking by using the "Arbitration programming paradigm", which is described later.




===== Multithreading

The conventional solution for function calls that take real time is to use multithreading.

At first this seems elegant as it keeps the same direct style syntax used for function calls that are non-blocking. This has the advantage that the code in the requester is written in almost the same way whether or not the instances it will be wired to will block. There is useful design-time decoupling resulting from that - the requester does not have to know what it will be wired to. It also appears to abstract concurrency, allowing other tasks to execute while the thread is blocked.

In ALA, every instance of an abstraction containing a prescriptive routine that could potentially block would need its own thread. But unfortunately threads do not remain confined within abstractions. They have far reaching effects as they call out into other abstractions. And abstractions that need to do work while waiting on a blocking call will themselves need multiple threads. 

Because instances of abstractions do not know to whom they will be wired, they would need to assume that incoming function calls could be on a different thread. This would cause the multithreading model to have collaborative coupling between abstractions to have sufficient locking without causing deadlocks. This is the same problem for conventional classes as well, but its worse in ALA because abstraction internals must have zero design-time coupling with one another. They cannot collaborate on locking.

If a single thread is capable of doing all the work, I don't recommend multithreading for solving the problem of function calls that take real time, even if unwieldy callbacks are the only alternative. Using callbacks to implement asynchronous ports is at least contained inside an abstraction. 

Using non-preemptive multi-threading avoids race condition and deadlock problems by not requiring locks. All non-blocking sections of routines will run uninterrupted. 

Once a multithreading is available, it tends to be the solution for every concurrency problem. That tends to commit code to prescriptive style even when a state machine would be better. (Prescriptive style as opposed to reactive style was discussed above.)

If we want to abort a blocked synchronous function call, (in the same way that we can abort an await with a CompletionToken,) we could have a second method in the programming paradigm interface called Cancel(). When the interface is implemented, the Cancel function (which has to run on a different thread) must release the block at the point where it is blocked, and cause it to return. It would return with a cancelled result so that the calling thread can follow a different flow. I have not tried this programming paradigm as yet.

Of course multithreading is still a solution for _throughput_ types of performance issues. Multithreading is discussed further in a later section. 



==== Priorities

Synchronous communications are deterministic. They prescribe the order in which everything happens. Furthermore, they effectively put a system wide lock on everything until the entire function calling tree completes. Nothing else can happen anywhere until it finishes.

Asynchronous communications, on the other hand, is inherently less deterministic. The non-determinism is made necessary by the external system, things like: real-time I/O, external networks, or by the need to improve performance.

During asynchronous communications, the functions can be executed in the order in which they are scheduled (using a simple queue) by default. This is what we did in the sample code at the start of this chapter. If this ordering scheme is used, then from the point of view of an asynchronous call tree, the natural order of execution is different from the synchronous function call tree. A synchronous function call tree will be depth first, whereas an asynchronous function call tree will be width first.

During the execution of a call tree, other call trees may be executing in parallel. This does not mean parallel in the fine grained sense of multithreading. It means parallel in the course grained sense that between the execution of asynchronous functions, other functions of other call trees may execute.

One consequence of asynchronous communications is that if any resource, including any object, is left in an invalid state between the running of two asynchronous functions, it must be locked. The need for locking is much less common than in a multithreaded situation. How locking can be accomplished without introducing coupling into the abstractions by using an arbitration programming paradigm is discussed later. Locking will change the order that functions execute.

The order of execution of asynchronous functions, can also be explicitly changed using priorities. Priorities are usually used to explicitly improve performance by doing more urgent things first.

Because the order of execution is outside the control of the abstractions involved, domain abstractions should not care about when it's one-way asynchronous communications are executed. If the priority system were to reverse the order of execution of every asynchronous function in the system, a domain abstraction results should be the same (except for its performance). If the order does matter, the order needs to be explicit in some way.
For example, a domain abstraction could use a 2-way communications port so that it gets a communication back when something is complete and it can move onto the next step. Another example is to use an _activity_ programming paradigm (UML activity diagram). Abstractions have _start_ input port and a _finished_ output port. The application wires instances of them in a sequence.

Priorities are generally a system wide concern, so the application abstraction (or feature or user story abstractions) are the only ones that have the knowledge to know how to set priorities.

In conventional modular systems, priorities are usually a cross-cutting concern, but in ALA they are cohesive with the wiring code, which is already in one place for a given feature or user story. The application may need to. prioritize the features and user stories.

TBD Show example implementation code for priorities. Add an optional priority parameter to the WireTo of the asynchronous programming paradigm abstraction. 
The appplication can use priority numbers such as 0,1,2. We need a default priority so that WireTo can be called without specifying a priority. The application would configure the default, for example to 1.

The final requirement is that applications can still use the asynchronous programming paradigm without using priorities at all. There would be a default default priority level of 0.

A priority abstraction could be created in the domain abstractions layer. It would contain a dictionary for priority levels. You would not use an enum for priority level in this abstractions because the levels are specific to an application. The application configures the dictionary with level names such as Low, Middle, High, that associate with numeric priorities.

Also, we need to consider if domain abstractions may ever want to use priorities internally. If so we need to do it in such a way that they do not have a dependency on a priority abstraction because if they are used without priorities, we don't want to have to include the priority abstraction.  

////

JRS: I don't the following is well thought out. Need to do example code with priorities to see how it all works out.

===== Avoiding global priorities

Since priorities are usually an application wide concern, we would seemingly need global priority levels such as High, Medium, and Low. These might be an enum in an abstraction in the domain abstractions layer. The application layer then uses them to set priorities. However, the priorities Low, Medium and High may not be abstract enough to be reusable. Another application may need more priority levels.

Domain abstractions should not set priorities. But they could have optional priority configurations passed to them by the application. If they had to know about the enum type, the enum would be even lower in the programming paradigms layer, which is even more inconsistent with the abstraction level of specific priorities. 

So there could be a _priority abstraction_ in the programming paradigms layer, that knows about the concept of priority but not the specific priority levels. It could contain a dictionary, which the application configures with the set of levels it wants to use. 

With this design, domain abstractions would have a dependency on the priority abstraction even when they didn't use it. We would like to avoid such a dependency so that we don't have to include it in projects that won't use it. 
In applications that do use priorties, most wirings in the system will not care about the priority, and so we want to be able to have a default priority so we don't have to specify it. 

An example might be an application that has a fast real-time sensor and actuator feature. It may have other features for the user to make adjustment settings through a UI. And then it may have algorithms that analyse long term trends. All the wirings used by the fast real-time feature could be specified to have high priority. The settings features would have default priority. The long term algorithms could be given low priority.

The implementation of priorities could be done by adding an override of the WireTo extension method that takes a priority as a parameter. It would be implemented within the asynchronous programming paradigm abstraction. 

TBD: implement an example priority system, preferably with a stand-alone abstraction in the programming paradigms layer (which would need to be wired to a port on the event-loop abstraction to somehow control the order of the event list.)

////

==== Busy resources

When a resource that takes is used asynchronously, more than one user may try to use it at the same time. For example a transaction on a database may involve several asynchronous function calls, and have multiple users. It would need to be locked for the duration of the transaction. Or an ADC converter that takes time to do a conversion may be used by multiple users. It would be busy to new requests while it is performing a conversion. If the resource is busy, the communication to the resource will need to be queued until it is ready.

The reactor pattern can handle this situation. It can check if the receiver is busy before giving it the communication. The dispatcher wont remove asynchronous function calls from the queue unless the destination resource is ready for it.

If a simple event loop is used, a solution to this problem, is an intermediary object that is wired in front of the resource. It keeps its own queue of event objects. When the resource signals that it is free, it takes the first event from the queue and sends it to the resource via the main event loop. That way only one event at a time can be in the event loop's queue.







=== Example Programming paradigms 

In the previous section of this chapter, we discussed many aspects of execution models in general. Many were applicable to both event-driven and 1-way dataflow programming paradigms. 

Nest we will look at some particular programming paradigms and see how their execution models might work. It is not an exhaustive list. There are no doubt many other possibilities waiting to be invented that have new meanings for the composition of abstractions, and allow succinct expression of requirements. 


=== Request/response

A common type of 2-way communication is request/response. This programming paradigm is fundamentally an orchestration of two one-way messages, but we are used to thinking of it as a fundamental communication pattern in its own right. That's because it's implemented so easily with a common function call. 
Earlier in the chapter we observed that if requesters were asynchronous by nature, they would have wiring compatibility with either synchronous or asynchronous receivers. And we discussed ways of writing requesters to be asynchronous. 

A request carries two types of implicit information. Firstly, since they are wired point to point, a request is implicitly a command. It doesn't need any command name or any explicit data specifying a command. Secondly a request and a response implicitly carry timing information. The time that they occur is in itself information.  

Examples of request/response:

* The requester needs to know when it's completed (before it continues with the next line of code).
* The requester needs to know a success or failure status of a command.
* The requester needs to request latest information (pull) (e.g. from an I/O port).
* The requester needs to request lazy information (information not calculated until its needed).
* The requester needs to request specific information e.g from a database.


==== Wiring incompatible request/response ports


As discussed in earlier sections, synchronous and asynchronous 2-way communications have different advantages. There is a principle, GALS, that suggests that we use synchronous locally (within a processor) and asynchronous globally across processors. I think this is too simplistic. There are reasons other than cross-processor communications that cause certain communications to take real time, such as IO or delays. These communications should be asynchronous, and then all the ones that might be wired to them need to be asynchronous as well. Nevertheless there may be some communications in the average application which needs the advantages of synchronous communications. 

The request/response ports of domain abstractions may end up a mixture of synchronous and asynchronous.

If the requester is asynchronous and the responder is synchronous, there is little problem in connecting them using an intermediary object. When the requester calls the intermediary, the intermediary in turn calls the responder which returns immediately. The intermediary then places the result in the task or future object, or calls the requester back if it uses a callback.

If the requester is synchronous and the responder is asynchronous, it would be possible to create an intermediary adapter, but it will block the requester's thread, which probably isn't what we want. The requester would need its own thread (or its requester), which, as I said earlier I don't recommend as the way to solve this problem. So they are essentially incompatible. The requester code would need to change to asynchronous, as described by one of the methods above. 




=== Event-driven programming paradigm

We now return to the 'Event driven' programming paradigm. At the beginning of this chapter we showed both synchronous and asynchronous code examples of this paradigm, both of which used the IEvent interface.

'Event' is an overloaded term in software engineering. Sometimes it means asynchronous, as in using an event loop. Sometimes it means indirect, as in C# events. Sometimes it means both. Earlier in this chapter we clarified these two independent notions. We discussed that in ALA, communications between abstractions within a layer are always indirect and explicit. We also discussed that they may be either synchronous or asynchronous. And we discussed 1-way and 2-way communications.

The interpretation of event-driven that I use is asynchronous and 1-way. Of course it's always indirect and explicit in ALA.

Note that this interpretation is different from the C# language version of events. C# events are synchronous (they get delivered and processed before the function returns). C# events also directly support fanout. C# receivers are usually registered by the receiver itself (observer or publish/subscribe pattern). In ALA of course, events must be wired by a layer above.

In my interpretation of the event-driven programming paradigm, output ports can only be wired point to point. You would use a fanout intermediary object to achieve wiring an event to multiple destinations.

Even though my interpretation of even-driven is asynchronous, the output ports use a function or method call. This is fine because they are 1-way communications. The function gets the event on its way and returns immediately. The return itself carries no information.

When an event is taken from the event queue and dispatched to the receiver, we call it a task. The task is just the execution of a function or method, (which is different from a C# task object). A task must always runs to completion quickly. No task should take real time to execute (spin loop, or block).

==== Events with parameters

Another section of this chapter discusses the dataflow programming paradigm. Dataflow can be similar to event-driven with a parameter when it pushes data. However Dataflow has variants where can be synchronous, can be pull rather than push, and can send a whole table of data in batches. For this reason Dataflow and event-driven are considered different programming paradigms. 


==== Reactive vs prescriptive programming

Event-driven programming is a _reactive_ style in that it contrasts with the _prescriptive_ or _orchestrated_ style of the imperative or activity programming paradigms. In event-driven, the system is idle until something happens, and then things react to it, possibly changing some state, possibly generating more events, completion events, or timeout events. Event driven systems like to use interrupt routines to get events from the outside into the system. The interrupt routine puts the event directly into the main loop event queue. 

In a reactive system, we don't know what will happen next, in either the outside world or what code will execute next. It is less deterministic. Reacting to an event often changes some stored state. This state may change the way we will react to subsequent events. In other words, event-driven often goes hand in hand with state machines. 

Event-driven programming is generally not thought of as a request/response type of paradigm. There can be a response, but it would be thought of as a completely separate message that needs its own point to point wiring. We don't need synchronous communications because there is no response associated with an event (in the same wiring).

ALA is polyglot with respect to programming paradigms, so there is no reason to try to make an entire system either event-driven or prescriptive. Both can be mixed for maximum expressibility of the requirements.

When there are no forces favouring reactive or prescriptive, I generally default to reactive. This is because reactive systems are more versatile in maintenance. A prescriptive style becomes awkward when an unforeseen event needs to be handled in the middle of a prescriptive routine. The flow of the routine becomes more complicated.


==== Properties of event-driven designs

* Event-driven design easily accommodates events happening externally to the system at unpredictable times. We may be busy processing a previous event when a new events occurs. We typically have an interrupt put the event into the asynchronous event queue. When we are ready to process the event, we may still want to process higher priority events first.

* Long running tasks such as a heavy algorithm or updating a large display may cause issues with latency for other events. They need to be split into a series of tasks, usually at the outer loop. The loop state needs to be coded manually as a state machine. The C# 'yield return' keyword will tell the compiler to do this for you.

* Event driven systems need a Timer abstraction to be provided in the programming paradigms layer. The Timer can be asked to issue an event at a future time. It can be asked to issue events at regular intervals. 

* Wiring in ALA may be circular. There is no problem with this from a dependency point of view. Since event-driven is asynchronous there are no issues execution wise either. If they were synchronous, there would be recursion and an infinite loop. Events may flow around the circle continuously. If there are no delays around the circle, the main loop will be constantly busy processing the events as fast as it can.
+
Events in a loop should not fan out. Events in a loop that reproduce more events will overload the event queue.

* The reactor pattern can be used for when the receiver is not ready. The reactor pattern is an event loop which will check if the receiver is in a ready state before dispatching any events to it.

* A developer used to a synchronous function calling style may expect what looks like a synchronous function call inside a domain abstraction to fully process the event before returning. The port itself will show that it uses an IEvent interface. Inside the IEvent abstraction it can explain that it is an asynchronous programming paradigm. However, where the code actually sends the event, it will only have output.Send(); The choice of the word _Send_ rather than _Execute_ is to indicate it's only sending the event not executing it.

* Because the event-driven programming paradigm is asynchronous, senders and receivers can be on different processors or different locations. The decision about where instances of domain abstractions run can even be after the application or user stories abstractions are written. This means that within the architectural 4+1 views framework, the physical view can be changed independently of the logical view. 


==== Global event names

Some conventional event-driven systems use global event names for inter-communication between modules. Each receiver names the events it is interested in, effectively a variation of the observer or publish subscribe pattern. They do this by registering to global event or signal names. This is considered relatively decoupled by its proponents, because senders and receivers don't know directly about each other, only about global events names. It is illegal in ALA because most events will not abstract enough to be named and become globals. They will tend to be specific to pairs of modules that need to communicate.

Event names then essentially become symbolic wirings. Symbolic wiring is difficult to follow because you have to search for where the names appear throughout the entire code. 

By effectively collaborating on symbol names, abstractions are coupled with each other still. It's a rigid system because modules could not be rewired in a different way without changing them.  

In ALA we use point to point wiring instead, or should I say port to port. Wiring is brought out to a coherent place. Because the wiring is point to point, the events are anonymous. You don't have to name the lines on a diagram.  

Having said that, it is possible to have an event that is abstract enough to go into a layer below. Such an event would need to be used by many many domain abstractions so that it is truly more abstract. If only a few domain abstractions need to use an event, then they should still use ports and be all wired up.

If you do create a global abstract event, it would be so ubiquitous that you never want to use the domain abstractions without it. They will have a dependency on it after all.

I can't think of an example of such an event. Perhaps an event called _initialize_. It is generated after the wiring code has executed but before an application is set running. Domain abstractions use it to do initialization that needs the wiring in place.

Another example may be a _closing_ event, giving domain abstraction instances a chance to persist their context data before the application closes down. 



=== Dataflow

A dataflow model is a model in which wired instances in the program (or connected boxes on a diagram) are a path of data without being a path of execution-flow. The execution flow is like in another dimension relative to the data flow - it may go all over the place.

A stream of data flows between the connected components. Each component processes data at its inputs and sends it out of its outputs.

Each input and output can be operated in either push or pull mode. Usually the system prescribes all pull (LINQ), all push (RX), all inputs pull and outputs push (active objects with queues) or all outputs pull and inputs push (active connectors). In ALA we can use a mix of these different mechanism when we define the programming paradigm interfaces.

The network can be circular provided some kind of execution semantic finishes the underlying CPU execution at some point (see synchronous programming below).

The dataflow paradigm raises the question of type compatibility and type safety. Ideally the types used by the components are either parameterised and specified by the application at each connection or determined through type inference.  


==== IDataFlow<T>

I frequently use dataflow execution models.

Here is one variation which works well:

TBD


This variation has these properties:

* On a diagram, the line (wire) represents a variable that holds the value.
* Fan-out - one output can connect to multiple inputs. All inputs read the same output variable.
* Fan-in - multiple outputs cannot connect to one input.
* Each output is implemented by a single memory variable whose scope is effectively all the places connected by the line (wire).
* Receivers can get an event when the value changes
* Receivers can read and re-read their inputs at any time.
* Operator don't need to have an output variable, they can pass the get through and recalculate every time instead. 

Here is the version I use most often.

TBD


Note that domain abstractions may not collaborate on a specific type for T. A pair of domain abstraction may not, for example, share a DTO (data transfer object) class as that would then be an interface specific to one or other of those classes. T must be more abstract and come from a lower layer, so is often a primitive type from the programming language. T may be passed in by the application, which always knows types of data moving through the system. 

Type inferencing is desirable. For example, an instance of a _DataStore<T>_ abstraction could be configured by the application to have some specific fields. Ideally this is the only time the application specifies the fields. The application wires it to a _select_ abstraction that removes one field and then to a _join_ abstraction that adds one field. From there it is wired to a _form_ abstraction that displays the fields. Ideally the form, select and join abstractions do not also have to be configured by the application to know the types of their ports. Instead they are able to infer the type as an anonymous class as it goes from port to port at compile-time.  


==== ITable

This interface moves a whole table of data at once. The table has rows and columns. The columns are determined at runtime by the source. 

Run-time types can also be used. For example, the fields in an instance of a table abstraction may not be fully known at compile-time. This is especially true if the table abstraction provides persistence, or, for example, if the data source is a CSV file with unknown fields. In this case a ITable programming paradigm would transfer type information at run-time as well as the data itself.


TBD implementation examples


==== Glitches

All systems can have glitches when data flows are pushed in a diamond pattern. The diamond pattern occurs when an output is wired to two or more places, and then the outputs of those places eventually come back together. If they never come together, even both seen by a human, then we generally don't care what order everything is executed in. But when they come together, the first input that arrives with new data will cause processing, and use old data on the other inputs. This unplanned combination of potentially inconsistent data processed together is a glitch. It even happens in electronic circuits.

The following composition of dataflow operators is meant to calculate (X+1)*(X+2)

[plantuml,file="diagram-25.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="2!"
graph [rankdir=LR]
node [shape=Mrecord]
Add1 [label="<f0> Add|<f1> 1"]
Add2 [label="<f0> Add|<f1> 2"]
D [style=invis]
E [style=invis]
F [style=invis]
D -> X [style="invis"]
X -> Add1
X -> Add2
Add1 -> Mul
Add2 -> Mul
Mul -> E [style="invis"]
E -> F [style="invis"]
}
@enddot
----

When X changes, there can be a glitch, a short period of time, in which the output is (C~new~+1)*(C~old~+2).

In imperative programming, this problem is up to the developer to manage. He will usually arrange the order of execution and arrange for a single function or method to be called at the place where the data-paths come back together. As he does this, he is introducing a lot of non-obvious coupling indisde the modules of the system, which is one of the big problems with imperative programming.

When we have composability, we don't know inside the abstractions how data will propagate outside, and how it will arrive at its inputs. We want to execute whenever any of our inputs change, because as far as we know it may be the only change that might happen. So we really want the execution model to take care of eliminating glitches automatically for us.

This is a work in progress for the IDataFlow execution model described above.
In the meantime, as a work-around I take care of it at the application level using a pattern. When I know dataflows will re-merge in a potentially inconsistent manner, I wire in an instance of an abstraction called 'Order' between the output and all its destination inputs. This instance of order is configured to explicitly control the order that the output date stream events are executed in. Then I will use a second abstraction called 'EventBlock' at the end of all data paths except one, the one that executes last.    

[plantuml,file="diagram-26.png"]
----
@startdot
digraph foo {
# edge [color=green]
size="2!"
graph [rankdir=LR]
node [shape=Mrecord]
Add1 [label="<f0> Plus|<f1> 1"]
Add2 [label="<f0> Plus|<f1> 2"]
X -> Order
Order -> Add1 [label="1"]
Order -> Add2 [label="2"]
Add2 -> Mult
Add1 -> EventBlock
EventBlock -> Mult
{rank=same Add1 Add2}
}
@enddot
----
By default multiple IDataFlows wired to a single output are executed in the order that they are wired anyway. On the diagram, they are drawn top to bottom in that order.  This improves the determinism but is a little too implicit for my liking, so that is why I use the order abstraction.


==== Live dataflow

As used in the coffee-maker example earlier, this paradigm simulates electronic circuits instead of using the concept of discrete messages. Semantically the inputs have the values of the outputs they are wired to at all times. This type of flow is readily implemented with shared memory variables.

FRP (Functional Reactive Programming) also is effectively a live dataflow execution model.


==== Synchronous dataflow

The use of the word synchronous here is different from its use in the discussion of synchronous/asynchronous events above. Here it means a master system clock clocks the data around the system on regular ticks. At each tick, every instance latches its own inputs and then processes them and places the results on their outputs. Data progresses through one operator per tick, so takes more time to get through the system from inputs to outputs. The result is a more deterministic and mathematically analysable system. 

The execution timing and the timing of outputs occurs at a predictable tick time, albeit on a slower time scale than an asynchronous system. All timings are lifted into the normal design space.

Glitches that could occur in an asynchronous system (discussed earlier) are eliminated at the level of single clock ticks. A fast glitch could not occur. A glitch would occur when different data paths had different lengths, and would last for at least one tick duration. Controlling glitches is therefore lifted into the normal design space.



=== Activity-flow

The name Activity-flow comes from the UML activity diagram. Activities that are wired together execute in order. One starts when the previous one finishes. The activity itself may take a long time to complete (without blocking the CPU). Activity flows can split, run concurrently and recombine. 

Activity-flow contrasts with event-driven. Where event-driven is reactive, activity-flow is prescriptive. It orchestrates what will happen rather than reacting to what might happen.

Activity-flow is not the same as the old flow diagrams. Flow diagrams were for the imperative programming paradigm where the flow was the flow of the CPU. Activity flow can have delays and other time discontinuities as it syncs with what's happening in the outside world.

Activity-flow's execution model can be the same as event driven. Each domain abstraction has a _start_ input port and a _done_ output port. The 'done' port of one instance of a domain abstraction can be wired to the 'start' port of the next. The ports are just event ports and can be wired for synchronous or asynchronous execution.

If the Activity-flow is a linear sequence, we can consider wiring the instances using text. However activity-flow abstractions will often need other wiring (using other programming paradigms) to UI or other input/output. C
The domain abstractions may have request/response ports for their I/O. These may be synchronous or asynchronous depending on the design factors discussed earlier. It may wish to poll something external at regular intervals to see if it's complete, so it may register on a timer for regular events. (The timer is an abstraction in the programming paradigms layer, which is typically wired to the event-loop abstraction for asynchronous execution).

The domain abstractions may internally use an asynchronous execution model, such as for a delay. 




==== Structured activity flow wiring using text (experimental)

This is a thought experiment at this stage.
The experiment is to see if we can do structured programming for activity flow.
Remember activity flow is instances of domain abstractions, each of which generally has a _start_ port and a _done_ port.

The idea is to mimic imperative structured programming. Structural programming is what got rid of the goto and introduce block structured statements such as while and if. It is generally laid out with indenting that exactly matches the nested structure of braces. Your brain sees the indenting but the compiler sees the curly braces. (Except for Python which makes the compiler use what the brain sees).

In this program, we will string together some instances of domain abstractions and include a loop and a conditional. The indenting structure is the same as for the imperative version.

Remember this code is not executing the activity flow, it is just wiring it all up for later execution.

TBD need the corrsponding diagram here to show what this code is trying to do

.ActivityFlow.cs
[source,C#]
....
program = new A();
    program.
    .WireIn(new B())
    .WireIn(
        Loop (
            new C()
            .WireIn(
                If (new D(),
                    new E(),
                    new F()
                )
            )
            .WireIn(new G())
            ,
            new H();   
        )
    )
    .WireIn(new I())
....

First remember that WireIn returns its second parameter to support this fluent style.
A is the first activity.
A's done port is wired to B's start port, so B is the second activity.
Everything else is in a loop.
The 'Loop' function takes two parameters, one is another flow and one is the looping condition, which in this case is H.
B gets wired to C.
'If' is a function that takes three parameters, a condition, which in this case is D, and two flows.
C gets wired to D.
The 'If' function expects D to have two done ports, called donetrue and donefalse.
It wires donetrue to E.
It wires donefalse to F.
'If' wires the done ports of both E and F to a null activity instance to recombine the flow. 
The null instance is returned by 'If'.
The null instance is wired to G.
G gets wired to H.
The 'Loop' function expects H to have two exit ports calls done and loop.
'Loop' wires H's loop port to C, and returns H.
H is wired to I.

This code looks okay, however, as is often the problem with text based representations of relationships, most of the instances will probably need additional wiring to other things as well. If this is the case, and the requirements implicitly contains a graph structure rather than a tree structure, then a diagram wll be the best way to represent it. 



=== Work-flow

Persisted Activity-flow. This includes long running activities within a business process such as an insurance claim.


=== IIterator

This dataflow interface allows moving a finite number of data values at once. It does so without having to save all the values anywhere in the stream, so has an efficient execution model that moves one data value at a time through the whole network.

This is the ALA equivalent of both IEnumerator and IObserver as used by monads. ALA uses the WireTo extension method that it already has to do the Bind operation. So the IIterator interface is wired in the same consistent way as all the other paradigm interfaces. There is no need for IEnumerable and IObservable type interfaces to support Also unlike monads, multiple arbirary interfaces can be wired between two objects with a single wiring operation.

IIterator has two variants that handle push and pull execution models. Either the A object can push data to the B object, or the A object can pull data from the B object. 

TBD implementation examples



=== UI layout

This programming paradigm is used for laying out a graphical user interface. A relationship means put the target instance of a UI element inside the first instance of a UI element. The order of the fanout of relationships sets the order that the elements appear. For most UI domain abstractions, UI elements default to going go vertically downwards.

I use two domain abstractions called vertical and horizontal to control whether they are layed out vertically or horizontally.

Here is the interface for use by domain abstractions that will use .NET's WPF class library for the implementation.

.IUI.cs
[source,C#]
....
using System.Windows;

namespace ProgrammingParadigms
{
    /// <summary>
    /// Hierarchical containment structure of the UI
    /// </summary>
    public interface IUI
    {
        UIElement GetWPFElement();
    }
}
....


This programming paradigm is similar to XAML. It doesn't use XML syntax, it uses wiring code or diagrams the same as all other programming paradigms. Binding to data in XAML is done using dataflow ports on domain abstractions. Unlike XML, the entire application is built the same way.



=== UI navigation flow

TBD

=== Data schema

TBD

=== Locking resources



Even in a single threaded system, we still have concurrency at a course grained level. We want to allow our one thread to do other tasks whenever something else is waiting. Or, whenever an asynchronous communication occurs, we may choose to do previously queued tasks, or higher priority tasks, before processing the latest one. We can call the concurrent sets of tasks an activity.

We may have a resource or external device that can be be used by multiple activities. There is a set of tasks that need to complete on the resource without interrupton by other activities. This is called a transaction. Examples of resources that can have transactions are a database or an external device such as a robot arm. Several queries or movements may be involved in the transaction. 

We need a locking mechanism for the resource. I recommend an arbitration programming paradigm. At the application level, we need to specify which instances of domain abstractions that perform transactions need to collaborate by locking or waiting for a given resource. 

Every domain abstractions that performs a transaction on a given resource has a port of this programming paradigm. All instances using a given resource are wired to a single instance of an arbitrator abstraction. Effectively this wiring specifies the collaboration that must occur between the instances. This collaboration is done at the abstraction level of the system, where it belongs, not inside the abstractions.

The ALAExample project at www.github.com/johnspray74 has an example of this. The IArbitrator interface is considered a programming paradigm. It contains an async method for locking the resource. This method can be awaited on until the resource is free. A second method releases the resource, which would allow another activity waiting to proceed.

The arbitrator abstraction could be given the ability to detect deadlocks and even break deadlocks.


=== State machines

To get used to how different these programming paradigms can be, let's go now to something completely different - state machines. We wont be going into understanding them at the code level because we want to support hierarchical state machines, and the code for that is a little bit non-trivial, but we do want to get an understanding of how state machines are just another programming paradigm that allows us to wire together instance of abstractions. The meaning of the wiring is different than what it was for the event programming paradigm. 

I assume a basic understanding of what state machines are.

[.float-group]
-- 
image::FSM-generic.png[FSM-generic.png, title="State machine execution model", float="left"]

At first it can be difficult to express the solution to a requirements problem as a state machine, even when the state machine is a suitable way to solve the problem. It takes some getting used to the first time. But it only takes a little bit of practice to begin to master it.
--

I once had to express a set of user stories that involved different things that could happen from the outside, either through the UI or other inputs. I knew these were the kind of user stories that were nicely expressed by a state machine, but I had no idea where to start. I only knew that the previously written C code to do the job was a big mess that could no longer be maintained. But I started drawing the state machine, first on paper and then in Visio, and everything started to fall into place very nicely. Before I knew it I had represented what used to be 5000 lines of C code by a single A3 sized state machine diagram. This diagram so well represented the user stories that it was easy to maintain for years to come. This experience was a big factor in the final conception of ALA.   

Here is the diagram.

image::BigStateMachine.pdf.jpg[BigStateMachine.pdf.jpg, title="My first significant state machine for a real embedded device", link=images/BigStateMachine.pdf.jpg]

Notice that the diagram makes heavy use of hierarchical states (boxes inside boxes). These turn out to be important in most of my state machines.

State machine diagrams are drawn in their own unique way. The boxes of the diagram are instances of the abstraction "State". The lines on a state machine diagram are actually instances of another abstraction, "Transition". Out of interest, to relate a state machine diagram to a more conventional ALA wiring diagram, you would replace all the lines on the state machine with boxes representing instances of Transition. The event, guard and actions that associate with a transition then go inside the transition box to configure it. Lines would then wire the transition box to its source state instance and destination state instance. Hierarchy is drawn on the state machine by boxes inside boxes, but in the conventional ALA wiring diagram, the boxes would be drawn outside with lines showing the tree structure. This analogous to the tree structured wiring we have used in previous examples for expressing UIs, which are actually 'contains' relationships. 

The graphical tool being developed will allow the drawing of hierarchical state machines. It will internally transform it to conventional wiring of instances of states and transitions. Interfaces called something like ITransitionSource, ITransitionDestination and IHiercharical would be used to make it execute. It is a simple matter to write code inside the state and transition abstractions to make them execute that would be adequately efficient for most purposes. 

How to make hierarchical state machine execute in an optimally efficient way is a non-trivial problem, but I have worked out the templates for what the C code should look like. Generating this code is a topic for another web page.


=== Imperative

Much conventional code is written using the so called _imperative_ programming paradigm. This paradigm has the same execution model of the underlying CPU hardware. Imperative means sequential execution flow of instructions or statements in computer time. 

Imperative is seldom a good programming paradigm for expressing whole user stories. Even though we call our imperative languages high level languages, its actually quite a low level programming paradigm. However it is efficient because it executes almost directly on the hardware. Imperative highly prescriptive. We can code applications in it directly or we can use it to build other programming paradigms.

Function or method calls go to a named destination, and are synchronous (pass the CPU to the called function for execution, and pass it back to the caller on completion. 

The imperative programming paradigm is wonderful for writing algorithms that are not tied to real-time. However, in modern software, that is a tiny fraction of what programs do. We will seldom use the 'imperative' programming paradigm in ALA. 

Imperative can be structured to comply with ALA constraints, almost. The user story simply makes function calls or method calls to the domain abstractions in the layer below. The problem is that the user story ends up controlling the execution flow, and it handles the data at runtime. The data it receives from one domain abstraction will be passed to the next domain abstraction. This is not really a responsibility we want to put on the user story. We want to factor out execution flow and data. We want the user story to be just about composing instances of domain abstractions.



=== Multithreading

[TIP]
----
Compared with ALA, modular programming will look like a big pool of mud. Multithreaded programming will look like a big pool of boiling mud.
----

In the section about request/response, we briefly considered using multithreading to solve the problem when the request/response is implemented as a synchronous function call, but it takes time and the call blocks. 

In this section we discuss briefly why we avoid using multithreading to solve that particular problem, and discuss what problems might justify using multithreading.

TBD WIP

Because threads block, we must put everything that needs to be concurrent on different threads. Whether it's a conventional architecture or an ALA architecture this leads to coupling throughout the system. Modules may tend to be based on threads rather than a more logical separation. Furthermore, different parts of the system have to collaborate by locking accesses to shared state. There is a misconception that shared state is caused by globals. This is incorrect. Shared state occurs all the time in object oriented programs. Any objects accessed from different threads are shared state even if all state in an object is private. So if a UI object gets work done by a different thread so that the UI remain responsive, then the result will come back to the UI objects on a different thread unless this is carefully avoided. By default most objects are not thread-safe. Missing locks will lead to race conditions. As locks are added, there is even more blocking occurring. This can reduce performance, increase non-determinism, or require even more threads. Too much locking can lead to deadlocks or priority inversions. These issues will hide and appear rarely. 

Unless it is required for latency or other performance throughput reasons that can't be solved on a single thread, I don't recommend going into the quagmire of pre-emptive multithreading. Even if another thread is needed for a specific performance case, I still recommend putting the majority of code in one thread despite any difficulties that entails (as discussed below).

=== Agent based programming

Note that there is a different programming style of multithreading that doesn't use shared state. It is called agent based programming. In this style, we think of every thread as effectively being on a different processor. They can only communicate with one another with messages. Every thread has a single input queue. _All_ communications are asynchronous. Synchronous calls between agents is not possible, so there is no shared state.

The thread's main loop does nothing other than take events from the input queue one at a time, process them, and asynchronously sends events to other such threads. This execution model is a completely different thing. It is called the agent model or producer/consumer. It is safe because there is no shared state and locks are not required. If there is a 'shared' resource, one thread can be assigned to resource. This model does not solve the problem of how to do synchronous request-response calls that block. It is not even the prescriptive programming style that we are trying to achieve with request/response. Every thread is already transformed into a reactive style. Such an execution model is equivalent to a single threaded system where all calls are asynchronous. Like an all asynchronous execution model, performance can be improved by assigning certain abstraction instances to their own processors.



==== State machine vs multithreading concurrency styles

A bigger problem with callback functions or futures is that if the requester call is inside structured statements such as a loop or if statement, or has been called from another function, all the code right back to main() needs to be rewritten like a state machine. It must keep state variables to remember what would normally be implicit in the program counter state, and manually store any other stack based state that the compiler would normally handle for the execution flow through the program. If the code is a simple function called directly from main, this can be done fairly easily. Each time the function is called, it reads the state, which is usually a function pointer, and dispatches to it. Callbacks or continuations go to their own functions. 
+
An advantage of this style of programming is that it easily handles all time discontinuities - things that would otherwise block a thread. It allows reacting to unexpected events much more easily. And it allows longish routines to yield by simply returning part way through, say inside a loop, to reduce the latency of any other concurrent tasks waiting to execute. When the main loop calls back, it can use the state variables to resume processing where it left off. 
+
The great disadvantage of this style of programming is when the program is more prescriptive than reactive. There is a fixed sequence of things that will happen, and we want to express that as normal sequential lines of code, even though certain operations will block. For example, we are moving a large amount of data. Exceptions to the prescribed sequence are rare. I find that async/await or co-routines are the best solutions for this situation. If they are not available, then a cooperative (non-preemptive) thread could be considered to solve the one situation.
+
One of the most common requirements for concurrency is responding to user input. For this we may specify a soft deadline of 0.1s. This means that all state machine, callbacks, or other run-to-completion routines should execute in less than 0.1s. This not difficult to do because the vast majority will execute very quickly. What I sometimes do is put in a system timer to measure the longest running routine. It's usually updating a large display. 
+
What I see happening in most traditional systems is that once an RTOS is included in the system, it is considered to be the solution to _all_ concurrency in the system. But probably 99% of concurrency in most systems can be done on a single thread. Most tasks may have priorities, but will wait until the CPU resource gets to them. So what I do is avoid using threads except for when the specific case of performance can't be solved in any other way. So, in my entire career in embedded systems, I have never ended up having to use a second thread, even when I have an RTOS already in the system at my disposal. Short interrupt routines have handled all situations with hard real time latency requirements. The state machine programming style has better suited the reactive nature of most embedded systems.
+
Remember you can only have one highest priority thread. If you are really in a situation where you have one or more hard real time deadlines that can't be done in interrupts,  then you should probably consider putting in multiple MCUs rather than trying to do, for example, rate-monotonic analysis.
+
Of course, if your system has multiple CPU cores, then you probably have a performance requirement that will need multiple threads to make use of them.




[TIP]
====
Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems. - Jaimie Zawinski

Some people, when confronted with a problem, think "I know, I'll use threads." Now they have ten problems. - Bill Schindler
====



==== Agents on a single thread

TBD

////

=== Asynchronous direct style (curiosity only)

As a curiosity, an asynchronous event-loop execution model could have its priorities manipulated in such as way as to execute everything in the same order as the equivalent synchronous function call tree order.

This is just a thought experiemnt to see what would happen.

To accomplish this, the execution engine would use a priority numbering scheme that is itself three structured. For example, if the function currently being processed has priority 2.4, then when it asynchronously calls another function, it is given priority 2.4.0. The next function called is 2.4.1, etc.

Asynchronous functions can be waiting on some arbitrary condition before they can run, as is provided by the reactor pattern. So if 2.4.0 is waiting on something before it can run, but 2.4.1 is ready to run it can't until 2.4.0 has.

If function 2.4.0 starts but takes time, e.g its starts a delay or its starts a robot arm moving, it starts another function, which would have priority 2.4.0.1, that runs when the delay or operation completes. 

The root number is teated differently. They are still priorities, but they don't wait for earlier numbers. The event loop will run tham all in scheduled order. Root numbers are therefore the analog of multiple threads. Multiple trees can be running at the same time, each with a different root number. Within each tree, events must be processed in order of their tree priority numbers. Everything still runs on a single thread of course. 

As with multithreaded programming, you would have primitives for Delay and Await. 

Note that unlike blocking synchronous function calls, and unlike *await*, a function that calls another asynchronous function will run to completion. Anything that is done synchronously will be done before anything that is done asynchronously, which seems like a serious problem. You would need to use Run to do everything.  

I should do a project or two using this to find out if it has any useful properties. I think it allows pseudo-direct style coding. Consider a conventional piece of multithreading code to make a bridge go up and down. It makes 4 blocking function calls and 2 non-blocking function calls:.

main.c
[source,C]
....
start()
{
    while (true)
    {
        LightOn();  // non blocking
        BridgeUp(); 
        Delay(1000);
        BridgeDown(); 
        LightOff(); // non blocking
        Delay(2000);
    }
}
....


Asynchronous analog version:

[source,C]
....
Start()
{
    Run(LightOn());
    Run(BridgeUp());
    Run(AsyncDelay(1000));
    Run(AsyncBridgeDown());
    Run(AsyncLightOff());
    Run(Start());
    Output("Started"); // synchronous 
}

BridgeUp()
{
    Run(StartMotor());
    Run(Delay(1000));
    Run(StopMotor());
}

....

In this second version, the Run function is used to schedule an asynchronous function with tree priority. Note that Run(function()) is short for Run(()=>function())


Note that if Run is used to schedule the function it is currently running, it detects that and does not use recursive tree numbering.

The start function runs to completion immediately. The scheduled asynchronous functions would then have priority numbers of (assuming the start function has priority 1) 1.0, 1.1, 1.1.0, 1.1.1, 1.1.2, 1.2, 1.3, etc. The execution order will then be the same as blocking synchronous calls. 

If we didn't have the tree priority system described above, then StartMotor() would be run after LightOff() which isn't what is intended.

But we appear to have direct style code that executes asynchronously for very little effort. We didn't have to split the Start function up into a lot of separate callback functions.

All we are trying to do here is get an analog of direct style synchronous blocking function calls using asynchronous function calls without needing a thread.

We can have local variables in the function, but there's probably not much point because the function executes at one point in time. We can't direct return values from the function calls and pass them to the next, obviously, however it seems possible to do that using futures. Here is a continuous streaming from input to output program.

[source,C]
....
Copy()
{
    Future<string> line = Run(Input());
    Run(Ouput(line));
    Run(Copy);
}
....


What about copying a file:.

[source,C]
....
CopyFile(f,g)
{
    Run(OpenFile(f));
    Run(OpenFile(g));
    Run(CopyLine(f,g));
    Run(Closefile(f));
    Run(Closefile(g));
}

CopyLine(f,g)
{
    Future<string> line = Run(Input(f)); 
    RunIf((line)=>line.value!=null,
        Run((line)=>Output(g, line.value)),
        Run(CopyLine(f,g))  // loop
    );
}
....

The code is structured the same as a normal code, but it is creating the program that will run later. RunIf takes three parameters. The lambda expression it will use for the conditional, and two Run functions it runs now. The first Run function will append .0 to the tree numbering and the second will append .1. At runtime, if the lambad expression is false, it will run all the functions with the .0 numbers. If true, it will run all the functions with .1 numbers. 

So all the functions should be left permanently in the 'queue', wich isn't a queue at all - it is a program made up functions in a list, each with a tree structured number that an interpreter uses to direct the flow.

The way we were doing loops wont work because we would be adding more and more functions into the list with the same numbers. Just leave them in the list along with a loop function that is part of the interpreter. We should make RunLoop work the same way as RunIf. The loop function tells the interpreter how to flow through the program.

You must be careful to use only Run functions. We have to keep remembering that they _set up_ what is to happen later. One slip back into using a synchronous function, and the program wont work. It's a bit like getting used to using monads.

Best use a language that supports async/await.

////





=== Example project - Ten-pin bowling

The full source code for the bowling application can be viewed or downloaded from here: https://github.com/johnspray74/GameScoring[https://github.com/johnspray74/GameScoring]



The ten-pin bowling problem is a common coding kata. Usually the problem presented is just to return the total score, but in this example we will tackle the more complicated problem of keeping the score required for a real scorecard, which means we need to keep all the individual frame ball scores. We can afford to do this even for a pedagogical sized example because ALA can provide a simple enough solution.





[plantuml,file="bowling_scorecard2.png"]
----

@startditaa --no-separation --no-shadows

/-----+-----+-----+-----+-----+-----+-----+-----+-----+--------\.
|   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |    10  |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
| 1| 4| 4| 5| 6| /| 5| /|  | X| -| 1| 7| /| 6| /|  | X| 2| /| 6|
+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+  +--+--+
|   5 |  14 |  29 |  49 |  60 |  61 |  77 |  97 | 117 |   133  |
\-----+-----+-----+-----+-----+-----+-----+-----+-----+--------/

                    A ten-pin bowling scorecard
@endditaa
----


The ALA method starts by "describing the requirements in terms of abstractions that you invent". When we start describing the requirements of ten-pin bowling, we immediately find that "a game consists of multiple frames", and a "frame consists of multiple balls". Let's invent an abstraction to express that. Let's call it a "Frame". Instances of Frame can be wired together by a "ConsistsOf" relationship. So let's invent an abstract interface to represent that, and call it 'IConsistsOf'.

Here is the diagram of what we have so far.

////
[plantuml,file="bowling.png"]
----
@startditaa --no-separation --no-shadows utf-8

 nFrames==10     score==10 || nBalls==2
   |              |
   v              v
+-----+        +-----+
|     |        |     |
|Frame|------->|Frame|
|     |        |     |    
+-----+        +-----+
@endditaa
----
////

[plantuml,file="diagram-bowling-1.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
frame [label="Frame|\"frame\"|balls==2 \|\| pins==10"]
ball [label="SinglePlay|\"throw\""]
game -> frame -> ball [label = "IConsistsOf"]
}
}
@enddot
----

This is the first time we are using a diagram for an ALA application, so le's go through the conventions used.

The name in the top of the boxes is the abstraction name. The name just beneath that is the name of an instance of the abstraction. For the bowling application above, we are using two instances of the Frame abstraction, one called "game" and one called "frame". Below the abstraction name and instance name go any configuration information of the instance.

The Frame abstraction is configured with a lambda function to tell it when it is finished. The Frame abstraction works like this - when its last child is complete it will create a new one. It will stop doing that when the lambda expression is true. It will tell its parent it is complete when both the lambda expression is true and its last child Frame is complete. 

The end of the chain is terminated with a leaf abstraction that also implements the 'IConsistsof' interface called 'SinglePlay'. It represents the most indivisible play of a game, which in bowling is one throw. Its job is to record the number of pins downed. 

The concept in the Frame abstraction is that at run-time it will form a composite pattern. As each down-stream child frame completes, a Frame will copy it to start a new one. This will form a tree structure. The "game" instance will end up with 10 "frames", and each frame instance will end up with 1, 2 or 3 SinglePlays.

Note, in reference to the ALA layers, this diagram sits entirely in the top layer, the Application layer. The boxes are instances of abstractions that come from the second layer, the Domain Abstractions layer. The arrows are instances of the programming paradigm, 'InConsistsOf', which comes from the third layer, the ProgrammingParadigms layer.  

This diagram will score 10 frames of ten-pin bowling but does not yet handle strikes and spares. So let's do some 'maintenance' of our application. Because the application so far consists of simple abstractions, which are inherently stable, maintenance should be possible without changing these abstractions.

The way a ten-pin bowling scorecard works, bonuses are scored in a different way for the first 9 frames than for the last frame. In the first nine frames, the bonus ball scores come from following frames, and just appear added to the frame's total. They do no appear as explicit throws. In the last frame, they are shown as explicit throws on the scorecard. That is why there are up to 3 throws in that last frame. 

To handle the different last frame, we just need to modify the completion lambda expression to this. 

 frameNum<9 && (balls==2 || pins==10) // completion condition for frames 1..9
 || (balls==2 && pins<10 || balls==3) // completion condition for frame 10

To handle bonuses for the first 9 frames, we introduce a new abstraction. Let's call it Bonuses. Although we are inventing it first for the game of ten-pin bowling, it is important to think of it as a general purpose, potentially reusable abstraction.

What the Bonus abstraction does is, after its child frame completes, it continues adding plays to the score until its own lambda function returns true.

The completed ten-pin bowling scorer is this:


[plantuml,file="diagram-bowling-2.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
subgraph cluster_C {
label="Ten-Pin Bowling"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
game [label="Frame|\"game\"|nFrames==10"]
bonus [label="Bonus||score\<10 \|\| plays==3"]
frame [label="Frame|\"frame\"|frameNum\<9 && (balls==2 \|\| pins==10)\n \|\|\ (balls==2 && pins\<10 \|\| balls==3)"]
ball [label="SinglePlay"]
game -> bonus -> frame -> ball
}
}
@enddot
----

Note that the "game" instance (the left box of the diagram) implements IConsistsOf. This is where the outside world interfaces to this scoring engine. During a game, the number of pins knocked down by each throw is sent to this IConsistsOf interface. To get the score out, we would call a GetScore method in this interface. 
The hard architectural work is done. We have invented abstractions to make it easy to express requirements. We have a diagram that describes the requirements. And the diagram is executable. All we have to do is put some implementation code inside those abstractions and the application will actually execute.  

First let's turn the diagram into equivalent code. At the moment, there are no automated tools for converting such diagrams to code. But it is a simple matter to do it manually. We get the code below:

....
private IConsistsOf game = new Frame("game")
    .setIsFrameCompleteLambda((gameNumber, frames, score) => frames==10)
    .WireTo(new Bonus("bonus")
        .setIsBonusesCompleteLambda((plays, score) => score<10 || plays==3)
        .WireTo(new Frame("frame")
            .setIsFrameCompleteLambda((frameNumber, balls, pins) => frameNumber<9 && (balls==2 || pins[0]==10) || (balls==2 && pins[0]<10 || balls == 3))
            .WireTo(new SinglePlay("SinglePlay")
    )));
....

All we have done is use the 'new' keyword for every box in the diagram. We have made the constructor take the instance name as a string. (This name is not used except to identify instances during debugging.) We use a method called "WireTo" for every line in the diagram. More on that in a minute. And we pass any optional configuration into the instances using setter methods. The WireTo method and the configuration setter methods all return the 'this' pointer, which allows us to write this code in fluent style. If you are not familiar with fluent style it is just making methods return the this reference, or another object, so that you can chain together method calls using dot operators.

Not all ALA applications will be put together using the method in the previous paragraph, but I have found it a fairly good way to do it for most of them, so we will see this same method used for other example projects to come. 

So far, this has been a fairly top-down, waterfall-like approach. We have something that describes all the details of the requirements, but we haven't considered implementation at all. Past experience tells us this may lead us into dangerous territory. Will the devil be in the details? Will the design have to change once we start implementing the abstractions? The first few times I did this, I was unsure. I was not even sure it could actually be made to work. The reason it does work is because of the way we have handled details. Firstly all details from requirements are in the diagram. The diagram is not an overview of the structure. It is the actual application. All other details, implementation details, are inside abstractions, where they are hidden even at design-time. Being inside abstractions isolates them from affecting anything else. So, it should now be a simple matter of writing classes for those three abstractions and the whole thing will come to life. 
Implementing the three abstractions turns out to be straightforward.

First, design some methods for the IConsistOf interface that we think we will need to make the execution model work:

....
    public interface IConsistsOf
    {
        void Ball(int score);
        bool IsComplete();
        int GetScore();
        int GetnPlays();
        IConsistsOf GetCopy(int frameNumber);
        List<IConsistsOf> GetSubFrames();
    }
....

The first four methods are fairly obvious. The Ball method receives the score on a play. The Complete, GetScore and GetnPlays methods return the state of the sub-part of the game. The GetCopy method asks the object to return a copy of itself (prototype pattern). When a child frame completes, we will call this to get another one. The GetSubFrames method is there to allow getting the scores from all the individual parts of the game as required.

The SinglePlay and Bonus abstractions are very straightforward. 

So let's code the Frame abstraction.
Firstly, Frame both implements and accepts IConsistsOf. A field is needed to accept an IConsistsOf. The WireTo method will set this field: 

....
// Frame.cs
private IConsistsOf downstream;
....


Frame has one 'state' variable which is the list of subframes. This is the composite pattern we referred to earlier, and what ends up forming the tree.

....
// Frame.cs

private List<IConsistsOf> subFrames;
private readonly Func<int, int, int, bool> isFrameComplete;
private readonly int frameNumber = 0;
....

The second variable is the lambda expression that is a configuration passed to us by the application. It would be readonly (immutable) except that I wanted to use a setter method to pass it in, not the constructor, to indicate it is optional. 

The third variable is the frameNumber, also immutable. It allows frame objects to know which child they are to their parent - e.g. 1st frame, 2nd frame etc. This value is passed to the lambda expression in case it wants to use it. For example, the lambda expression for a bowling frame needs to know if it is the last frame.  

The methods of the IConsistsOf interface are now straightforward to write. Let's go over a few of them to get the idea. Here is the most complicated of them, the Ball method:

....
public void Ball(int player, int score)
{
    // 1. Check if our frame is complete, and do nothing
    // 2. See if our last subframe is complete, if so, start a new subframe
    // 3. Pass the ball score to all subframes

    if (IsComplete()) return;

    if (subFrames.Count==0 || subFrames.Last().IsComplete())
    {
        subFrames.Add(downstream.GetCopy(subFrames.Count)); 
    }

    foreach (IConsistsOf s in subFrames)
    {
        s.Ball(player, score);
    }
}
....

It looks to see if the last child frame has completed, and if so starts a new child frame. Then it just passes on the ball score to all the child objects. Any that have completed will ignore it.

The IsComplete method checks two things: 1) that the last child object is complete and 2) that the lambda expression says we are complete:

....
private bool IsComplete()
{
    if (subFrames.Count == 0) return false; // no plays yet
    return (subFrames.Last().IsComplete()) && 
        (isLambdaComplete == null ||
         isLambdaComplete(frameNumber, GetnPlays(), GetScore()));
}
....

....

....

GetScore simply gets the sum of the scores of all the child objects:


....
private int GetScore()
{
    return subFrames.Select(sf => sf.GetScore()).Sum();
}
....

The GetCopy method must make a copy of ourself. This is where the prototype pattern is used. This involves making a copy of our child as well. We will be given a new frameNumber by our parent.

....
IConsistsOf GetCopy(int frameNumber)
{
    var gf = new Frame(frameNumber);
    gf.objectName = this.objectName;
    gf.subFrames = new List<IConsistsOf>();
    gf.downstream = downstream.GetCopy(0);
    gf.isLambdaComplete = this.isLambdaComplete;
    return gf as IConsistsOf;
}
....

The few remaining methods of the IConsistOf interface are trivial. The implementation of IConsistsOf for the other two abstractions, SinglePlay and Bonuses, is similarly straightforward. Note that whereas Frame uses the composite pattern, Bonuses uses the decorator pattern. It implements and requires the IConsistsOf interface. The SinglePlay abstraction, being a leaf abstraction, only implements the IConsistsOf interface. 

One method we haven't discussed is the wireTo method that we used extensively in the application code to wire together instances of our domain abstractions. The wireTo method for Frame is shown below:  

....
public Frame WireTo(IConsistsOf c)
{
    downstream = c;
    return this;
}
....

This method does not need to be implemented in every domain abstraction. I use an extension method for WireTo. The WireTo extension method uses reflection to find the local variable to assign to.

The WireTo method will turn out to be useful in many ALA designs. Remember in ALA we "express requirements by composing instances of abstractions". If the 'instances' of 'abstractions' are implemented as 'objects' of 'classes', then we will use the wireTo method. If the 'instances' of 'abstractions' are 'invocations' of 'functions', as we did in the example project in Chapter One, we wont use WireTo obviously. In the coffeemaker example to come, 'instances' of 'abstractions' are 'references' to 'modules' because a given application would only have one of each abstraction.

The wireTo method returns 'this', which is what allows the fluent coding style used in the application code. The configuration setter methods also return the this reference so that they too can be used in the fluent style. 

Here is the full code for the Frame abstraction (with comments removed as we just explained everything above):

....
// Frame.c
using System;
using System.Collections.Generic;
using System.Linq;
using GameScoring.ProgrammingParadigms;
using System.Text;

namespace GameScoring.DomainAbstractions
{

    public class Frame : IConsistsOf
    {
        private Func<int, int, int[], bool> isLambdaComplete;
        private readonly int frameNumber = 0;
        private IConsistsOf downstream;
        private string objectName;
        private List<IConsistsOf> subFrames = new List<IConsistsOf>();


        public Frame(string name)  
        {
            objectName = name;
        }




        public Frame(int frameNumber)
        {
            this.frameNumber = frameNumber;
        }



        // Configuration setters follow. 

        public Frame setIsFrameCompleteLambda(Func<int, int, int[], bool> lambda)
        {
            isLambdaComplete = lambda;
            return this;
        }





        // Methods to implement the IConsistsOf interface follow


        public void Ball(int player, int score)
        {
            if (IsComplete()) return;

            if (subFrames.Count==0 || subFrames.Last().IsComplete())
            {
                subFrames.Add(downstream.GetCopy(subFrames.Count));
            }

            foreach (IConsistsOf s in subFrames)
            {
                s.Ball(player, score);
            }
        }




        public bool IsComplete()
        {
            if (subFrames.Count == 0) return false; 
            return (subFrames.Last().IsComplete()) && 
                (isLambdaComplete == null || 
                 isLambdaComplete(frameNumber, GetnPlays(), GetScore()));
        }




        public int GetnPlays()
        {
            return subFrames.Count();
        }




        public int[] GetScore()
        {
            return subFrames.Select(sf => sf.GetScore()).Sum();
        }



        List<IConsistsOf> IConsistsOf.GetSubFrames()
        {
            return subFrames;
        }




        IConsistsOf IConsistsOf.GetCopy(int frameNumber)
        {
            var gf = new Frame(frameNumber);
            gf.objectName = this.objectName;
            gf.subFrames = new List<IConsistsOf>();
            gf.downstream = downstream.GetCopy(0);
            gf.isLambdaComplete = this.isLambdaComplete;
            return gf as IConsistsOf;
        }

    }
}


....





=== Example project - Tennis

Now let's modify the bowling application to score tennis. If the bowling game hadn't been implemented using ALA, you probably wouldn't contemplate doing this. But ALA excels for maintainability, and I want to show that off by changing Bowling to Tennis. The Frame and IConsistsOf abstractions look like they could be pretty handy for Tennis. A match consists of sets, which consists of games, which consists of SinglePlays.

We will need to make a small generalization to the Frame abstraction first. This will allow it to keep score for two players. We just change the type of the score from int to int[]. The Ball method will be generalised to take a player parameter to indicate which player won a play. A generalization of an abstraction to make it more reusable is a common operation in ALA.

The only other thing we will need to do is invent a new abstraction to convert a score such as 6,4 into a score like 1,0, because, for example, the winner of a game takes one point into the set score. This new abstraction is called WinnerTakesPoint (WTP in the diagram). 

Here is the tennis scoring game:

[plantuml,file="tennis1.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
match -> wtp1 -> set -> wtp2 -> game -> play
// }
}
@enddot
----

The diagram expresses all the details of the requirements of tennis except the tiebreak.

Here is the diagram's corresponding code:

....
private IConsistsOf match = new Frame()
    .setIsFrameCompleteLambda((matchNumber, nSets, score) => score.Max()==3)
    .WireTo(new WinnerTakesPoint()
        .WireTo(new Frame()                     
            .setIsFrameCompleteLambda((setNumber, nGames, score) => score.Max()>=6 && Math.Abs(score[0]-score[1])>=2)
            .WireTo(new WinnerTakesPoint()
                .WireTo(new Frame()          
                    .setIsFrameCompleteLambda((gameNumber, nBalls, score) => score.Max()>=4 && Math.Abs(score[0]-score[1])>=2) 
                    .WireTo(new SinglePlay()))))));
....

The new WinnerTakesPoint abstraction is easy to write. It is a decorator that implements and requires the IConsistsOf interface. Most methods pass through except the GetScore, which returns 0,0 until the down-stream object completes, then it returns either 1,0 or 0,1 depending on which player has the higher score.

And just like that, the tennis application will now execute. The frame abstraction we invented for bowling is already done.

==== Add tiebreak

Now let's switch our attention back to another example of maintenance. Let's add the tiebreak feature. Another instance of Frame will score the tiebreak quite nicely. However we will need an abstraction that can switch us from playing the set to the tie break. Let's call it Switch, and give it a lambda function to configure it with when to switch from one subframe tree to another. Switch simply returns the sum of scores of its two subtrees. Here then is the full description of the rules of tennis:


[plantuml,file="tennis2.png"]
----
@startdot
digraph foo {
graph [rankdir=LR]
// subgraph cluster_C {
label="Tennis scoring"
style=rounded
#node [style=rounded]
node [shape=Mrecord]
match [label="Frame|\"match\"|score.Max()==3"]
wtp1 [label="WTP"]
set [label="Frame|\"set\"|score.Max()\>=6 && \nMath.Abs(score[0]-score[1])\>=2"]
wtp2 [label="WTP"]
game [label="Frame|\"game\"|score.Max()\>=4 && \nMath.Abs(score[0]-score[1])\>=2"]
play [label="SinglePlay"]
switch [label="Switch||(setNumber\<4 &&\n score[0]==6 && score[1]==6"]
wtp3 [label="WTP"]
tiebreak [label="Frame|\"tiebreak\"|score.Max()==7"]
play2 [label="SinglePlay"]
match -> wtp1 -> switch -> set -> wtp2 -> game -> play
switch:s -> wtp3:w
wtp3 -> tiebreak -> play2
{rank=same set wtp3}
// }
}
@enddot
----

And here is the code version of that diagram. This application passes an exhaustive set of tests for the scoring of tennis.

....
private IConsistsOf match = new Frame("match")
    .setIsFrameCompleteLambda((matchNumber, nSets, score) => score.Max()==3)
    .WireTo(new WinnerTakesPoint("winnerOfSet")
        .WireTo(new Switch("switch")
            .setSwitchLambda((setNumber, nGames, score) => (setNumber<4 && score[0]==6 && score[1]==6))   
            .WireTo(new Frame("set")                     
                .setIsFrameCompleteLambda((setNumber, nGames, score) => score.Max()>=6 && Math.Abs(score[0]-score[1])>=2)
                .WireTo(new WinnerTakesPoint("winnerOfGame")            
                    .WireTo(new Frame("game")          
                        .setIsFrameCompleteLambda((gameNumber, nBalls, score) => score.Max()>=4 && Math.Abs(score[0]-score[1])>=2) 
                        .WireTo(new SinglePlay("singlePlayGame"))
                    )
                )
            )
            .WireTo(new WinnerTakesPoint("winnerOfTieBreak")
                .WireTo(new Frame("tiebreak")          
                    .setIsFrameCompleteLambda((setNumber, nBalls, score) => score.Max()==7)
                    .WireTo(new SinglePlay("singlePlayTiebreak"))
            )
        )
    )
);
....

And just like that we have a full featured executable tennis scoring engine.

==== Final notes

Notice that I have added string names to the instances of Frame and other objects. This is not required to make the program function, but generally is a good habit to get into in ALA. It is because in ALA we typically use multiple instances of abstractions in different parts of the program. The names give us a way of identifying the different instances during any debugging. Using them I can Console.Writeline debugging information along with the object's name.

Around 8 lines of code express the rules of ten-pin bowling and around 15 lines of code express the rules of tennis. That sounds about right for the inherent complexity of the two games. The two rule descriptions actually execute and pass a large battery of tests. 

The domain abstractions are zero-coupled with one another, and are each straightforward to write by just implementing the methods of the IConsistOf interface according to what the abstraction does. The abstractions are simple and stable. So no part of the program is more complex than its own local part.

The domain abstractions are reusable in the domain of game scoring. And, my experience was that as the details inside the abstractions were implemented, the application design didn't have to change. 

Why two example applications? The reason for doing two applications in this example is two-fold. 

. To show the decreasing maintenance effort. The Tennis game was done easily because it reused domain building blocks we had already created for bowling.

. To emphasis where all the details of the requirements end up. The only difference between the bowling and tennis applications is the two diagrams, which are translated into two code files: bowling.cs and tennis.cs of 8 lines and 15 lines respectively. These two files completely express the detailed requirements of their respective games. No other source files have any knowledge of these specific games. Furthermore, Bowling.cs and Tennis.cs do not do anything other than express requirements. All implementation to actually make it execute is hidden in domain abstractions and programming paradigm abstractions. 



Here is a link to the code on Github: https://github.com/johnspray74/GameScoring[GameScoring code]
